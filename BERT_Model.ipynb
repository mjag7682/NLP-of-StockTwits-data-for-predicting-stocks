{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"BERT_Model.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"ec6f358a26194569accda2982ebcc76b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_4c85f4b6371d47e1a7b70f313c361b81","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_2d79b90546ec481e848c5f88bdb6fdf8","IPY_MODEL_b549bc97ff4d42e097b19a42d866cfd3"]}},"4c85f4b6371d47e1a7b70f313c361b81":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"2d79b90546ec481e848c5f88bdb6fdf8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_7e36b288c6bb4d13b99a5ddf7982cb10","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":231508,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":231508,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_803f9ae4210445dea0be014dc4ecdfb4"}},"b549bc97ff4d42e097b19a42d866cfd3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_1b629cef2ab8460eb1659a3ca68d58d5","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 232k/232k [00:00&lt;00:00, 2.08MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_414513a47b86468eb88a3fd1d22384ab"}},"7e36b288c6bb4d13b99a5ddf7982cb10":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"803f9ae4210445dea0be014dc4ecdfb4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"1b629cef2ab8460eb1659a3ca68d58d5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"414513a47b86468eb88a3fd1d22384ab":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"56c9c5a08f1f4c0ca3db38d438bee020":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_9e1cb474795947a4b9a794c23a8e1003","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_da53c1e934664e8aaf76a87bd5b17e12","IPY_MODEL_5fa3896b44af4e2bb466b54d54c5f923"]}},"9e1cb474795947a4b9a794c23a8e1003":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"da53c1e934664e8aaf76a87bd5b17e12":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_799c14801e6547aabf874fbdefbfed82","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":433,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":433,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_e55526aae4914523b2d83fcaf5f54537"}},"5fa3896b44af4e2bb466b54d54c5f923":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_673f5bb634cf4c16af0cb162cb022459","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 433/433 [00:06&lt;00:00, 67.6B/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_a49e772e19ee465a9f3a47f6f124a6e1"}},"799c14801e6547aabf874fbdefbfed82":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"e55526aae4914523b2d83fcaf5f54537":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"673f5bb634cf4c16af0cb162cb022459":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"a49e772e19ee465a9f3a47f6f124a6e1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b26a82db343840e1a84dfc82d5a84cc4":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_f27bfc3d59b74822910917d5726025c7","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_3ef898de0b944cae957bef08dfd7b63f","IPY_MODEL_56cde8de1dd3489c9cf899e9731e4da2"]}},"f27bfc3d59b74822910917d5726025c7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"3ef898de0b944cae957bef08dfd7b63f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_bda462daa54740cf82c87f36c17da1b6","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":440473133,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":440473133,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_59eb674a64334c5bbb6e3c5f6fa7ee1c"}},"56cde8de1dd3489c9cf899e9731e4da2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_d2f1640becbc46578d57d90e85c60773","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 440M/440M [00:05&lt;00:00, 85.9MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_e7522525c4944404bcf79b0ffd276911"}},"bda462daa54740cf82c87f36c17da1b6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"59eb674a64334c5bbb6e3c5f6fa7ee1c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d2f1640becbc46578d57d90e85c60773":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"e7522525c4944404bcf79b0ffd276911":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"SNkbrqkPRIh5"},"source":["Parts of code Refereed from https://mccormickml.com/2019/07/22/BERT-fine-tuning/#34-training--validation-split"]},{"cell_type":"markdown","metadata":{"id":"vLSLfPT_NMRg"},"source":["# Check GPU"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"al2_IHd8wF60","outputId":"cce0f1cd-5ba3-4b56-8ddb-c335bd4406e1"},"source":["import tensorflow as tf\n","\n","# Get the GPU device name.\n","device_name = tf.test.gpu_device_name()\n","\n","# The device name should look like the following:\n","if device_name == '/device:GPU:0':\n","    print('Found GPU at: {}'.format(device_name))\n","else:\n","    raise SystemError('GPU device not found')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Found GPU at: /device:GPU:0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Uv_TjoTsNSTf"},"source":["Assign GPU"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pMBtcA_6wNA4","outputId":"63aeb4db-830f-4eab-e934-05c6bb75099f"},"source":["import torch\n","\n","# Check if GPU is available then telling PyTorch to use the GPU \n","if torch.cuda.is_available():    \n","    device = torch.device(\"cuda\")\n","    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n","    print('We will use the GPU:', torch.cuda.get_device_name(0))\n","\n","# Make use of CPU\n","else:\n","    print('No GPU available, using the CPU instead.')\n","    device = torch.device(\"cpu\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["There are 1 GPU(s) available.\n","We will use the GPU: Tesla T4\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"_J33v9bmNaVs"},"source":["Pip Install"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pmzjOrOMwPim","outputId":"70d99584-7cf0-4d13-dfe7-7611eb149791"},"source":["!pip install transformers"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/83/e74092e7f24a08d751aa59b37a9fc572b2e4af3918cb66f7766c3affb1b4/transformers-3.5.1-py3-none-any.whl (1.3MB)\n","\r\u001b[K     |▎                               | 10kB 16.1MB/s eta 0:00:01\r\u001b[K     |▌                               | 20kB 20.0MB/s eta 0:00:01\r\u001b[K     |▊                               | 30kB 22.0MB/s eta 0:00:01\r\u001b[K     |█                               | 40kB 24.5MB/s eta 0:00:01\r\u001b[K     |█▎                              | 51kB 25.7MB/s eta 0:00:01\r\u001b[K     |█▌                              | 61kB 26.7MB/s eta 0:00:01\r\u001b[K     |█▊                              | 71kB 26.8MB/s eta 0:00:01\r\u001b[K     |██                              | 81kB 27.0MB/s eta 0:00:01\r\u001b[K     |██▎                             | 92kB 27.4MB/s eta 0:00:01\r\u001b[K     |██▌                             | 102kB 25.6MB/s eta 0:00:01\r\u001b[K     |██▊                             | 112kB 25.6MB/s eta 0:00:01\r\u001b[K     |███                             | 122kB 25.6MB/s eta 0:00:01\r\u001b[K     |███▎                            | 133kB 25.6MB/s eta 0:00:01\r\u001b[K     |███▌                            | 143kB 25.6MB/s eta 0:00:01\r\u001b[K     |███▉                            | 153kB 25.6MB/s eta 0:00:01\r\u001b[K     |████                            | 163kB 25.6MB/s eta 0:00:01\r\u001b[K     |████▎                           | 174kB 25.6MB/s eta 0:00:01\r\u001b[K     |████▌                           | 184kB 25.6MB/s eta 0:00:01\r\u001b[K     |████▉                           | 194kB 25.6MB/s eta 0:00:01\r\u001b[K     |█████                           | 204kB 25.6MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 215kB 25.6MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 225kB 25.6MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 235kB 25.6MB/s eta 0:00:01\r\u001b[K     |██████                          | 245kB 25.6MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 256kB 25.6MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 266kB 25.6MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 276kB 25.6MB/s eta 0:00:01\r\u001b[K     |███████                         | 286kB 25.6MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 296kB 25.6MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 307kB 25.6MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 317kB 25.6MB/s eta 0:00:01\r\u001b[K     |████████                        | 327kB 25.6MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 337kB 25.6MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 348kB 25.6MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 358kB 25.6MB/s eta 0:00:01\r\u001b[K     |█████████                       | 368kB 25.6MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 378kB 25.6MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 389kB 25.6MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 399kB 25.6MB/s eta 0:00:01\r\u001b[K     |██████████                      | 409kB 25.6MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 419kB 25.6MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 430kB 25.6MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 440kB 25.6MB/s eta 0:00:01\r\u001b[K     |███████████                     | 450kB 25.6MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 460kB 25.6MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 471kB 25.6MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 481kB 25.6MB/s eta 0:00:01\r\u001b[K     |████████████                    | 491kB 25.6MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 501kB 25.6MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 512kB 25.6MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 522kB 25.6MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 532kB 25.6MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 542kB 25.6MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 552kB 25.6MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 563kB 25.6MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 573kB 25.6MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 583kB 25.6MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 593kB 25.6MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 604kB 25.6MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 614kB 25.6MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 624kB 25.6MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 634kB 25.6MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 645kB 25.6MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 655kB 25.6MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 665kB 25.6MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 675kB 25.6MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 686kB 25.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 696kB 25.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 706kB 25.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 716kB 25.6MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 727kB 25.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 737kB 25.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 747kB 25.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 757kB 25.6MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 768kB 25.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 778kB 25.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 788kB 25.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 798kB 25.6MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 808kB 25.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 819kB 25.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 829kB 25.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 839kB 25.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 849kB 25.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 860kB 25.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 870kB 25.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 880kB 25.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 890kB 25.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 901kB 25.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 911kB 25.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 921kB 25.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 931kB 25.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 942kB 25.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 952kB 25.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 962kB 25.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 972kB 25.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 983kB 25.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 993kB 25.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 1.0MB 25.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.0MB 25.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 1.0MB 25.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 1.0MB 25.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 1.0MB 25.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.1MB 25.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 1.1MB 25.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 1.1MB 25.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.1MB 25.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.1MB 25.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 1.1MB 25.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.1MB 25.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 1.1MB 25.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.1MB 25.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 1.1MB 25.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 1.2MB 25.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 1.2MB 25.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.2MB 25.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 1.2MB 25.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 1.2MB 25.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.2MB 25.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.2MB 25.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.2MB 25.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.2MB 25.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.2MB 25.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.3MB 25.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.3MB 25.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.3MB 25.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 1.3MB 25.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.3MB 25.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.3MB 25.6MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n","Collecting tokenizers==0.9.3\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/34/b39eb9994bc3c999270b69c9eea40ecc6f0e97991dba28282b9fd32d44ee/tokenizers-0.9.3-cp36-cp36m-manylinux1_x86_64.whl (2.9MB)\n","\u001b[K     |████████████████████████████████| 2.9MB 49.6MB/s \n","\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n","\u001b[K     |████████████████████████████████| 890kB 49.9MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n","Collecting sentencepiece==0.1.91\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n","\u001b[K     |████████████████████████████████| 1.1MB 46.6MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers) (3.12.4)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.17.0)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (50.3.2)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=4ca69027d3c08df11bda46e1b3f2c9951163ca69b3b767fed4f87f52b4820a7e\n","  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n","Successfully built sacremoses\n","Installing collected packages: tokenizers, sacremoses, sentencepiece, transformers\n","Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.9.3 transformers-3.5.1\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"7aVVPyenNcVE"},"source":["#Mount Google Drive"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n7YN3cXuwTOA","outputId":"eda7d5dd-2a05-4c1f-e307-e77592c02f43"},"source":["from google.colab import drive\n","drive.mount('drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"QIzMb0VcNjgj"},"source":["Imports"]},{"cell_type":"code","metadata":{"id":"2Fk4EMf1wYGj"},"source":["import pandas as pd\n","import re"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FoXO5gmgNejZ"},"source":["#Load Dataset"]},{"cell_type":"code","metadata":{"id":"H5wmVd9xwcI3"},"source":["import re\n","import pandas as pd\n","df = pd.read_csv('/content/drive/MyDrive/Data/Combined_FAANG_binary_previous.csv', sep=',')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":402},"id":"wrtZczhwwc1R","outputId":"3bef6723-0d30-480c-fb80-1bd85d189fc3"},"source":["df"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>symbol</th>\n","      <th>message</th>\n","      <th>datetime</th>\n","      <th>user</th>\n","      <th>message_id</th>\n","      <th>Date</th>\n","      <th>Time</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>AAPL</td>\n","      <td>qq next 60min confirm start rally aapl coming ...</td>\n","      <td>2015-12-21 18:37:24</td>\n","      <td>191996.0</td>\n","      <td>47148173.0</td>\n","      <td>2015-12-21</td>\n","      <td>18:37:24</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>AAPL</td>\n","      <td>aapl watching gap fill 169 20</td>\n","      <td>2018-11-24 07:02:32</td>\n","      <td>1665234.0</td>\n","      <td>146068732.0</td>\n","      <td>2018-11-24</td>\n","      <td>07:02:32</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>AAPL</td>\n","      <td>aapl weekly options gamblers lose</td>\n","      <td>2014-07-22 21:48:13</td>\n","      <td>71738.0</td>\n","      <td>24904954.0</td>\n","      <td>2014-07-22</td>\n","      <td>21:48:13</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>AAPL</td>\n","      <td>aapl</td>\n","      <td>2020-01-27 07:07:03</td>\n","      <td>1229493.0</td>\n","      <td>191978042.0</td>\n","      <td>2020-01-27</td>\n","      <td>07:07:03</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>AAPL</td>\n","      <td>key levels watch aapl</td>\n","      <td>2014-06-27 15:19:47</td>\n","      <td>106412.0</td>\n","      <td>24190263.0</td>\n","      <td>2014-06-27</td>\n","      <td>15:19:47</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>2566853</th>\n","      <td>NFLX</td>\n","      <td>nflx sister owns kinda thinking telling sell b...</td>\n","      <td>2019-01-11 20:51:22</td>\n","      <td>607557.0</td>\n","      <td>150426203.0</td>\n","      <td>2019-01-11</td>\n","      <td>20:51:22</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2566854</th>\n","      <td>NFLX</td>\n","      <td>nflx bought 123 shares think hit 175 tomorrow</td>\n","      <td>2017-07-17 19:34:14</td>\n","      <td>453320.0</td>\n","      <td>89017330.0</td>\n","      <td>2017-07-17</td>\n","      <td>19:34:14</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2566855</th>\n","      <td>NFLX</td>\n","      <td>quot vsfinancials quot investcorrectly netflix...</td>\n","      <td>2015-06-10 13:02:32</td>\n","      <td>419879.0</td>\n","      <td>38055272.0</td>\n","      <td>2015-06-10</td>\n","      <td>13:02:32</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2566856</th>\n","      <td>NFLX</td>\n","      <td>mgt 32 million volume aapl 41 nflx 11 msft 8 31</td>\n","      <td>2016-05-12 15:37:16</td>\n","      <td>248795.0</td>\n","      <td>54671454.0</td>\n","      <td>2016-05-12</td>\n","      <td>15:37:16</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2566857</th>\n","      <td>NFLX</td>\n","      <td>nflx stock unstable ups downs like ex girlfriend</td>\n","      <td>2020-02-27 14:52:21</td>\n","      <td>2153160.0</td>\n","      <td>196927202.0</td>\n","      <td>2020-02-27</td>\n","      <td>14:52:21</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>2566858 rows × 8 columns</p>\n","</div>"],"text/plain":["        symbol  ... label\n","0         AAPL  ...     1\n","1         AAPL  ...     1\n","2         AAPL  ...     1\n","3         AAPL  ...     0\n","4         AAPL  ...     1\n","...        ...  ...   ...\n","2566853   NFLX  ...     1\n","2566854   NFLX  ...     1\n","2566855   NFLX  ...     1\n","2566856   NFLX  ...     0\n","2566857   NFLX  ...     0\n","\n","[2566858 rows x 8 columns]"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"MnWcqS2w0Og_"},"source":["# df['polarity'].isnan()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KgJKEyGfNmM8"},"source":["Limiting the dataset time-line (Change date for 1 year or 2 year)"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":402},"id":"VIDgR0hWwgVJ","outputId":"ca3d1510-4697-416b-f48a-1f737b0a8d5e"},"source":["df.drop(df[df['Date'] <= '2018-07-20'].index, inplace = True)\n","df\n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>symbol</th>\n","      <th>message</th>\n","      <th>datetime</th>\n","      <th>user</th>\n","      <th>message_id</th>\n","      <th>Date</th>\n","      <th>Time</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1</th>\n","      <td>AAPL</td>\n","      <td>aapl watching gap fill 169 20</td>\n","      <td>2018-11-24 07:02:32</td>\n","      <td>1665234.0</td>\n","      <td>146068732.0</td>\n","      <td>2018-11-24</td>\n","      <td>07:02:32</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>AAPL</td>\n","      <td>aapl</td>\n","      <td>2020-01-27 07:07:03</td>\n","      <td>1229493.0</td>\n","      <td>191978042.0</td>\n","      <td>2020-01-27</td>\n","      <td>07:07:03</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>AAPL</td>\n","      <td>aapl loads cash hand great service business lo...</td>\n","      <td>2018-11-01 23:39:14</td>\n","      <td>123291.0</td>\n","      <td>143688765.0</td>\n","      <td>2018-11-01</td>\n","      <td>23:39:14</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>AAPL</td>\n","      <td>qq became euphoric calls exp week aiming ath f...</td>\n","      <td>2020-05-13 02:13:00</td>\n","      <td>2250451.0</td>\n","      <td>212222428.0</td>\n","      <td>2020-05-13</td>\n","      <td>02:13:00</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>AAPL</td>\n","      <td>spy novices like davey day trader lose money s...</td>\n","      <td>2020-06-24 11:12:09</td>\n","      <td>543250.0</td>\n","      <td>222404886.0</td>\n","      <td>2020-06-24</td>\n","      <td>11:12:09</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>2566843</th>\n","      <td>NFLX</td>\n","      <td>nflx last 3 months big green candles followed ...</td>\n","      <td>2019-05-01 18:21:25</td>\n","      <td>637003.0</td>\n","      <td>162589986.0</td>\n","      <td>2019-05-01</td>\n","      <td>18:21:25</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2566848</th>\n","      <td>NFLX</td>\n","      <td>nflx full move key support</td>\n","      <td>2020-03-12 16:52:43</td>\n","      <td>677915.0</td>\n","      <td>199933357.0</td>\n","      <td>2020-03-12</td>\n","      <td>16:52:43</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2566849</th>\n","      <td>NFLX</td>\n","      <td>spy spx nflx nvda virtually volume today absen...</td>\n","      <td>2019-10-14 18:16:28</td>\n","      <td>55818.0</td>\n","      <td>180328889.0</td>\n","      <td>2019-10-14</td>\n","      <td>18:16:28</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2566853</th>\n","      <td>NFLX</td>\n","      <td>nflx sister owns kinda thinking telling sell b...</td>\n","      <td>2019-01-11 20:51:22</td>\n","      <td>607557.0</td>\n","      <td>150426203.0</td>\n","      <td>2019-01-11</td>\n","      <td>20:51:22</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2566857</th>\n","      <td>NFLX</td>\n","      <td>nflx stock unstable ups downs like ex girlfriend</td>\n","      <td>2020-02-27 14:52:21</td>\n","      <td>2153160.0</td>\n","      <td>196927202.0</td>\n","      <td>2020-02-27</td>\n","      <td>14:52:21</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1021450 rows × 8 columns</p>\n","</div>"],"text/plain":["        symbol  ... label\n","1         AAPL  ...     1\n","3         AAPL  ...     0\n","5         AAPL  ...     1\n","8         AAPL  ...     0\n","9         AAPL  ...     0\n","...        ...  ...   ...\n","2566843   NFLX  ...     1\n","2566848   NFLX  ...     0\n","2566849   NFLX  ...     1\n","2566853   NFLX  ...     1\n","2566857   NFLX  ...     0\n","\n","[1021450 rows x 8 columns]"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"1Cx6gLeFz67L"},"source":["df = df.sample(frac=1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ryjgW3juNw1B"},"source":["Replacing value of Negative label from -1 to 0"]},{"cell_type":"code","metadata":{"id":"eLXnCl5Mwj3w"},"source":["df.drop(df[df['label'] == 0].index, inplace = True) \n","df[\"label\"].replace({-1: 0}, inplace=True) \n","# 0 - Neutral, 1 - positive, 2 - Negative - 3 lables\n","# 0 - Negative, 1 - positive"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cT_i1dK7N0dI"},"source":["#Splitting Dataset"]},{"cell_type":"code","metadata":{"id":"Yjdxyy7Sk5Xh"},"source":["from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(df['message'].values, df['label'].values, test_size=0.1, random_state=42)\n","# df_train = pd.DataFrame(X_train,columns=['message'])\n","# df_train"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IJviEv1swiLr","outputId":"96710ee7-c1a1-4318-b71d-cc3464b0ddb5"},"source":["df_train = pd.DataFrame(y_train)\n","df_train.value_counts()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1    493779\n","0    425526\n","dtype: int64"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w_2Z-t3Ly_Zq","outputId":"14f27068-59f7-4463-cf69-7895f9aca0ab"},"source":["df_test = pd.DataFrame(y_test)\n","df_test.value_counts()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1    55115\n","0    47030\n","dtype: int64"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rqDDjGsCq59s","outputId":"72a4e324-ab5a-4d1c-9be5-914f7341e973"},"source":["X_train"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array(['aapl lot apple feels like shit duck let us call like going plummet nose ducking dive bedrock metrics show one thing sure even repeated drops stocks simply gone outrageously overpriced overpriced coronavirus catalyst kicked current cycle doubt another factor play 2019 stock prices roared ahead earnings stalled creating mismatch inflated valuations dependent rising profits profits hit wall says franz wall grown higher earnings expectations amp p 500 already muted come given potential supply disruptions caused coronavirus outbreak',\n","       'ba aapl amzn shop beware market makers manipulation open',\n","       'amzn speaking amzn aapl er tap next week big tech house hearings sched begin mon bezos cook expected testify coincidence sarcasm',\n","       ..., 'aapl ah always good sign goes tmr',\n","       'aapl jpm bac month ago strongest econ ever recession games jamie dimon needs step like 2016',\n","       'amzn 1843 might waterfall'], dtype=object)"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"code","metadata":{"id":"cWhvYFTIwngw"},"source":["sentences = X_train\n","labels = y_train"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mBZ5y0-xN7R9"},"source":["#BERT Model "]},{"cell_type":"markdown","metadata":{"id":"riYlyAaTwpD-"},"source":["### Bert tokenizer"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":82,"referenced_widgets":["ec6f358a26194569accda2982ebcc76b","4c85f4b6371d47e1a7b70f313c361b81","2d79b90546ec481e848c5f88bdb6fdf8","b549bc97ff4d42e097b19a42d866cfd3","7e36b288c6bb4d13b99a5ddf7982cb10","803f9ae4210445dea0be014dc4ecdfb4","1b629cef2ab8460eb1659a3ca68d58d5","414513a47b86468eb88a3fd1d22384ab"]},"id":"0r9fwNZnwrZt","outputId":"387462ae-5296-44ed-af7b-9c0581652545"},"source":["from transformers import BertTokenizer\n","\n","# Load the BERT tokenizer.\n","print('Loading BERT tokenizer...')\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Loading BERT tokenizer...\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ec6f358a26194569accda2982ebcc76b","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"phtGTD2uOKA2"},"source":["Changing Text to the format required for BERT model training"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8LSOQ7RNwuBY","outputId":"1b55f78b-287f-444f-8fb9-1e79573bfb75"},"source":["input_ids = []\n","attention_masks = []\n","\n","# For each sentence encode_plus would tokenize, add special tokens ([CLS], [SEP]), map tokens to IDs, create attention masks\n","for sent in sentences:\n","    encoded_dict = tokenizer.encode_plus(\n","                        sent,                     \n","                        add_special_tokens = True,\n","                        max_length = 160,         \n","                        truncation = True,\n","                        pad_to_max_length = True,\n","                        return_attention_mask = True,   \n","                        return_tensors = 'pt',    \n","                   )\n","    \n","    # Add the encoded sentence to the list.    \n","    input_ids.append(encoded_dict['input_ids'])\n","    \n","    # And its attention mask (simply differentiates padding from non-padding).\n","    attention_masks.append(encoded_dict['attention_mask'])\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VZtyEQm7wwjd","outputId":"8e3b3aa7-9380-4be2-8854-8e5992cf98bb"},"source":["# Converting the lists into tensors\n","input_ids = torch.cat(input_ids, dim=0)\n","attention_masks = torch.cat(attention_masks, dim=0)\n","labels = torch.tensor(labels)\n","\n","print('Original: ', sentences[0])\n","print('Token IDs:', input_ids[0])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Original:  aapl bulls need chew red candle 210\n","Token IDs: tensor([  101,  9779, 24759, 12065,  2342, 21271,  2417, 13541, 12875,   102,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D2L3zgP60kpe","outputId":"8cc2a5cb-7df0-41dc-90d2-23bff71936fc"},"source":["from torch.utils.data import TensorDataset, random_split\n","\n","dataset = TensorDataset(input_ids, attention_masks, labels)\n","\n","# Creating TensorDataset by combining the training inputs\n","train_size = int(0.9 * len(dataset))\n","val_size = len(dataset) - train_size\n","\n","# Dividing the training and validation dataset by selecting samples randomly\n","train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n","\n","print('{} training samples'.format(train_size))\n","print('{} validation samples'.format(val_size))\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["827374 training samples\n","91931 validation samples\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"vLlOL-I70nMC"},"source":["from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","\n","batch_size = 32\n","\n","train_dataloader = DataLoader(\n","            train_dataset,  \n","            sampler = RandomSampler(train_dataset), \n","            batch_size = batch_size \n","        )\n","\n","validation_dataloader = DataLoader(\n","            val_dataset, \n","            sampler = SequentialSampler(val_dataset), \n","            batch_size = batch_size \n","        )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["56c9c5a08f1f4c0ca3db38d438bee020","9e1cb474795947a4b9a794c23a8e1003","da53c1e934664e8aaf76a87bd5b17e12","5fa3896b44af4e2bb466b54d54c5f923","799c14801e6547aabf874fbdefbfed82","e55526aae4914523b2d83fcaf5f54537","673f5bb634cf4c16af0cb162cb022459","a49e772e19ee465a9f3a47f6f124a6e1","b26a82db343840e1a84dfc82d5a84cc4","f27bfc3d59b74822910917d5726025c7","3ef898de0b944cae957bef08dfd7b63f","56cde8de1dd3489c9cf899e9731e4da2","bda462daa54740cf82c87f36c17da1b6","59eb674a64334c5bbb6e3c5f6fa7ee1c","d2f1640becbc46578d57d90e85c60773","e7522525c4944404bcf79b0ffd276911"]},"id":"oqRRlZwB0u87","outputId":"6cb27cc7-295b-4ed5-a49c-c9559e5bcbaf"},"source":["from transformers import BertModel, BertForSequenceClassification, AdamW, BertConfig\n","\n","\n","# Loading BertForSequenceClassification model for training\n","model = BertForSequenceClassification.from_pretrained(\n","    \"bert-base-uncased\",\n","    num_labels = 2, #change here if classes changed to 3   \n","    output_attentions = False,\n","    output_hidden_states = False, \n",")\n","\n","torch.cuda.empty_cache()\n","\n","# Assigning the model on the GPU.\n","model.cuda()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"56c9c5a08f1f4c0ca3db38d438bee020","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b26a82db343840e1a84dfc82d5a84cc4","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descri…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["BertForSequenceClassification(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"markdown","metadata":{"id":"XSQZhp9nO_xh"},"source":["Printing the Model architecture"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0Tbs4xVJ0xhC","outputId":"26713843-ef54-419a-ab75-97f7f636f741"},"source":["params = list(model.named_parameters())\n","\n","print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n","\n","print('==== Embedding Layer ====\\n')\n","\n","for p in params[0:5]:\n","    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n","\n","print('\\n==== First Transformer ====\\n')\n","\n","for p in params[5:21]:\n","    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n","\n","print('\\n==== Output Layer ====\\n')\n","\n","for p in params[-4:]:\n","    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["The BERT model has 201 different named parameters.\n","\n","==== Embedding Layer ====\n","\n","bert.embeddings.word_embeddings.weight                  (30522, 768)\n","bert.embeddings.position_embeddings.weight                (512, 768)\n","bert.embeddings.token_type_embeddings.weight                (2, 768)\n","bert.embeddings.LayerNorm.weight                              (768,)\n","bert.embeddings.LayerNorm.bias                                (768,)\n","\n","==== First Transformer ====\n","\n","bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n","bert.encoder.layer.0.attention.self.query.bias                (768,)\n","bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n","bert.encoder.layer.0.attention.self.key.bias                  (768,)\n","bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n","bert.encoder.layer.0.attention.self.value.bias                (768,)\n","bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n","bert.encoder.layer.0.attention.output.dense.bias              (768,)\n","bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n","bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n","bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n","bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n","bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n","bert.encoder.layer.0.output.dense.bias                        (768,)\n","bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n","bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n","\n","==== Output Layer ====\n","\n","bert.pooler.dense.weight                                  (768, 768)\n","bert.pooler.dense.bias                                        (768,)\n","classifier.weight                                           (2, 768)\n","classifier.bias                                                 (2,)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"e1UgNu9HQise"},"source":["#Fine Tune"]},{"cell_type":"code","metadata":{"id":"C4wj6KHk0zvj"},"source":["optimizer = AdamW(model.parameters(),\n","                  lr = 1e-5, \n","                  eps = 1e-8,\n","                  weight_decay = 1e-5\n","                )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SllBDIii02Au"},"source":["from transformers import get_linear_schedule_with_warmup\n","epochs = 2\n","total_steps = len(train_dataloader) * epochs\n","scheduler = get_linear_schedule_with_warmup(optimizer, \n","                                            num_warmup_steps = 0, # Default value in run_glue.py\n","                                            num_training_steps = total_steps)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-8Wv4I9g06hn"},"source":["import numpy as np\n","# Function for calculating the accuracy \n","def flat_accuracy(preds, labels):\n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gHL4PUji08x-"},"source":["torch.cuda.empty_cache()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CX7FjKkG09yN"},"source":["import time\n","import datetime\n","\n","def format_time(elapsed):\n","    '''\n","    Takes a time in seconds and returns a string hh:mm:ss\n","    '''\n","    elapsed_rounded = int(round((elapsed)))\n","    \n","    return str(datetime.timedelta(seconds=elapsed_rounded))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2Lz6xDdd0_Xf","outputId":"522c4145-93a8-4cfe-aa2d-01065834c376"},"source":["import random\n","import numpy as np\n","\n","seed_val = 42\n","\n","random.seed(seed_val)\n","np.random.seed(seed_val)\n","torch.manual_seed(seed_val)\n","torch.cuda.manual_seed_all(seed_val)\n","\n","\n","training_stats = []\n","total_t0 = time.time()\n","\n","for epoch_i in range(0, epochs):\n","    \n","    # ========================================\n","    #               Training\n","    # ========================================\n","\n","    print(\"\")\n","    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n","    print('Training...')\n","    t0 = time.time()\n","    total_train_loss = 0\n","    \n","    #Setting the model to training mode\n","    model.train()\n","\n","    # This block is for progress update of every batch \n","    for step, batch in enumerate(train_dataloader):\n","\n","        \n","        if step % 100 == 0 and not step == 0:\n","            elapsed = format_time(time.time() - t0)\n","            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n","\n","        # From the dataloader unpacking the training batch  \n","        b_input_ids = batch[0].to(device)\n","        b_input_mask = batch[1].to(device)\n","        b_labels = batch[2].to(device)\n","\n","        model.zero_grad()        \n","        \n","        #model outputs\n","        loss, logits = model(b_input_ids, \n","                             token_type_ids=None, \n","                             attention_mask=b_input_mask, \n","                             labels=b_labels)\n","\n","\n","        total_train_loss += loss.item()\n","\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","        optimizer.step()\n","        scheduler.step()\n","    avg_train_loss = total_train_loss / len(train_dataloader)            \n","    training_time = format_time(time.time() - t0)\n","\n","    print(\"\")\n","    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n","    print(\"  Training epcoh took: {:}\".format(training_time))\n","        \n","    # ========================================\n","    #               Validation\n","    # ========================================\n"," \n","    print(\"\")\n","    print(\"Running Validation...\")\n","\n","    t0 = time.time()\n","\n","    # Setting the model to evaluation mode\n","    model.eval()\n","\n","    # Tracking variables \n","    total_eval_accuracy = 0\n","    total_eval_loss = 0\n","    nb_eval_steps = 0\n","\n"," \n","    for batch in validation_dataloader:\n","        \n","        # Unpack this validation batch from our dataloader. \n","        b_input_ids = batch[0].to(device)\n","        b_input_mask = batch[1].to(device)\n","        b_labels = batch[2].to(device)\n","\n","        with torch.no_grad():        \n","\n","            (loss, logits) = model(b_input_ids, \n","                                   token_type_ids=None, \n","                                   attention_mask=b_input_mask,\n","                                   labels=b_labels)\n","\n","        total_eval_loss += loss.item()\n","\n","        logits = logits.detach().cpu().numpy()\n","        label_ids = b_labels.to('cpu').numpy()\n","        total_eval_accuracy += flat_accuracy(logits, label_ids)\n","    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n","    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n","\n","    avg_val_loss = total_eval_loss / len(validation_dataloader)\n","    validation_time = format_time(time.time() - t0)\n","    \n","    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n","    print(\"  Validation took: {:}\".format(validation_time))\n","\n","    training_stats.append(\n","        {\n","            'epoch': epoch_i + 1,\n","            'Training Loss': avg_train_loss,\n","            'Valid. Loss': avg_val_loss,\n","            'Valid. Accur.': avg_val_accuracy,\n","            'Training Time': training_time,\n","            'Validation Time': validation_time\n","        }\n","    )\n","\n","print(\"\")\n","print(\"Training complete!\")\n","\n","print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","======== Epoch 1 / 2 ========\n","Training...\n","  Batch   100  of  25,856.    Elapsed: 0:00:51.\n","  Batch   200  of  25,856.    Elapsed: 0:01:41.\n","  Batch   300  of  25,856.    Elapsed: 0:02:32.\n","  Batch   400  of  25,856.    Elapsed: 0:03:22.\n","  Batch   500  of  25,856.    Elapsed: 0:04:13.\n","  Batch   600  of  25,856.    Elapsed: 0:05:03.\n","  Batch   700  of  25,856.    Elapsed: 0:05:54.\n","  Batch   800  of  25,856.    Elapsed: 0:06:44.\n","  Batch   900  of  25,856.    Elapsed: 0:07:35.\n","  Batch 1,000  of  25,856.    Elapsed: 0:08:25.\n","  Batch 1,100  of  25,856.    Elapsed: 0:09:16.\n","  Batch 1,200  of  25,856.    Elapsed: 0:10:06.\n","  Batch 1,300  of  25,856.    Elapsed: 0:10:57.\n","  Batch 1,400  of  25,856.    Elapsed: 0:11:47.\n","  Batch 1,500  of  25,856.    Elapsed: 0:12:38.\n","  Batch 1,600  of  25,856.    Elapsed: 0:13:28.\n","  Batch 1,700  of  25,856.    Elapsed: 0:14:19.\n","  Batch 1,800  of  25,856.    Elapsed: 0:15:09.\n","  Batch 1,900  of  25,856.    Elapsed: 0:16:00.\n","  Batch 2,000  of  25,856.    Elapsed: 0:16:50.\n","  Batch 2,100  of  25,856.    Elapsed: 0:17:41.\n","  Batch 2,200  of  25,856.    Elapsed: 0:18:32.\n","  Batch 2,300  of  25,856.    Elapsed: 0:19:22.\n","  Batch 2,400  of  25,856.    Elapsed: 0:20:13.\n","  Batch 2,500  of  25,856.    Elapsed: 0:21:03.\n","  Batch 2,600  of  25,856.    Elapsed: 0:21:54.\n","  Batch 2,700  of  25,856.    Elapsed: 0:22:44.\n","  Batch 2,800  of  25,856.    Elapsed: 0:23:35.\n","  Batch 2,900  of  25,856.    Elapsed: 0:24:26.\n","  Batch 3,000  of  25,856.    Elapsed: 0:25:16.\n","  Batch 3,100  of  25,856.    Elapsed: 0:26:07.\n","  Batch 3,200  of  25,856.    Elapsed: 0:26:57.\n","  Batch 3,300  of  25,856.    Elapsed: 0:27:48.\n","  Batch 3,400  of  25,856.    Elapsed: 0:28:38.\n","  Batch 3,500  of  25,856.    Elapsed: 0:29:29.\n","  Batch 3,600  of  25,856.    Elapsed: 0:30:20.\n","  Batch 3,700  of  25,856.    Elapsed: 0:31:10.\n","  Batch 3,800  of  25,856.    Elapsed: 0:32:01.\n","  Batch 3,900  of  25,856.    Elapsed: 0:32:52.\n","  Batch 4,000  of  25,856.    Elapsed: 0:33:42.\n","  Batch 4,100  of  25,856.    Elapsed: 0:34:33.\n","  Batch 4,200  of  25,856.    Elapsed: 0:35:24.\n","  Batch 4,300  of  25,856.    Elapsed: 0:36:14.\n","  Batch 4,400  of  25,856.    Elapsed: 0:37:05.\n","  Batch 4,500  of  25,856.    Elapsed: 0:37:56.\n","  Batch 4,600  of  25,856.    Elapsed: 0:38:46.\n","  Batch 4,700  of  25,856.    Elapsed: 0:39:37.\n","  Batch 4,800  of  25,856.    Elapsed: 0:40:28.\n","  Batch 4,900  of  25,856.    Elapsed: 0:41:18.\n","  Batch 5,000  of  25,856.    Elapsed: 0:42:09.\n","  Batch 5,100  of  25,856.    Elapsed: 0:43:00.\n","  Batch 5,200  of  25,856.    Elapsed: 0:43:50.\n","  Batch 5,300  of  25,856.    Elapsed: 0:44:41.\n","  Batch 5,400  of  25,856.    Elapsed: 0:45:32.\n","  Batch 5,500  of  25,856.    Elapsed: 0:46:23.\n","  Batch 5,600  of  25,856.    Elapsed: 0:47:13.\n","  Batch 5,700  of  25,856.    Elapsed: 0:48:04.\n","  Batch 5,800  of  25,856.    Elapsed: 0:48:55.\n","  Batch 5,900  of  25,856.    Elapsed: 0:49:46.\n","  Batch 6,000  of  25,856.    Elapsed: 0:50:37.\n","  Batch 6,100  of  25,856.    Elapsed: 0:51:28.\n","  Batch 6,200  of  25,856.    Elapsed: 0:52:18.\n","  Batch 6,300  of  25,856.    Elapsed: 0:53:09.\n","  Batch 6,400  of  25,856.    Elapsed: 0:54:00.\n","  Batch 6,500  of  25,856.    Elapsed: 0:54:51.\n","  Batch 6,600  of  25,856.    Elapsed: 0:55:42.\n","  Batch 6,700  of  25,856.    Elapsed: 0:56:33.\n","  Batch 6,800  of  25,856.    Elapsed: 0:57:23.\n","  Batch 6,900  of  25,856.    Elapsed: 0:58:14.\n","  Batch 7,000  of  25,856.    Elapsed: 0:59:05.\n","  Batch 7,100  of  25,856.    Elapsed: 0:59:56.\n","  Batch 7,200  of  25,856.    Elapsed: 1:00:47.\n","  Batch 7,300  of  25,856.    Elapsed: 1:01:37.\n","  Batch 7,400  of  25,856.    Elapsed: 1:02:28.\n","  Batch 7,500  of  25,856.    Elapsed: 1:03:19.\n","  Batch 7,600  of  25,856.    Elapsed: 1:04:10.\n","  Batch 7,700  of  25,856.    Elapsed: 1:05:01.\n","  Batch 7,800  of  25,856.    Elapsed: 1:05:52.\n","  Batch 7,900  of  25,856.    Elapsed: 1:06:42.\n","  Batch 8,000  of  25,856.    Elapsed: 1:07:33.\n","  Batch 8,100  of  25,856.    Elapsed: 1:08:24.\n","  Batch 8,200  of  25,856.    Elapsed: 1:09:15.\n","  Batch 8,300  of  25,856.    Elapsed: 1:10:06.\n","  Batch 8,400  of  25,856.    Elapsed: 1:10:56.\n","  Batch 8,500  of  25,856.    Elapsed: 1:11:47.\n","  Batch 8,600  of  25,856.    Elapsed: 1:12:38.\n","  Batch 8,700  of  25,856.    Elapsed: 1:13:28.\n","  Batch 8,800  of  25,856.    Elapsed: 1:14:19.\n","  Batch 8,900  of  25,856.    Elapsed: 1:15:10.\n","  Batch 9,000  of  25,856.    Elapsed: 1:16:00.\n","  Batch 9,100  of  25,856.    Elapsed: 1:16:51.\n","  Batch 9,200  of  25,856.    Elapsed: 1:17:42.\n","  Batch 9,300  of  25,856.    Elapsed: 1:18:32.\n","  Batch 9,400  of  25,856.    Elapsed: 1:19:23.\n","  Batch 9,500  of  25,856.    Elapsed: 1:20:14.\n","  Batch 9,600  of  25,856.    Elapsed: 1:21:04.\n","  Batch 9,700  of  25,856.    Elapsed: 1:21:55.\n","  Batch 9,800  of  25,856.    Elapsed: 1:22:46.\n","  Batch 9,900  of  25,856.    Elapsed: 1:23:37.\n","  Batch 10,000  of  25,856.    Elapsed: 1:24:27.\n","  Batch 10,100  of  25,856.    Elapsed: 1:25:18.\n","  Batch 10,200  of  25,856.    Elapsed: 1:26:09.\n","  Batch 10,300  of  25,856.    Elapsed: 1:26:59.\n","  Batch 10,400  of  25,856.    Elapsed: 1:27:50.\n","  Batch 10,500  of  25,856.    Elapsed: 1:28:41.\n","  Batch 10,600  of  25,856.    Elapsed: 1:29:32.\n","  Batch 10,700  of  25,856.    Elapsed: 1:30:22.\n","  Batch 10,800  of  25,856.    Elapsed: 1:31:13.\n","  Batch 10,900  of  25,856.    Elapsed: 1:32:04.\n","  Batch 11,000  of  25,856.    Elapsed: 1:32:55.\n","  Batch 11,100  of  25,856.    Elapsed: 1:33:45.\n","  Batch 11,200  of  25,856.    Elapsed: 1:34:36.\n","  Batch 11,300  of  25,856.    Elapsed: 1:35:27.\n","  Batch 11,400  of  25,856.    Elapsed: 1:36:18.\n","  Batch 11,500  of  25,856.    Elapsed: 1:37:08.\n","  Batch 11,600  of  25,856.    Elapsed: 1:37:59.\n","  Batch 11,700  of  25,856.    Elapsed: 1:38:50.\n","  Batch 11,800  of  25,856.    Elapsed: 1:39:41.\n","  Batch 11,900  of  25,856.    Elapsed: 1:40:31.\n","  Batch 12,000  of  25,856.    Elapsed: 1:41:22.\n","  Batch 12,100  of  25,856.    Elapsed: 1:42:13.\n","  Batch 12,200  of  25,856.    Elapsed: 1:43:04.\n","  Batch 12,300  of  25,856.    Elapsed: 1:43:55.\n","  Batch 12,400  of  25,856.    Elapsed: 1:44:45.\n","  Batch 12,500  of  25,856.    Elapsed: 1:45:36.\n","  Batch 12,600  of  25,856.    Elapsed: 1:46:27.\n","  Batch 12,700  of  25,856.    Elapsed: 1:47:18.\n","  Batch 12,800  of  25,856.    Elapsed: 1:48:09.\n","  Batch 12,900  of  25,856.    Elapsed: 1:48:59.\n","  Batch 13,000  of  25,856.    Elapsed: 1:49:50.\n","  Batch 13,100  of  25,856.    Elapsed: 1:50:41.\n","  Batch 13,200  of  25,856.    Elapsed: 1:51:32.\n","  Batch 13,300  of  25,856.    Elapsed: 1:52:22.\n","  Batch 13,400  of  25,856.    Elapsed: 1:53:13.\n","  Batch 13,500  of  25,856.    Elapsed: 1:54:04.\n","  Batch 13,600  of  25,856.    Elapsed: 1:54:55.\n","  Batch 13,700  of  25,856.    Elapsed: 1:55:46.\n","  Batch 13,800  of  25,856.    Elapsed: 1:56:36.\n","  Batch 13,900  of  25,856.    Elapsed: 1:57:27.\n","  Batch 14,000  of  25,856.    Elapsed: 1:58:18.\n","  Batch 14,100  of  25,856.    Elapsed: 1:59:09.\n","  Batch 14,200  of  25,856.    Elapsed: 2:00:00.\n","  Batch 14,300  of  25,856.    Elapsed: 2:00:50.\n","  Batch 14,400  of  25,856.    Elapsed: 2:01:41.\n","  Batch 14,500  of  25,856.    Elapsed: 2:02:32.\n","  Batch 14,600  of  25,856.    Elapsed: 2:03:23.\n","  Batch 14,700  of  25,856.    Elapsed: 2:04:14.\n","  Batch 14,800  of  25,856.    Elapsed: 2:05:05.\n","  Batch 14,900  of  25,856.    Elapsed: 2:05:55.\n","  Batch 15,000  of  25,856.    Elapsed: 2:06:46.\n","  Batch 15,100  of  25,856.    Elapsed: 2:07:37.\n","  Batch 15,200  of  25,856.    Elapsed: 2:08:28.\n","  Batch 15,300  of  25,856.    Elapsed: 2:09:18.\n","  Batch 15,400  of  25,856.    Elapsed: 2:10:09.\n","  Batch 15,500  of  25,856.    Elapsed: 2:11:00.\n","  Batch 15,600  of  25,856.    Elapsed: 2:11:50.\n","  Batch 15,700  of  25,856.    Elapsed: 2:12:41.\n","  Batch 15,800  of  25,856.    Elapsed: 2:13:32.\n","  Batch 15,900  of  25,856.    Elapsed: 2:14:23.\n","  Batch 16,000  of  25,856.    Elapsed: 2:15:13.\n","  Batch 16,100  of  25,856.    Elapsed: 2:16:04.\n","  Batch 16,200  of  25,856.    Elapsed: 2:16:55.\n","  Batch 16,300  of  25,856.    Elapsed: 2:17:45.\n","  Batch 16,400  of  25,856.    Elapsed: 2:18:36.\n","  Batch 16,500  of  25,856.    Elapsed: 2:19:27.\n","  Batch 16,600  of  25,856.    Elapsed: 2:20:17.\n","  Batch 16,700  of  25,856.    Elapsed: 2:21:08.\n","  Batch 16,800  of  25,856.    Elapsed: 2:21:59.\n","  Batch 16,900  of  25,856.    Elapsed: 2:22:50.\n","  Batch 17,000  of  25,856.    Elapsed: 2:23:40.\n","  Batch 17,100  of  25,856.    Elapsed: 2:24:31.\n","  Batch 17,200  of  25,856.    Elapsed: 2:25:22.\n","  Batch 17,300  of  25,856.    Elapsed: 2:26:12.\n","  Batch 17,400  of  25,856.    Elapsed: 2:27:03.\n","  Batch 17,500  of  25,856.    Elapsed: 2:27:54.\n","  Batch 17,600  of  25,856.    Elapsed: 2:28:44.\n","  Batch 17,700  of  25,856.    Elapsed: 2:29:35.\n","  Batch 17,800  of  25,856.    Elapsed: 2:30:26.\n","  Batch 17,900  of  25,856.    Elapsed: 2:31:17.\n","  Batch 18,000  of  25,856.    Elapsed: 2:32:07.\n","  Batch 18,100  of  25,856.    Elapsed: 2:32:58.\n","  Batch 18,200  of  25,856.    Elapsed: 2:33:49.\n","  Batch 18,300  of  25,856.    Elapsed: 2:34:39.\n","  Batch 18,400  of  25,856.    Elapsed: 2:35:30.\n","  Batch 18,500  of  25,856.    Elapsed: 2:36:21.\n","  Batch 18,600  of  25,856.    Elapsed: 2:37:12.\n","  Batch 18,700  of  25,856.    Elapsed: 2:38:02.\n","  Batch 18,800  of  25,856.    Elapsed: 2:38:53.\n","  Batch 18,900  of  25,856.    Elapsed: 2:39:44.\n","  Batch 19,000  of  25,856.    Elapsed: 2:40:34.\n","  Batch 19,100  of  25,856.    Elapsed: 2:41:25.\n","  Batch 19,200  of  25,856.    Elapsed: 2:42:16.\n","  Batch 19,300  of  25,856.    Elapsed: 2:43:07.\n","  Batch 19,400  of  25,856.    Elapsed: 2:43:57.\n","  Batch 19,500  of  25,856.    Elapsed: 2:44:48.\n","  Batch 19,600  of  25,856.    Elapsed: 2:45:39.\n","  Batch 19,700  of  25,856.    Elapsed: 2:46:29.\n","  Batch 19,800  of  25,856.    Elapsed: 2:47:20.\n","  Batch 19,900  of  25,856.    Elapsed: 2:48:11.\n","  Batch 20,000  of  25,856.    Elapsed: 2:49:01.\n","  Batch 20,100  of  25,856.    Elapsed: 2:49:52.\n","  Batch 20,200  of  25,856.    Elapsed: 2:50:43.\n","  Batch 20,300  of  25,856.    Elapsed: 2:51:33.\n","  Batch 20,400  of  25,856.    Elapsed: 2:52:24.\n","  Batch 20,500  of  25,856.    Elapsed: 2:53:15.\n","  Batch 20,600  of  25,856.    Elapsed: 2:54:06.\n","  Batch 20,700  of  25,856.    Elapsed: 2:54:56.\n","  Batch 20,800  of  25,856.    Elapsed: 2:55:47.\n","  Batch 20,900  of  25,856.    Elapsed: 2:56:38.\n","  Batch 21,000  of  25,856.    Elapsed: 2:57:28.\n","  Batch 21,100  of  25,856.    Elapsed: 2:58:19.\n","  Batch 21,200  of  25,856.    Elapsed: 2:59:10.\n","  Batch 21,300  of  25,856.    Elapsed: 3:00:01.\n","  Batch 21,400  of  25,856.    Elapsed: 3:00:51.\n","  Batch 21,500  of  25,856.    Elapsed: 3:01:42.\n","  Batch 21,600  of  25,856.    Elapsed: 3:02:33.\n","  Batch 21,700  of  25,856.    Elapsed: 3:03:23.\n","  Batch 21,800  of  25,856.    Elapsed: 3:04:14.\n","  Batch 21,900  of  25,856.    Elapsed: 3:05:05.\n","  Batch 22,000  of  25,856.    Elapsed: 3:05:56.\n","  Batch 22,100  of  25,856.    Elapsed: 3:06:46.\n","  Batch 22,200  of  25,856.    Elapsed: 3:07:37.\n","  Batch 22,300  of  25,856.    Elapsed: 3:08:28.\n","  Batch 22,400  of  25,856.    Elapsed: 3:09:19.\n","  Batch 22,500  of  25,856.    Elapsed: 3:10:09.\n","  Batch 22,600  of  25,856.    Elapsed: 3:11:00.\n","  Batch 22,700  of  25,856.    Elapsed: 3:11:51.\n","  Batch 22,800  of  25,856.    Elapsed: 3:12:41.\n","  Batch 22,900  of  25,856.    Elapsed: 3:13:32.\n","  Batch 23,000  of  25,856.    Elapsed: 3:14:23.\n","  Batch 23,100  of  25,856.    Elapsed: 3:15:14.\n","  Batch 23,200  of  25,856.    Elapsed: 3:16:04.\n","  Batch 23,300  of  25,856.    Elapsed: 3:16:55.\n","  Batch 23,400  of  25,856.    Elapsed: 3:17:46.\n","  Batch 23,500  of  25,856.    Elapsed: 3:18:36.\n","  Batch 23,600  of  25,856.    Elapsed: 3:19:27.\n","  Batch 23,700  of  25,856.    Elapsed: 3:20:18.\n","  Batch 23,800  of  25,856.    Elapsed: 3:21:09.\n","  Batch 23,900  of  25,856.    Elapsed: 3:21:59.\n","  Batch 24,000  of  25,856.    Elapsed: 3:22:50.\n","  Batch 24,100  of  25,856.    Elapsed: 3:23:41.\n","  Batch 24,200  of  25,856.    Elapsed: 3:24:32.\n","  Batch 24,300  of  25,856.    Elapsed: 3:25:22.\n","  Batch 24,400  of  25,856.    Elapsed: 3:26:13.\n","  Batch 24,500  of  25,856.    Elapsed: 3:27:04.\n","  Batch 24,600  of  25,856.    Elapsed: 3:27:54.\n","  Batch 24,700  of  25,856.    Elapsed: 3:28:45.\n","  Batch 24,800  of  25,856.    Elapsed: 3:29:35.\n","  Batch 24,900  of  25,856.    Elapsed: 3:30:26.\n","  Batch 25,000  of  25,856.    Elapsed: 3:31:16.\n","  Batch 25,100  of  25,856.    Elapsed: 3:32:07.\n","  Batch 25,200  of  25,856.    Elapsed: 3:32:57.\n","  Batch 25,300  of  25,856.    Elapsed: 3:33:48.\n","  Batch 25,400  of  25,856.    Elapsed: 3:34:38.\n","  Batch 25,500  of  25,856.    Elapsed: 3:35:28.\n","  Batch 25,600  of  25,856.    Elapsed: 3:36:19.\n","  Batch 25,700  of  25,856.    Elapsed: 3:37:09.\n","  Batch 25,800  of  25,856.    Elapsed: 3:38:00.\n","\n","  Average training loss: 0.68\n","  Training epcoh took: 3:38:28\n","\n","Running Validation...\n","  Accuracy: 0.57\n","  Validation Loss: 0.68\n","  Validation took: 0:07:47\n","\n","======== Epoch 2 / 2 ========\n","Training...\n","  Batch   100  of  25,856.    Elapsed: 0:00:50.\n","  Batch   200  of  25,856.    Elapsed: 0:01:41.\n","  Batch   300  of  25,856.    Elapsed: 0:02:31.\n","  Batch   400  of  25,856.    Elapsed: 0:03:22.\n","  Batch   500  of  25,856.    Elapsed: 0:04:12.\n","  Batch   600  of  25,856.    Elapsed: 0:05:02.\n","  Batch   700  of  25,856.    Elapsed: 0:05:53.\n","  Batch   800  of  25,856.    Elapsed: 0:06:43.\n","  Batch   900  of  25,856.    Elapsed: 0:07:34.\n","  Batch 1,000  of  25,856.    Elapsed: 0:08:24.\n","  Batch 1,100  of  25,856.    Elapsed: 0:09:14.\n","  Batch 1,200  of  25,856.    Elapsed: 0:10:05.\n","  Batch 1,300  of  25,856.    Elapsed: 0:10:55.\n","  Batch 1,400  of  25,856.    Elapsed: 0:11:46.\n","  Batch 1,500  of  25,856.    Elapsed: 0:12:36.\n","  Batch 1,600  of  25,856.    Elapsed: 0:13:27.\n","  Batch 1,700  of  25,856.    Elapsed: 0:14:17.\n","  Batch 1,800  of  25,856.    Elapsed: 0:15:08.\n","  Batch 1,900  of  25,856.    Elapsed: 0:15:58.\n","  Batch 2,000  of  25,856.    Elapsed: 0:16:49.\n","  Batch 2,100  of  25,856.    Elapsed: 0:17:39.\n","  Batch 2,200  of  25,856.    Elapsed: 0:18:30.\n","  Batch 2,300  of  25,856.    Elapsed: 0:19:20.\n","  Batch 2,400  of  25,856.    Elapsed: 0:20:10.\n","  Batch 2,500  of  25,856.    Elapsed: 0:21:01.\n","  Batch 2,600  of  25,856.    Elapsed: 0:21:51.\n","  Batch 2,700  of  25,856.    Elapsed: 0:22:42.\n","  Batch 2,800  of  25,856.    Elapsed: 0:23:32.\n","  Batch 2,900  of  25,856.    Elapsed: 0:24:23.\n","  Batch 3,000  of  25,856.    Elapsed: 0:25:13.\n","  Batch 3,100  of  25,856.    Elapsed: 0:26:03.\n","  Batch 3,200  of  25,856.    Elapsed: 0:26:54.\n","  Batch 3,300  of  25,856.    Elapsed: 0:27:44.\n","  Batch 3,400  of  25,856.    Elapsed: 0:28:35.\n","  Batch 3,500  of  25,856.    Elapsed: 0:29:25.\n","  Batch 3,600  of  25,856.    Elapsed: 0:30:16.\n","  Batch 3,700  of  25,856.    Elapsed: 0:31:06.\n","  Batch 3,800  of  25,856.    Elapsed: 0:31:57.\n","  Batch 3,900  of  25,856.    Elapsed: 0:32:47.\n","  Batch 4,000  of  25,856.    Elapsed: 0:33:37.\n","  Batch 4,100  of  25,856.    Elapsed: 0:34:28.\n","  Batch 4,200  of  25,856.    Elapsed: 0:35:18.\n","  Batch 4,300  of  25,856.    Elapsed: 0:36:09.\n","  Batch 4,400  of  25,856.    Elapsed: 0:37:00.\n","  Batch 4,500  of  25,856.    Elapsed: 0:37:50.\n","  Batch 4,600  of  25,856.    Elapsed: 0:38:41.\n","  Batch 4,700  of  25,856.    Elapsed: 0:39:32.\n","  Batch 4,800  of  25,856.    Elapsed: 0:40:22.\n","  Batch 4,900  of  25,856.    Elapsed: 0:41:13.\n","  Batch 5,000  of  25,856.    Elapsed: 0:42:04.\n","  Batch 5,100  of  25,856.    Elapsed: 0:42:54.\n","  Batch 5,200  of  25,856.    Elapsed: 0:43:45.\n","  Batch 5,300  of  25,856.    Elapsed: 0:44:36.\n","  Batch 5,400  of  25,856.    Elapsed: 0:45:26.\n","  Batch 5,500  of  25,856.    Elapsed: 0:46:17.\n","  Batch 5,600  of  25,856.    Elapsed: 0:47:07.\n","  Batch 5,700  of  25,856.    Elapsed: 0:47:58.\n","  Batch 5,800  of  25,856.    Elapsed: 0:48:48.\n","  Batch 5,900  of  25,856.    Elapsed: 0:49:39.\n","  Batch 6,000  of  25,856.    Elapsed: 0:50:30.\n","  Batch 6,100  of  25,856.    Elapsed: 0:51:20.\n","  Batch 6,200  of  25,856.    Elapsed: 0:52:10.\n","  Batch 6,300  of  25,856.    Elapsed: 0:53:01.\n","  Batch 6,400  of  25,856.    Elapsed: 0:53:51.\n","  Batch 6,500  of  25,856.    Elapsed: 0:54:42.\n","  Batch 6,600  of  25,856.    Elapsed: 0:55:32.\n","  Batch 6,700  of  25,856.    Elapsed: 0:56:23.\n","  Batch 6,800  of  25,856.    Elapsed: 0:57:13.\n","  Batch 6,900  of  25,856.    Elapsed: 0:58:03.\n","  Batch 7,000  of  25,856.    Elapsed: 0:58:54.\n","  Batch 7,100  of  25,856.    Elapsed: 0:59:45.\n","  Batch 7,200  of  25,856.    Elapsed: 1:00:35.\n","  Batch 7,300  of  25,856.    Elapsed: 1:01:26.\n","  Batch 7,400  of  25,856.    Elapsed: 1:02:17.\n","  Batch 7,500  of  25,856.    Elapsed: 1:03:08.\n","  Batch 7,600  of  25,856.    Elapsed: 1:03:59.\n","  Batch 7,700  of  25,856.    Elapsed: 1:04:50.\n","  Batch 7,800  of  25,856.    Elapsed: 1:05:40.\n","  Batch 7,900  of  25,856.    Elapsed: 1:06:31.\n","  Batch 8,000  of  25,856.    Elapsed: 1:07:22.\n","  Batch 8,100  of  25,856.    Elapsed: 1:08:13.\n","  Batch 8,200  of  25,856.    Elapsed: 1:09:04.\n","  Batch 8,300  of  25,856.    Elapsed: 1:09:55.\n","  Batch 8,400  of  25,856.    Elapsed: 1:10:46.\n","  Batch 8,500  of  25,856.    Elapsed: 1:11:37.\n","  Batch 8,600  of  25,856.    Elapsed: 1:12:28.\n","  Batch 8,700  of  25,856.    Elapsed: 1:13:19.\n","  Batch 8,800  of  25,856.    Elapsed: 1:14:10.\n","  Batch 8,900  of  25,856.    Elapsed: 1:15:01.\n","  Batch 9,000  of  25,856.    Elapsed: 1:15:52.\n","  Batch 9,100  of  25,856.    Elapsed: 1:16:43.\n","  Batch 9,200  of  25,856.    Elapsed: 1:17:34.\n","  Batch 9,300  of  25,856.    Elapsed: 1:18:25.\n","  Batch 9,400  of  25,856.    Elapsed: 1:19:16.\n","  Batch 9,500  of  25,856.    Elapsed: 1:20:07.\n","  Batch 9,600  of  25,856.    Elapsed: 1:20:58.\n","  Batch 9,700  of  25,856.    Elapsed: 1:21:49.\n","  Batch 9,800  of  25,856.    Elapsed: 1:22:40.\n","  Batch 9,900  of  25,856.    Elapsed: 1:23:31.\n","  Batch 10,000  of  25,856.    Elapsed: 1:24:22.\n","  Batch 10,100  of  25,856.    Elapsed: 1:25:13.\n","  Batch 10,200  of  25,856.    Elapsed: 1:26:04.\n","  Batch 10,300  of  25,856.    Elapsed: 1:26:55.\n","  Batch 10,400  of  25,856.    Elapsed: 1:27:46.\n","  Batch 10,500  of  25,856.    Elapsed: 1:28:37.\n","  Batch 10,600  of  25,856.    Elapsed: 1:29:28.\n","  Batch 10,700  of  25,856.    Elapsed: 1:30:19.\n","  Batch 10,800  of  25,856.    Elapsed: 1:31:09.\n","  Batch 10,900  of  25,856.    Elapsed: 1:32:00.\n","  Batch 11,000  of  25,856.    Elapsed: 1:32:51.\n","  Batch 11,100  of  25,856.    Elapsed: 1:33:42.\n","  Batch 11,200  of  25,856.    Elapsed: 1:34:32.\n","  Batch 11,300  of  25,856.    Elapsed: 1:35:23.\n","  Batch 11,400  of  25,856.    Elapsed: 1:36:14.\n","  Batch 11,500  of  25,856.    Elapsed: 1:37:05.\n","  Batch 11,600  of  25,856.    Elapsed: 1:37:56.\n","  Batch 11,700  of  25,856.    Elapsed: 1:38:47.\n","  Batch 11,800  of  25,856.    Elapsed: 1:39:38.\n","  Batch 11,900  of  25,856.    Elapsed: 1:40:29.\n","  Batch 12,000  of  25,856.    Elapsed: 1:41:20.\n","  Batch 12,100  of  25,856.    Elapsed: 1:42:10.\n","  Batch 12,200  of  25,856.    Elapsed: 1:43:01.\n","  Batch 12,300  of  25,856.    Elapsed: 1:43:52.\n","  Batch 12,400  of  25,856.    Elapsed: 1:44:42.\n","  Batch 12,500  of  25,856.    Elapsed: 1:45:33.\n","  Batch 12,600  of  25,856.    Elapsed: 1:46:24.\n","  Batch 12,700  of  25,856.    Elapsed: 1:47:14.\n","  Batch 12,800  of  25,856.    Elapsed: 1:48:05.\n","  Batch 12,900  of  25,856.    Elapsed: 1:48:56.\n","  Batch 13,000  of  25,856.    Elapsed: 1:49:47.\n","  Batch 13,100  of  25,856.    Elapsed: 1:50:37.\n","  Batch 13,200  of  25,856.    Elapsed: 1:51:28.\n","  Batch 13,300  of  25,856.    Elapsed: 1:52:19.\n","  Batch 13,400  of  25,856.    Elapsed: 1:53:09.\n","  Batch 13,500  of  25,856.    Elapsed: 1:54:00.\n","  Batch 13,600  of  25,856.    Elapsed: 1:54:51.\n","  Batch 13,700  of  25,856.    Elapsed: 1:55:41.\n","  Batch 13,800  of  25,856.    Elapsed: 1:56:32.\n","  Batch 13,900  of  25,856.    Elapsed: 1:57:23.\n","  Batch 14,000  of  25,856.    Elapsed: 1:58:13.\n","  Batch 14,100  of  25,856.    Elapsed: 1:59:04.\n","  Batch 14,200  of  25,856.    Elapsed: 1:59:54.\n","  Batch 14,300  of  25,856.    Elapsed: 2:00:45.\n","  Batch 14,400  of  25,856.    Elapsed: 2:01:36.\n","  Batch 14,500  of  25,856.    Elapsed: 2:02:27.\n","  Batch 14,600  of  25,856.    Elapsed: 2:03:17.\n","  Batch 14,700  of  25,856.    Elapsed: 2:04:08.\n","  Batch 14,800  of  25,856.    Elapsed: 2:04:59.\n","  Batch 14,900  of  25,856.    Elapsed: 2:05:49.\n","  Batch 15,000  of  25,856.    Elapsed: 2:06:40.\n","  Batch 15,100  of  25,856.    Elapsed: 2:07:30.\n","  Batch 15,200  of  25,856.    Elapsed: 2:08:21.\n","  Batch 15,300  of  25,856.    Elapsed: 2:09:11.\n","  Batch 15,400  of  25,856.    Elapsed: 2:10:02.\n","  Batch 15,500  of  25,856.    Elapsed: 2:10:53.\n","  Batch 15,600  of  25,856.    Elapsed: 2:11:43.\n","  Batch 15,700  of  25,856.    Elapsed: 2:12:34.\n","  Batch 15,800  of  25,856.    Elapsed: 2:13:24.\n","  Batch 15,900  of  25,856.    Elapsed: 2:14:15.\n","  Batch 16,000  of  25,856.    Elapsed: 2:15:06.\n","  Batch 16,100  of  25,856.    Elapsed: 2:15:56.\n","  Batch 16,200  of  25,856.    Elapsed: 2:16:47.\n","  Batch 16,300  of  25,856.    Elapsed: 2:17:37.\n","  Batch 16,400  of  25,856.    Elapsed: 2:18:28.\n","  Batch 16,500  of  25,856.    Elapsed: 2:19:18.\n","  Batch 16,600  of  25,856.    Elapsed: 2:20:09.\n","  Batch 16,700  of  25,856.    Elapsed: 2:21:00.\n","  Batch 16,800  of  25,856.    Elapsed: 2:21:50.\n","  Batch 16,900  of  25,856.    Elapsed: 2:22:41.\n","  Batch 17,000  of  25,856.    Elapsed: 2:23:31.\n","  Batch 17,100  of  25,856.    Elapsed: 2:24:22.\n","  Batch 17,200  of  25,856.    Elapsed: 2:25:13.\n","  Batch 17,300  of  25,856.    Elapsed: 2:26:03.\n","  Batch 17,400  of  25,856.    Elapsed: 2:26:54.\n","  Batch 17,500  of  25,856.    Elapsed: 2:27:44.\n","  Batch 17,600  of  25,856.    Elapsed: 2:28:35.\n","  Batch 17,700  of  25,856.    Elapsed: 2:29:26.\n","  Batch 17,800  of  25,856.    Elapsed: 2:30:16.\n","  Batch 17,900  of  25,856.    Elapsed: 2:31:07.\n","  Batch 18,000  of  25,856.    Elapsed: 2:31:58.\n","  Batch 18,100  of  25,856.    Elapsed: 2:32:49.\n","  Batch 18,200  of  25,856.    Elapsed: 2:33:39.\n","  Batch 18,300  of  25,856.    Elapsed: 2:34:30.\n","  Batch 18,400  of  25,856.    Elapsed: 2:35:21.\n","  Batch 18,500  of  25,856.    Elapsed: 2:36:12.\n","  Batch 18,600  of  25,856.    Elapsed: 2:37:02.\n","  Batch 18,700  of  25,856.    Elapsed: 2:37:53.\n","  Batch 18,800  of  25,856.    Elapsed: 2:38:44.\n","  Batch 18,900  of  25,856.    Elapsed: 2:39:35.\n","  Batch 19,000  of  25,856.    Elapsed: 2:40:25.\n","  Batch 19,100  of  25,856.    Elapsed: 2:41:16.\n","  Batch 19,200  of  25,856.    Elapsed: 2:42:07.\n","  Batch 19,300  of  25,856.    Elapsed: 2:42:58.\n","  Batch 19,400  of  25,856.    Elapsed: 2:43:49.\n","  Batch 19,500  of  25,856.    Elapsed: 2:44:40.\n","  Batch 19,600  of  25,856.    Elapsed: 2:45:31.\n","  Batch 19,700  of  25,856.    Elapsed: 2:46:21.\n","  Batch 19,800  of  25,856.    Elapsed: 2:47:12.\n","  Batch 19,900  of  25,856.    Elapsed: 2:48:03.\n","  Batch 20,000  of  25,856.    Elapsed: 2:48:54.\n","  Batch 20,100  of  25,856.    Elapsed: 2:49:45.\n","  Batch 20,200  of  25,856.    Elapsed: 2:50:35.\n","  Batch 20,300  of  25,856.    Elapsed: 2:51:26.\n","  Batch 20,400  of  25,856.    Elapsed: 2:52:17.\n","  Batch 20,500  of  25,856.    Elapsed: 2:53:07.\n","  Batch 20,600  of  25,856.    Elapsed: 2:53:58.\n","  Batch 20,700  of  25,856.    Elapsed: 2:54:49.\n","  Batch 20,800  of  25,856.    Elapsed: 2:55:39.\n","  Batch 20,900  of  25,856.    Elapsed: 2:56:30.\n","  Batch 21,000  of  25,856.    Elapsed: 2:57:21.\n","  Batch 21,100  of  25,856.    Elapsed: 2:58:11.\n","  Batch 21,200  of  25,856.    Elapsed: 2:59:02.\n","  Batch 21,300  of  25,856.    Elapsed: 2:59:53.\n","  Batch 21,400  of  25,856.    Elapsed: 3:00:44.\n","  Batch 21,500  of  25,856.    Elapsed: 3:01:34.\n","  Batch 21,600  of  25,856.    Elapsed: 3:02:25.\n","  Batch 21,700  of  25,856.    Elapsed: 3:03:15.\n","  Batch 21,800  of  25,856.    Elapsed: 3:04:06.\n","  Batch 21,900  of  25,856.    Elapsed: 3:04:57.\n","  Batch 22,000  of  25,856.    Elapsed: 3:05:47.\n","  Batch 22,100  of  25,856.    Elapsed: 3:06:38.\n","  Batch 22,200  of  25,856.    Elapsed: 3:07:28.\n","  Batch 22,300  of  25,856.    Elapsed: 3:08:19.\n","  Batch 22,400  of  25,856.    Elapsed: 3:09:10.\n","  Batch 22,500  of  25,856.    Elapsed: 3:10:00.\n","  Batch 22,600  of  25,856.    Elapsed: 3:10:51.\n","  Batch 22,700  of  25,856.    Elapsed: 3:11:42.\n","  Batch 22,800  of  25,856.    Elapsed: 3:12:32.\n","  Batch 22,900  of  25,856.    Elapsed: 3:13:23.\n","  Batch 23,000  of  25,856.    Elapsed: 3:14:14.\n","  Batch 23,100  of  25,856.    Elapsed: 3:15:04.\n","  Batch 23,200  of  25,856.    Elapsed: 3:15:55.\n","  Batch 23,300  of  25,856.    Elapsed: 3:16:46.\n","  Batch 23,400  of  25,856.    Elapsed: 3:17:36.\n","  Batch 23,500  of  25,856.    Elapsed: 3:18:27.\n","  Batch 23,600  of  25,856.    Elapsed: 3:19:18.\n","  Batch 23,700  of  25,856.    Elapsed: 3:20:08.\n","  Batch 23,800  of  25,856.    Elapsed: 3:20:59.\n","  Batch 23,900  of  25,856.    Elapsed: 3:21:50.\n","  Batch 24,000  of  25,856.    Elapsed: 3:22:40.\n","  Batch 24,100  of  25,856.    Elapsed: 3:23:31.\n","  Batch 24,200  of  25,856.    Elapsed: 3:24:21.\n","  Batch 24,300  of  25,856.    Elapsed: 3:25:12.\n","  Batch 24,400  of  25,856.    Elapsed: 3:26:03.\n","  Batch 24,500  of  25,856.    Elapsed: 3:26:53.\n","  Batch 24,600  of  25,856.    Elapsed: 3:27:44.\n","  Batch 24,700  of  25,856.    Elapsed: 3:28:35.\n","  Batch 24,800  of  25,856.    Elapsed: 3:29:25.\n","  Batch 24,900  of  25,856.    Elapsed: 3:30:16.\n","  Batch 25,000  of  25,856.    Elapsed: 3:31:07.\n","  Batch 25,100  of  25,856.    Elapsed: 3:31:57.\n","  Batch 25,200  of  25,856.    Elapsed: 3:32:48.\n","  Batch 25,300  of  25,856.    Elapsed: 3:33:39.\n","  Batch 25,400  of  25,856.    Elapsed: 3:34:29.\n","  Batch 25,500  of  25,856.    Elapsed: 3:35:20.\n","  Batch 25,600  of  25,856.    Elapsed: 3:36:11.\n","  Batch 25,700  of  25,856.    Elapsed: 3:37:02.\n","  Batch 25,800  of  25,856.    Elapsed: 3:37:52.\n","\n","  Average training loss: 0.67\n","  Training epcoh took: 3:38:20\n","\n","Running Validation...\n","  Accuracy: 0.57\n","  Validation Loss: 0.68\n","  Validation took: 0:07:48\n","\n","Training complete!\n","Total training took 7:32:23 (h:mm:ss)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"IkkJVAMfQSWI"},"source":["#Saving Fine-tuned Model"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OhaxUqXm4aza","outputId":"7be335bd-35a3-47ef-8f8a-c4f237aff411"},"source":["import os\n","#defining output directory\n","\n","output_dir = '/content/drive/MyDrive/BERT-Final/2Year_Binary/'\n","\n","# Create output directory if needed\n","if not os.path.exists(output_dir):\n","    os.makedirs(output_dir)\n","\n","print(\"Saving model to %s\" % output_dir)\n","\n","# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n","# They can then be reloaded using `from_pretrained()`\n","model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n","model_to_save.save_pretrained(output_dir)\n","tokenizer.save_pretrained(output_dir)\n","\n","# Good practice: save your training arguments together with the trained model\n","# torch.save(args, os.path.join(output_dir, 'training_args.bin'))\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Saving model to /content/drive/MyDrive/BERT-Final/2Year_Binary/\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["('/content/drive/MyDrive/BERT-Final/2Year_Binary/tokenizer_config.json',\n"," '/content/drive/MyDrive/BERT-Final/2Year_Binary/special_tokens_map.json',\n"," '/content/drive/MyDrive/BERT-Final/2Year_Binary/vocab.txt',\n"," '/content/drive/MyDrive/BERT-Final/2Year_Binary/added_tokens.json')"]},"metadata":{"tags":[]},"execution_count":28}]},{"cell_type":"code","metadata":{"id":"dSkxRVOAAx0W"},"source":["model_save_name = 'bert_model_2year.bin'\n","path = F\"/content/drive/MyDrive/BERT-Final/2Year_Binary/\"+model_save_name\n","torch.save(model.state_dict(), path)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d3Was9ISY5NA"},"source":["###Testing"]},{"cell_type":"code","metadata":{"id":"aHpMvJw-HsA3","colab":{"base_uri":"https://localhost:8080/"},"outputId":"954d116f-f732-4fed-b016-78ac9b72f3a3"},"source":["# Load a trained model and vocabulary that you have fine-tuned\n","from transformers import BertModel, BertForSequenceClassification, AdamW, BertConfig\n","from transformers import BertTokenizer\n","\n","output_dir = '/content/drive/MyDrive/BERT-Final/2Year_Binary/'\n","\n","model = BertForSequenceClassification.from_pretrained(output_dir)\n","tokenizer = BertTokenizer.from_pretrained(output_dir)\n","model.to(device)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["BertForSequenceClassification(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"code","metadata":{"id":"gq03DBSXIYzK"},"source":["labels = y_test"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SjfQSVN3Y9Zi","outputId":"76502eb8-1d99-4535-f00b-4530274e287b"},"source":["import pandas as pd\n","import torch\n","from torch.utils.data import TensorDataset, random_split\n","from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","\n","print('Number of test sentences: {:,}\\n'.format(df_test.shape[0]))\n","\n","# Create sentence and label lists\n","test_sentences = X_test\n","test_labels = y_test\n","\n","input_ids = []\n","attention_masks = []\n","\n","# For each sentence encode_plus would tokenize, add special tokens ([CLS], [SEP]), map tokens to IDs, create attention masks\n","for sent in test_sentences:\n","   \n","    encoded_dict = tokenizer.encode_plus(\n","                        sent,                      \n","                        add_special_tokens = True, \n","                        max_length = 160,          \n","                        pad_to_max_length = True,\n","                        return_attention_mask = True,  \n","                        return_tensors = 'pt',     \n","                   )\n","    \n","   \n","    input_ids.append(encoded_dict['input_ids'])\n","    attention_masks.append(encoded_dict['attention_mask'])\n","\n","input_ids = torch.cat(input_ids, dim=0)\n","attention_masks = torch.cat(attention_masks, dim=0)\n","labels = torch.tensor(labels)\n","batch_size = 32  \n","prediction_data = TensorDataset(input_ids, attention_masks, labels)\n","prediction_sampler = SequentialSampler(prediction_data)\n","prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"],"name":"stderr"},{"output_type":"stream","text":["Number of test sentences: 102,145\n","\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EHkPWxlYInq-","outputId":"2793837d-77b7-4a7e-d2a2-736af0318426"},"source":["\n","print('Predicting labels for {:,} test sentences...'.format(len(input_ids)))\n","model.eval()\n","predictions , true_labels = [], []\n","t0 = time.time()\n","for step, batch in enumerate(prediction_dataloader):\n","  if step % 100 == 0 and not step == 0:\n","      elapsed = format_time(time.time() - t0)\n","      print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(prediction_dataloader), elapsed))\n","\n","  batch = tuple(t for t in batch)\n","  \n"," \n","  b_input_ids = batch[0].to(device)\n","  b_input_mask = batch[1].to(device)\n","  b_labels = batch[2].to(device)\n","  \n","\n","  with torch.no_grad():\n","      outputs = model(b_input_ids, token_type_ids=None, \n","                      attention_mask=b_input_mask)\n","\n","  logits = outputs[0]\n","  logits = logits.detach().cpu().numpy()\n","  label_ids = b_labels.to('cpu').numpy()\n","  predictions.append(logits)\n","  true_labels.append(label_ids)\n","\n","print('    DONE.')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Predicting labels for 102,145 test sentences...\n","  Batch   100  of  3,193.    Elapsed: 0:00:28.\n","  Batch   200  of  3,193.    Elapsed: 0:00:58.\n","  Batch   300  of  3,193.    Elapsed: 0:01:29.\n","  Batch   400  of  3,193.    Elapsed: 0:02:00.\n","  Batch   500  of  3,193.    Elapsed: 0:02:32.\n","  Batch   600  of  3,193.    Elapsed: 0:03:06.\n","  Batch   700  of  3,193.    Elapsed: 0:03:39.\n","  Batch   800  of  3,193.    Elapsed: 0:04:12.\n","  Batch   900  of  3,193.    Elapsed: 0:04:45.\n","  Batch 1,000  of  3,193.    Elapsed: 0:05:18.\n","  Batch 1,100  of  3,193.    Elapsed: 0:05:51.\n","  Batch 1,200  of  3,193.    Elapsed: 0:06:24.\n","  Batch 1,300  of  3,193.    Elapsed: 0:06:57.\n","  Batch 1,400  of  3,193.    Elapsed: 0:07:30.\n","  Batch 1,500  of  3,193.    Elapsed: 0:08:03.\n","  Batch 1,600  of  3,193.    Elapsed: 0:08:36.\n","  Batch 1,700  of  3,193.    Elapsed: 0:09:09.\n","  Batch 1,800  of  3,193.    Elapsed: 0:09:42.\n","  Batch 1,900  of  3,193.    Elapsed: 0:10:15.\n","  Batch 2,000  of  3,193.    Elapsed: 0:10:48.\n","  Batch 2,100  of  3,193.    Elapsed: 0:11:21.\n","  Batch 2,200  of  3,193.    Elapsed: 0:11:54.\n","  Batch 2,300  of  3,193.    Elapsed: 0:12:28.\n","  Batch 2,400  of  3,193.    Elapsed: 0:13:01.\n","  Batch 2,500  of  3,193.    Elapsed: 0:13:34.\n","  Batch 2,600  of  3,193.    Elapsed: 0:14:07.\n","  Batch 2,700  of  3,193.    Elapsed: 0:14:40.\n","  Batch 2,800  of  3,193.    Elapsed: 0:15:13.\n","  Batch 2,900  of  3,193.    Elapsed: 0:15:46.\n","  Batch 3,000  of  3,193.    Elapsed: 0:16:19.\n","  Batch 3,100  of  3,193.    Elapsed: 0:16:52.\n","    DONE.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"4RcGE9jzRAaU"},"source":["#Classification Report "]},{"cell_type":"code","metadata":{"id":"MGVHxKxvJTBt","colab":{"base_uri":"https://localhost:8080/"},"outputId":"276fae63-037e-4c3d-e970-8a658fb0e153"},"source":["import numpy as np\n","from sklearn.metrics import precision_recall_fscore_support\n","flat_predictions = np.concatenate(predictions, axis=0)\n","\n","# For each sample, pick the label (0 or 1) with the higher score.\n","flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n","\n","# Combine the correct labels for each batch into a single list.\n","flat_true_labels = np.concatenate(true_labels, axis=0)\n","\n","print(\"F1 score: {}\".format(precision_recall_fscore_support(flat_predictions, flat_true_labels, average='macro')))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["F1 score: (0.5933196066624191, 0.6043964133974533, 0.5883778879480615, None)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"AbFyHnaOJVzB","colab":{"base_uri":"https://localhost:8080/"},"outputId":"879f2c9d-40e2-413c-e6c5-a5f43c3dfb94"},"source":["from sklearn.metrics import classification_report\n","print(classification_report(flat_true_labels, flat_predictions,zero_division=1))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.60      0.43      0.50     47030\n","           1       0.61      0.75      0.67     55115\n","\n","    accuracy                           0.61    102145\n","   macro avg       0.60      0.59      0.59    102145\n","weighted avg       0.60      0.61      0.60    102145\n","\n"],"name":"stdout"}]}]}