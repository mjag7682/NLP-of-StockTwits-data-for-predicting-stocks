{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Preprocessing.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"HkqDSyOVDpRD"},"source":["## Imports and Installs"]},{"cell_type":"code","metadata":{"id":"_gXtdFlzndWh"},"source":["!pip install contractions\n","!pip install emoji\n","!pip install ekphrasis\n","\n","import math\n","import pandas as pd\n","import re\n","import emoji\n","import nltk\n","import contractions\n","import torch\n","\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","from nltk.tokenize import TweetTokenizer \n","from nltk.corpus import stopwords as sw\n","from nltk.tokenize import word_tokenize \n","from nltk.tokenize.treebank import TreebankWordDetokenizer\n","import requests \n","from pprint import pprint\n","import numpy as np\n","from nltk.stem import PorterStemmer\n","\n","import tensorflow as tf\n","import tensorflow_hub as hub\n","from datetime import datetime\n","from tensorflow import keras\n","\n","import os\n","import pprint\n","import json\n","import random\n","import string\n","import sys\n","from ekphrasis.classes.segmenter import Segmenter\n","import itertools"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vFOsh5pJq2QZ","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"c516c542-f0c2-4583-bbb3-be46124a21b9"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"T1O0ewkADyRM"},"source":["## Load file"]},{"cell_type":"code","metadata":{"id":"BVXGLwXaq5I5"},"source":["df_comb = pd.read_csv('/content/drive/My Drive/Labelled (Binary - previous day open close)/labelled_WMT.csv')\n","combine_ds = df_comb.sample(frac=1)\n","combine_ds"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B8G7WU8SD08e"},"source":["## Preprocess data"]},{"cell_type":"markdown","metadata":{"id":"Kf4ryxOzF2L7"},"source":["Fill NaN messages with '0', which will be dropped later\r\n"]},{"cell_type":"code","metadata":{"id":"-de4QdxRK2N7"},"source":["combine_ds.isna().values.any()\n","combine_ds['message'] = combine_ds['message'].fillna('0')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1_CB61cDF4HC"},"source":["Case-folding\r\n"]},{"cell_type":"code","metadata":{"id":"z70r1eYrnoB_"},"source":["combine_ds['message'] = combine_ds['message'].str.lower()\n","message = combine_ds['message'].tolist()\n","print(message[:10])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rp4bT3ZgF5K-"},"source":["Functions definitions"]},{"cell_type":"code","metadata":{"id":"C65MA9lJnwEv"},"source":["# Function to remove stopwords\n","def remove_stopwords(msg):\n","    filtered_sentence = [w for w in msg_tokens if not w in stop_words]\n","    return filtered_sentence\n","\n","# Function to remove punctuations, URLs, Mentions, Hashtags and _ from emojis\n","def remove_punctuation_re(x):\n","    x = ' '.join(re.sub(\"https?://\\S+\",\"\",x).split())     # Removing URLs\n","    x = ' '.join(re.sub(\"^@\\S+|\\s@\\S+\",\"\",x).split())     # Removing Mentions\n","    x = ' '.join(re.sub(r'[^\\w\\s]',\" \",x).split())        # Removes Hashtags\n","    x = ' '.join(re.sub(r'_',\" \",x).split())              # Removing _ from emojis text\n","    return x\n","\n","# Function to replace repeating letter\n","def rpt_replace(match):\n","    return match.group(1)+match.group(1)\n","\n","# Function to substitute original word with replaced word, if any\n","def processRepeatings(data):\n","    re_t= re.sub(message_rpt, rpt_replace, data )\n","    return re_t"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_3YWRwzLBGs1"},"source":["# Download stopwords\n","stop_words = sw.words(\"english\")\n","\n","# Create tokenizer and detokenizer\n","tweet_tokenizer = TweetTokenizer()\n","detokenizer = TreebankWordDetokenizer()\n","\n","# For repeating characters in words\n","message_rpt = re.compile(r\"(.)\\1{2,}\", re.IGNORECASE)\n","\n","# Segmenter using the word statistics from Twitter\n","seg_tw = Segmenter(corpus=\"twitter\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4LRFAxgcn_fi"},"source":["message_p = []\n","count = 0\n","\n","for msg in message:\n","    print(count)\n","    count += 1\n","\n","    if msg == '0': # NaN replaced by '0'\n","      message_p.append('-1')\n","\n","    else:\n","      # remove emojis\n","      msg = emoji.demojize(msg)\n","\n","      # fix contractions\n","      msg = contractions.fix(msg)\n","\n","      # remove punctuations\n","      msg = remove_punctuation_re(msg) \n","\n","      # tokenize\n","      msg_tokens = tweet_tokenizer.tokenize(msg)\n","\n","      # For compressing elongated words using Word segmenter\n","      message_seg = []\n","      for w in msg_tokens:\n","        if len(w) >= 300: # truncate word to 100 characters if it's original length is more than 300\n","          w = w[:100]\n","          print(w)\n","        message_seg.append(seg_tw.segment(w))\n","\n","      # remove stopwords\n","      msg = remove_stopwords(message_seg)\n","\n","      if 'rt' in msg:\n","        # remove retweets\n","        message_p.append('-1')\n","      else: \n","        # detokenize\n","        msg = detokenizer.detokenize(msg)\n","\n","        # removing repeating characters like hurrrryyyyyy-- worrks on tokenized list\n","        msg = processRepeatings(msg)\n","\n","        message_p.append(msg)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pXWjHAcQoIEh"},"source":["message_p[:10]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W4g9FpDaF9LF"},"source":["Assign preprocessed message list to dataframe\r\n"]},{"cell_type":"code","metadata":{"id":"KkSpMpJWAG4u"},"source":["combine_ds['message'] = message_p\n","combine_ds"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"y7uAQuWIShF5"},"source":["combine_ds['polarity'].value_counts()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XEGW2nGeF-zh"},"source":["Drop Retweets\r\n"]},{"cell_type":"code","metadata":{"id":"3DraQK_uDcBU"},"source":["combine_ds.drop(combine_ds[combine_ds['message'] == '-1'].index, inplace = True) \n","combine_ds"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cKmZ0dT6SpsG"},"source":["combine_ds['polarity'].value_counts()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hkWouc77MGAE"},"source":["## Special handling for FB and TSLA"]},{"cell_type":"markdown","metadata":{"id":"j67Om93CGAcc"},"source":["For FB, drop tweets before IPO date\r\n"]},{"cell_type":"code","metadata":{"id":"XGkgKebCsDXh"},"source":["# FB_IPO = '2012-05-21'\n","# combine_ds.drop(combine_ds[combine_ds['Date'] < '2012-05-21'].index, inplace = True) \n","# combine_ds"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ccVdOXS6GCJl"},"source":["For TSLA, drop tweets before IPO date\r\n"]},{"cell_type":"code","metadata":{"id":"NPVbjN18s5G0"},"source":["# TSLA_IPO = '2010-06-30'\n","# combine_ds.drop(combine_ds[combine_ds['Date'] < TSLA_IPO].index, inplace = True) \n","# combine_ds"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TzydduNOvTuG"},"source":["combine_ds['polarity'].value_counts()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iA-bge_7MIuZ"},"source":["## Save preprocessed company file "]},{"cell_type":"code","metadata":{"id":"R3mJMzRESrdf"},"source":["combine_ds.to_csv('labelled_WMT.csv')\n","!cp labelled_WMT.csv \"/content/drive/My Drive/Prep Labelled (binary - previous day)\""],"execution_count":null,"outputs":[]}]}