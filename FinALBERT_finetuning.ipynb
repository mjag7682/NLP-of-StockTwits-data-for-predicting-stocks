{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"FinALBERT_finetuning.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"oehL0ikEcrnO"},"source":["##Libraries"]},{"cell_type":"markdown","metadata":{"id":"hh_DfxkXHQUN"},"source":["Parts of code Refereed from https://www.kaggle.com/photon96/nlp-models-reddit/notebook"]},{"cell_type":"markdown","metadata":{"id":"3OMme0Micwrx"},"source":["Pip Install "]},{"cell_type":"code","metadata":{"id":"CqVQ1YxVMRdJ","colab":{"base_uri":"https://localhost:8080/"},"outputId":"2158f6b1-6da3-4b92-ef00-7a7a60dac63c"},"source":["!pip install tensorflow\n","!pip install transformers\n","# !pip install keras\n","# !pip3 install albert-tensorflow\n","# !pip install torch\n","# !pip install sentencepiece"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: tensorflow in /usr/local/lib/python3.6/dist-packages (2.3.0)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.15.0)\n","Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.2)\n","Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.4.1)\n","Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.6.3)\n","Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.3.0)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.3.0)\n","Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.3.3)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.35.1)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n","Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.10.0)\n","Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.33.2)\n","Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.12.4)\n","Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.10.0)\n","Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.1)\n","Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.3.0)\n","Requirement already satisfied: numpy<1.19.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.18.5)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.9.2->tensorflow) (50.3.2)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.0.1)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (2.23.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (3.3.3)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.7.0)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (0.4.2)\n","Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.17.2)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2020.11.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (3.0.4)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow) (2.0.0)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow) (1.3.0)\n","Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (4.6)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (0.2.8)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (4.1.1)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow) (3.4.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow) (3.1.0)\n","Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (0.4.8)\n","Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/83/e74092e7f24a08d751aa59b37a9fc572b2e4af3918cb66f7766c3affb1b4/transformers-3.5.1-py3-none-any.whl (1.3MB)\n","\u001b[K     |████████████████████████████████| 1.3MB 9.3MB/s \n","\u001b[?25hCollecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n","\u001b[K     |████████████████████████████████| 890kB 28.4MB/s \n","\u001b[?25hCollecting sentencepiece==0.1.91\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n","\u001b[K     |████████████████████████████████| 1.1MB 40.8MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n","Collecting tokenizers==0.9.3\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/34/b39eb9994bc3c999270b69c9eea40ecc6f0e97991dba28282b9fd32d44ee/tokenizers-0.9.3-cp36-cp36m-manylinux1_x86_64.whl (2.9MB)\n","\u001b[K     |████████████████████████████████| 2.9MB 56.4MB/s \n","\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers) (3.12.4)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.17.0)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.11.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (50.3.2)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=aa03800a928dcd48d5bb047c1e453cf2215fa18798b086d0db4f3cf7a6044f46\n","  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n","Successfully built sacremoses\n","Installing collected packages: sacremoses, sentencepiece, tokenizers, transformers\n","Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.9.3 transformers-3.5.1\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"0_G6zZtAMaRT"},"source":["# Check GPU"]},{"cell_type":"code","metadata":{"id":"abttdC5gMa8J","colab":{"base_uri":"https://localhost:8080/"},"outputId":"bb5b6141-ae70-4a90-c732-af58f060d672"},"source":["import tensorflow as tf\n","\n","# Get the GPU device name.\n","device_name = tf.test.gpu_device_name()\n","\n","# The device name should look like the following:\n","if device_name == '/device:GPU:0':\n","    print('Found GPU at: {}'.format(device_name))\n","else:\n","    raise SystemError('GPU device not found')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Found GPU at: /device:GPU:0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"fajd06t0c4fT"},"source":["Assign GPU"]},{"cell_type":"code","metadata":{"id":"ytxYs-jUMc-c","colab":{"base_uri":"https://localhost:8080/"},"outputId":"24377bba-cf3d-4913-96ae-8870c8bd6533"},"source":["import torch\n","\n","# Check if GPU is available then telling PyTorch to use the GPU \n","if torch.cuda.is_available():    \n","    device = torch.device(\"cuda\")\n","    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n","    print('We will use the GPU:', torch.cuda.get_device_name(0))\n","\n","# Make use of CPU\n","else:\n","    print('No GPU available, using the CPU instead.')\n","    device = torch.device(\"cpu\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["There are 1 GPU(s) available.\n","We will use the GPU: Tesla P100-PCIE-16GB\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"6SZr2opcMfS6"},"source":["# Mounting"]},{"cell_type":"code","metadata":{"id":"qjEx8w8rMf7V","colab":{"base_uri":"https://localhost:8080/"},"outputId":"3f1b2b96-c8ee-43bd-dfe9-717628939d1c"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"i8UMnIdBdPPT"},"source":["Loading Imports"]},{"cell_type":"code","metadata":{"id":"XR0FtDFDMudG"},"source":["from tensorflow.python.compiler.tensorrt import trt_convert as trt\n","import tensorflow as tf\n","import pandas as pd\n","import numpy as np\n","from keras.preprocessing.sequence import pad_sequences\n","import torch"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QSihfP_WdWyD"},"source":["Load the pre-trained FinALBERT Model"]},{"cell_type":"code","metadata":{"id":"uu7tk_G8Mx_-","colab":{"base_uri":"https://localhost:8080/"},"outputId":"954880eb-c1b8-44ba-ab33-69ac5e5fca21"},"source":["from transformers import AlbertTokenizer\n","from transformers.modeling_albert import AlbertModel, load_tf_weights_in_albert, AlbertPreTrainedModel\n","from transformers import AlbertForSequenceClassification,AlbertConfig\n","from transformers.tokenization_bert import BertTokenizer\n","import torch.nn as nn\n","from torch.nn import CrossEntropyLoss\n","VOCAB_FILE = \"/content/gdrive/My Drive/ModelFiles/30k-clean.model\" # This is the vocab file output from Build Vocab step\n","tokenizer = AlbertTokenizer.from_pretrained(VOCAB_FILE, do_lower_case=True)\n","config = AlbertConfig.from_json_file('/content/gdrive/My Drive/ModelFiles/albert_config.json') # Loading ALbert Config File\n","config.num_labels = 3\n","model = AlbertForSequenceClassification(config) #Using the transformer ALbert Model\n","model = load_tf_weights_in_albert(model, config,'/content/gdrive/My Drive/ModelFiles/models_recc2_model.ckpt-best.index')\n","print(model)\n","# model.cuda()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Calling AlbertTokenizer.from_pretrained() with the path to a single file or url is deprecated\n"],"name":"stderr"},{"output_type":"stream","text":["AlbertForSequenceClassification(\n","  (albert): AlbertModel(\n","    (embeddings): AlbertEmbeddings(\n","      (word_embeddings): Embedding(30000, 128, padding_idx=0)\n","      (position_embeddings): Embedding(256, 128)\n","      (token_type_embeddings): Embedding(3, 128)\n","      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0, inplace=False)\n","    )\n","    (encoder): AlbertTransformer(\n","      (embedding_hidden_mapping_in): Linear(in_features=128, out_features=768, bias=True)\n","      (albert_layer_groups): ModuleList(\n","        (0): AlbertLayerGroup(\n","          (albert_layers): ModuleList(\n","            (0): AlbertLayer(\n","              (full_layer_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (attention): AlbertAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (attention_dropout): Dropout(p=0, inplace=False)\n","                (output_dropout): Dropout(p=0, inplace=False)\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              )\n","              (ffn): Linear(in_features=768, out_features=3072, bias=True)\n","              (ffn_output): Linear(in_features=3072, out_features=768, bias=True)\n","              (dropout): Dropout(p=0, inplace=False)\n","            )\n","          )\n","        )\n","      )\n","    )\n","    (pooler): Linear(in_features=768, out_features=768, bias=True)\n","    (pooler_activation): Tanh()\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",")\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"3zS8x2rBdsnX"},"source":["#Load the FAANG Dataset"]},{"cell_type":"code","metadata":{"id":"xPB2nwZTQdrp","colab":{"base_uri":"https://localhost:8080/"},"outputId":"cb4c72ee-e452-43f9-9044-d24dbee14786"},"source":["\n","data = pd.read_csv('/content/gdrive/My Drive/Data/Combined_FAANG_percentage_2.2.csv')\n","# data = pd.read_csv('/content/gdrive/My Drive/Data/Combined_FAANG_binary_previous.csv')\n","print(\"Number of training examples {}\".format(len(data)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Number of training examples 2566826\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"dG_w7hXndwff"},"source":["Limitting Dataset to 1 year"]},{"cell_type":"code","metadata":{"id":"tq9iNdXvQjhh","colab":{"base_uri":"https://localhost:8080/","height":589},"outputId":"3515ee38-b147-4246-c3b9-52e5da604db0"},"source":["data.drop(data[data['Date'] < '2019-07-20'].index, inplace = True)\n","data"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>symbol</th>\n","      <th>message</th>\n","      <th>datetime</th>\n","      <th>user</th>\n","      <th>message_id</th>\n","      <th>Date</th>\n","      <th>Time</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>AAPL</td>\n","      <td>peak profit last 6 expired option alerts aapl ...</td>\n","      <td>2020-07-19 09:49:35</td>\n","      <td>1442893</td>\n","      <td>229008387</td>\n","      <td>2020-07-19</td>\n","      <td>09:49:35</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>AAPL</td>\n","      <td>aapl jul 17 382 50 calls option volume 144 44 ...</td>\n","      <td>2020-07-19 09:47:26</td>\n","      <td>1442893</td>\n","      <td>229008357</td>\n","      <td>2020-07-19</td>\n","      <td>09:47:26</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>AAPL</td>\n","      <td>tsla market true bubble territory profitable c...</td>\n","      <td>2020-07-19 09:01:25</td>\n","      <td>1115913</td>\n","      <td>229007569</td>\n","      <td>2020-07-19</td>\n","      <td>09:01:25</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>AAPL</td>\n","      <td>aapl analyzed 26 analysts buy consensus 86 ana...</td>\n","      <td>2020-07-19 08:13:00</td>\n","      <td>47688</td>\n","      <td>229006733</td>\n","      <td>2020-07-19</td>\n","      <td>08:13:00</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>AAPL</td>\n","      <td>aapl new article dogs dow august 4 adopt ignore</td>\n","      <td>2020-07-19 07:54:05</td>\n","      <td>1555408</td>\n","      <td>229006403</td>\n","      <td>2020-07-19</td>\n","      <td>07:54:05</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>2010966</th>\n","      <td>NFLX</td>\n","      <td>nflx bloated big really consolidation since 10...</td>\n","      <td>2019-07-20 00:13:38</td>\n","      <td>1586407</td>\n","      <td>171184891</td>\n","      <td>2019-07-20</td>\n","      <td>00:13:38</td>\n","      <td>-1</td>\n","    </tr>\n","    <tr>\n","      <th>2010967</th>\n","      <td>NFLX</td>\n","      <td>nflx post forever asking proof spent 200k call...</td>\n","      <td>2019-07-20 00:12:03</td>\n","      <td>1557059</td>\n","      <td>171184815</td>\n","      <td>2019-07-20</td>\n","      <td>00:12:03</td>\n","      <td>-1</td>\n","    </tr>\n","    <tr>\n","      <th>2010968</th>\n","      <td>NFLX</td>\n","      <td>nflx uhh short take back 270 27 start shipping...</td>\n","      <td>2019-07-20 00:10:42</td>\n","      <td>217259</td>\n","      <td>171184751</td>\n","      <td>2019-07-20</td>\n","      <td>00:10:42</td>\n","      <td>-1</td>\n","    </tr>\n","    <tr>\n","      <th>2010969</th>\n","      <td>NFLX</td>\n","      <td>nflx sub 300 wednesday</td>\n","      <td>2019-07-20 00:10:33</td>\n","      <td>1428495</td>\n","      <td>171184744</td>\n","      <td>2019-07-20</td>\n","      <td>00:10:33</td>\n","      <td>-1</td>\n","    </tr>\n","    <tr>\n","      <th>2010970</th>\n","      <td>NFLX</td>\n","      <td>nflx eeouch</td>\n","      <td>2019-07-20 00:08:02</td>\n","      <td>1572566</td>\n","      <td>171184607</td>\n","      <td>2019-07-20</td>\n","      <td>00:08:02</td>\n","      <td>-1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>514612 rows × 8 columns</p>\n","</div>"],"text/plain":["        symbol  ... label\n","0         AAPL  ...     1\n","1         AAPL  ...     1\n","2         AAPL  ...     1\n","3         AAPL  ...     1\n","4         AAPL  ...     1\n","...        ...  ...   ...\n","2010966   NFLX  ...    -1\n","2010967   NFLX  ...    -1\n","2010968   NFLX  ...    -1\n","2010969   NFLX  ...    -1\n","2010970   NFLX  ...    -1\n","\n","[514612 rows x 8 columns]"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"h1KLcfk6d2Rs"},"source":["Replacing value of Negative label from -1 to 2"]},{"cell_type":"code","metadata":{"id":"JSaUZIGPQmUK"},"source":["# df_label[\"label\"].replace({-1: 2}, inplace=True) # 0 - neu, 1 - pos, 2 - neg\n","# data.drop(data[data['label'] == 0].index, inplace = True) \n","data[\"label\"].replace({-1: 2}, inplace=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"A6F6lxBsQntM","colab":{"base_uri":"https://localhost:8080/","height":589},"outputId":"594c45ca-9e35-49fd-bd1f-0c7409b57886"},"source":["data = data.sample(frac=1)\n","data"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>symbol</th>\n","      <th>message</th>\n","      <th>datetime</th>\n","      <th>user</th>\n","      <th>message_id</th>\n","      <th>Date</th>\n","      <th>Time</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1945833</th>\n","      <td>NFLX</td>\n","      <td>21 march maddive selloff sector rotation foid ...</td>\n","      <td>2020-03-11 21:22:44</td>\n","      <td>2011874</td>\n","      <td>199716273</td>\n","      <td>2020-03-11</td>\n","      <td>21:22:44</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>216457</th>\n","      <td>AAPL</td>\n","      <td>aapl short position 225 54 20 00</td>\n","      <td>2019-09-12 14:57:06</td>\n","      <td>1601916</td>\n","      <td>177390427</td>\n","      <td>2019-09-12</td>\n","      <td>14:57:06</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1927007</th>\n","      <td>NFLX</td>\n","      <td>nflx baggies sell pump going back shortly</td>\n","      <td>2020-04-28 16:42:14</td>\n","      <td>1314165</td>\n","      <td>209040750</td>\n","      <td>2020-04-28</td>\n","      <td>16:42:14</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>1693218</th>\n","      <td>FB</td>\n","      <td>fb open 213</td>\n","      <td>2020-05-13 12:16:05</td>\n","      <td>1802585</td>\n","      <td>212267923</td>\n","      <td>2020-05-13</td>\n","      <td>12:16:05</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>1940529</th>\n","      <td>NFLX</td>\n","      <td>nflx leader teen streaming</td>\n","      <td>2020-04-08 17:50:35</td>\n","      <td>1104790</td>\n","      <td>205348240</td>\n","      <td>2020-04-08</td>\n","      <td>17:50:35</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>1972896</th>\n","      <td>NFLX</td>\n","      <td>nflx hope people like throwing away money lol</td>\n","      <td>2019-11-14 14:32:14</td>\n","      <td>1451331</td>\n","      <td>183686619</td>\n","      <td>2019-11-14</td>\n","      <td>14:32:14</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1926978</th>\n","      <td>NFLX</td>\n","      <td>nflx red rover red rover send cliff</td>\n","      <td>2020-04-28 17:38:13</td>\n","      <td>2727802</td>\n","      <td>209056269</td>\n","      <td>2020-04-28</td>\n","      <td>17:38:13</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>179352</th>\n","      <td>AAPL</td>\n","      <td>best imac amp mac black friday amp cyber monda...</td>\n","      <td>2019-11-28 07:59:06</td>\n","      <td>1765977</td>\n","      <td>185275089</td>\n","      <td>2019-11-28</td>\n","      <td>07:59:06</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>51796</th>\n","      <td>AAPL</td>\n","      <td>aapl money mouth face</td>\n","      <td>2020-05-04 13:29:21</td>\n","      <td>3086074</td>\n","      <td>210222253</td>\n","      <td>2020-05-04</td>\n","      <td>13:29:21</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1722179</th>\n","      <td>FB</td>\n","      <td>fb 177 5 today</td>\n","      <td>2019-09-26 12:50:27</td>\n","      <td>1577684</td>\n","      <td>178696247</td>\n","      <td>2019-09-26</td>\n","      <td>12:50:27</td>\n","      <td>2</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>514612 rows × 8 columns</p>\n","</div>"],"text/plain":["        symbol  ... label\n","1945833   NFLX  ...     2\n","216457    AAPL  ...     0\n","1927007   NFLX  ...     2\n","1693218     FB  ...     2\n","1940529   NFLX  ...     0\n","...        ...  ...   ...\n","1972896   NFLX  ...     1\n","1926978   NFLX  ...     2\n","179352    AAPL  ...     1\n","51796     AAPL  ...     1\n","1722179     FB  ...     2\n","\n","[514612 rows x 8 columns]"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"PtHSNmPcQrJt","colab":{"base_uri":"https://localhost:8080/","height":589},"outputId":"66dc8293-c03b-4bd2-ebeb-8a159582b220"},"source":["data = data.dropna(subset=['message'])  \n","data"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>symbol</th>\n","      <th>message</th>\n","      <th>datetime</th>\n","      <th>user</th>\n","      <th>message_id</th>\n","      <th>Date</th>\n","      <th>Time</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1945833</th>\n","      <td>NFLX</td>\n","      <td>21 march maddive selloff sector rotation foid ...</td>\n","      <td>2020-03-11 21:22:44</td>\n","      <td>2011874</td>\n","      <td>199716273</td>\n","      <td>2020-03-11</td>\n","      <td>21:22:44</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>216457</th>\n","      <td>AAPL</td>\n","      <td>aapl short position 225 54 20 00</td>\n","      <td>2019-09-12 14:57:06</td>\n","      <td>1601916</td>\n","      <td>177390427</td>\n","      <td>2019-09-12</td>\n","      <td>14:57:06</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1927007</th>\n","      <td>NFLX</td>\n","      <td>nflx baggies sell pump going back shortly</td>\n","      <td>2020-04-28 16:42:14</td>\n","      <td>1314165</td>\n","      <td>209040750</td>\n","      <td>2020-04-28</td>\n","      <td>16:42:14</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>1693218</th>\n","      <td>FB</td>\n","      <td>fb open 213</td>\n","      <td>2020-05-13 12:16:05</td>\n","      <td>1802585</td>\n","      <td>212267923</td>\n","      <td>2020-05-13</td>\n","      <td>12:16:05</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>1940529</th>\n","      <td>NFLX</td>\n","      <td>nflx leader teen streaming</td>\n","      <td>2020-04-08 17:50:35</td>\n","      <td>1104790</td>\n","      <td>205348240</td>\n","      <td>2020-04-08</td>\n","      <td>17:50:35</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>1972896</th>\n","      <td>NFLX</td>\n","      <td>nflx hope people like throwing away money lol</td>\n","      <td>2019-11-14 14:32:14</td>\n","      <td>1451331</td>\n","      <td>183686619</td>\n","      <td>2019-11-14</td>\n","      <td>14:32:14</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1926978</th>\n","      <td>NFLX</td>\n","      <td>nflx red rover red rover send cliff</td>\n","      <td>2020-04-28 17:38:13</td>\n","      <td>2727802</td>\n","      <td>209056269</td>\n","      <td>2020-04-28</td>\n","      <td>17:38:13</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>179352</th>\n","      <td>AAPL</td>\n","      <td>best imac amp mac black friday amp cyber monda...</td>\n","      <td>2019-11-28 07:59:06</td>\n","      <td>1765977</td>\n","      <td>185275089</td>\n","      <td>2019-11-28</td>\n","      <td>07:59:06</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>51796</th>\n","      <td>AAPL</td>\n","      <td>aapl money mouth face</td>\n","      <td>2020-05-04 13:29:21</td>\n","      <td>3086074</td>\n","      <td>210222253</td>\n","      <td>2020-05-04</td>\n","      <td>13:29:21</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1722179</th>\n","      <td>FB</td>\n","      <td>fb 177 5 today</td>\n","      <td>2019-09-26 12:50:27</td>\n","      <td>1577684</td>\n","      <td>178696247</td>\n","      <td>2019-09-26</td>\n","      <td>12:50:27</td>\n","      <td>2</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>514612 rows × 8 columns</p>\n","</div>"],"text/plain":["        symbol  ... label\n","1945833   NFLX  ...     2\n","216457    AAPL  ...     0\n","1927007   NFLX  ...     2\n","1693218     FB  ...     2\n","1940529   NFLX  ...     0\n","...        ...  ...   ...\n","1972896   NFLX  ...     1\n","1926978   NFLX  ...     2\n","179352    AAPL  ...     1\n","51796     AAPL  ...     1\n","1722179     FB  ...     2\n","\n","[514612 rows x 8 columns]"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"markdown","metadata":{"id":"8Hmomk-Ld8EO"},"source":["#Splitting Dataset to 90-10"]},{"cell_type":"code","metadata":{"id":"tvvMxuYPMli_"},"source":["from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(data['message'].values, data['label'].values, test_size=0.1, random_state=42)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_O7dw10deBsp"},"source":["Creating train and test dataframes"]},{"cell_type":"code","metadata":{"id":"sk7o0NIYP8bw","colab":{"base_uri":"https://localhost:8080/"},"outputId":"0786d88c-2a9f-4e5c-8fdb-e3cc797250d5"},"source":["df_train = pd.DataFrame(y_train)\n","print(df_train.value_counts())\n","\n","df_test = pd.DataFrame(y_test)\n","print(df_test.value_counts())"],"execution_count":null,"outputs":[{"output_type":"stream","text":["1    215016\n","2    153597\n","0     94537\n","dtype: int64\n","1    23934\n","2    17026\n","0    10502\n","dtype: int64\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"LQAOyicqMsCT"},"source":["# Fine Tune"]},{"cell_type":"code","metadata":{"id":"ahRfKyd7Q5R-"},"source":["train = X_train\n","labels = y_train"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XDT7CQkjeHaC"},"source":["Example of Tokenized messages"]},{"cell_type":"code","metadata":{"id":"nEcX_y4BQymH","colab":{"base_uri":"https://localhost:8080/"},"outputId":"edc6661a-9121-46f3-8b2a-a5e4ab70ae81"},"source":["# Print the original sentence.\n","print(' Original: ', train[10])\n","\n","# Print the sentence split into tokens.\n","print('Tokenized: ', tokenizer.tokenize(train[10]))\n","\n","# Print the sentence mapped to token ids.\n","print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(train[10])))"],"execution_count":null,"outputs":[{"output_type":"stream","text":[" Original:  aapl wtf thousands risk robinhood going cover losses missed profits like robinhood fuck ng\n","Tokenized:  ['▁a', 'a', 'pl', '▁w', 't', 'f', '▁thousands', '▁risk', '▁robin', 'hood', '▁going', '▁cover', '▁losses', '▁missed', '▁profit', 's', '▁like', '▁robin', 'hood', '▁fuck', '▁', 'ng']\n","Token IDs:  [15, 286, 14277, 2313, 258, 2513, 2062, 1044, 4854, 8600, 123, 1027, 6661, 1429, 1749, 20, 65, 4854, 8600, 2126, 245, 5563]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"eAzZVOyBQ9eM","colab":{"base_uri":"https://localhost:8080/"},"outputId":"eedf0836-cf15-4616-d8d0-13221c0ac6ca"},"source":["print(data.message.apply(lambda x: len(x)).quantile([0.9]))\n","MAX_LEN = 160"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0.9    156.0\n","Name: message, dtype: float64\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"_kYl4W__eMXt"},"source":["Tokenizing all of the messages and mapping of the tokens to thier word IDs.\n"]},{"cell_type":"code","metadata":{"id":"ODdVBYFCRAIJ","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b3e6b96e-0241-46ec-e2fa-00c0857cad12"},"source":["input_ids = []\n","attention_masks = []\n","\n","# For each sentence encode_plus would tokenize, add special tokens ([CLS], [SEP]), map tokens to IDs, create attention masks\n","for text in train:\n","    \n","    encoded_dict = tokenizer.encode_plus(\n","                        text,                      \n","                        add_special_tokens = True, \n","                        max_length = MAX_LEN,           \n","                        pad_to_max_length = True,\n","                        return_attention_mask = True,   \n","                        return_tensors = 'pt',     \n","                        truncation = True\n","                   )\n","    \n","    # Encoded sentence is being added to a new list\n","    input_ids.append(encoded_dict['input_ids'])\n","    \n","    # Attention masks are added to a new list \n","    attention_masks.append(encoded_dict['attention_mask'])\n","\n","# Lists are converted into tensors.\n","input_ids = torch.cat(input_ids, dim=0)\n","attention_masks = torch.cat(attention_masks, dim=0)\n","labels = torch.tensor(labels)\n","\n","print('Original: ', train[10])\n","print('Token IDs:', input_ids[10])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Original:  aapl wtf thousands risk robinhood going cover losses missed profits like robinhood fuck ng\n","Token IDs: tensor([    2,    15,   286, 14277,  2313,   258,  2513,  2062,  1044,  4854,\n","         8600,   123,  1027,  6661,  1429,  1749,    20,    65,  4854,  8600,\n","         2126,   245,  5563,     3,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"gfV6N5yReuqt"},"source":["Training & validation split\n"]},{"cell_type":"code","metadata":{"id":"vBqXso_GRD5G","colab":{"base_uri":"https://localhost:8080/"},"outputId":"20037af2-b7df-4b4d-ae8e-ba00ba479aef"},"source":["from torch.utils.data import TensorDataset, random_split\n","\n","\n","# Creating TensorDataset by combining the training inputs\n","dataset = TensorDataset(input_ids, attention_masks, labels)\n","\n","# 90-10 train-validation split\n","\n","# Calculate the number of samples to include in each set.\n","train_size = int(0.9 * len(dataset))\n","val_size = len(dataset) - train_size\n","\n","# Dividing the training and validation dataset by selecting samples randomly\n","train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n","\n","print('{} training samples'.format(train_size))\n","print('{} validation samples'.format(val_size))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["416835 training samples\n","46315 validation samples\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ZjRfMLfIRF_G"},"source":["def flat_accuracy(preds, labels):\n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    # classification_rep(labels_flat,pred_flat)\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n","    \n","##To print the model training times for each batch\n","import time\n","import datetime\n","\n","def format_time(elapsed):\n","    '''\n","    Takes a time in seconds and returns a string hh:mm:ss\n","    '''\n","    # Round to the nearest second.\n","    elapsed_rounded = int(round((elapsed)))\n","    \n","    # Format as hh:mm:ss\n","    return str(datetime.timedelta(seconds=elapsed_rounded))\n","\n","# Set the seed value all over the place to make this reproducible.\n","import random\n","def set_random(seed_val):\n","    random.seed(seed_val)\n","    np.random.seed(seed_val)\n","    torch.manual_seed(seed_val)\n","    torch.cuda.manual_seed_all(seed_val)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Afr0s4y9fOFO"},"source":["Function to print the classification Report"]},{"cell_type":"code","metadata":{"id":"Y-q7sHwBRIng"},"source":["from sklearn.metrics import classification_report\n","def classification_rep(y_test,log_predictions):\n","  print('-----------',len(log_predictions),log_predictions)\n","  print('+++++++',len(y_test),y_test)\n","  log_probs = tf.nn.log_softmax(log_predictions, axis=-1)\n","  print('=======', log_probs)\n","  pred_flat = np.argmax(log_probs, axis=1).flatten()\n","  labels_flat = y_test\n","  print(classification_report(labels_flat, pred_flat))\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"422jFMtmfVTT"},"source":["Pythong Garbage collection (To free up RAM)"]},{"cell_type":"code","metadata":{"id":"Tcu2OOFKRMeJ","colab":{"base_uri":"https://localhost:8080/"},"outputId":"2051540f-97f8-4c59-9cd3-293fde506d8e"},"source":["import gc\n","gc.collect()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["532"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"markdown","metadata":{"id":"_BNN-Qm0fbi-"},"source":["Model Training"]},{"cell_type":"code","metadata":{"id":"er5-38YUROAU"},"source":["def train_model(train_dataloader, optimizer, epochs):\n","    \n","    # To store the training and validation loss, validation accuracy, and timings.\n","    training_stats = []\n","\n","    # To measure the total training time for the whole run.\n","    total_t0 = time.time()\n","    all_preds = []\n","    all_labels = []\n","    # For each epoch...\n","    for epoch_i in range(0, epochs):\n","\n","        # ========================================\n","        #               Training\n","        # ========================================\n","\n","        # Perform one full pass over the training set.\n","\n","        print(\"\")\n","        print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n","        print('Training...')\n","\n","        \n","        t0 = time.time()\n","\n","        \n","        total_train_loss = 0\n","\n","        #Setting the model to training mode\n","\n","        model.train()\n","\n","        \n","        for step, batch in enumerate(train_dataloader):\n","\n","            # This block is for progress update of every batch \n","            if step % 40 == 0 and not step == 0:\n","               elapsed = format_time(time.time() - t0)\n","\n","                # Printing the progress.\n","                print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n","\n","            # From the dataloader unpacking the training batch  \n","            \n","            b_input_ids = batch[0].to(device)\n","            b_input_mask = batch[1].to(device)\n","            b_labels = batch[2].to(device)\n","\n","            model.zero_grad()        \n","\n","            #model outputs\n","            loss, logits = model(b_input_ids, \n","                                 attention_mask=b_input_mask, \n","                                 labels=b_labels)\n","\n","            total_train_loss += loss.item()\n","            loss.backward()\n","\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","            optimizer.step()\n","            scheduler.step()\n","\n","        avg_train_loss = total_train_loss / len(train_dataloader)            \n","        #Time taken for epoch \n","        training_time = format_time(time.time() - t0)\n","\n","        print(\"\")\n","        print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n","        print(\"  Training epcoh took: {:}\".format(training_time))\n","\n","        # ========================================\n","        #               Validation\n","        # ========================================\n","\n","        print(\"\")\n","        print(\"Running Validation...\")\n","\n","        t0 = time.time()\n","\n","        # Setting the model to evaluation mode\n","        model.eval()\n","\n","  \n","        total_eval_accuracy = 0\n","        total_eval_loss = 0\n","        nb_eval_steps = 0\n","\n","        \n","        for batch in validation_dataloader:\n","            # Unpack this validation batch from our dataloader. \n","\n","            b_input_ids = batch[0].to(device)\n","            b_input_mask = batch[1].to(device)\n","            b_labels = batch[2].to(device)\n","            with torch.no_grad():        \n","                (loss, logits) = model(b_input_ids, \n","                                       attention_mask=b_input_mask,\n","                                       labels=b_labels)\n","\n","            total_eval_loss += loss.item()\n","            logits = logits.detach().cpu().numpy()\n","            label_ids = b_labels.to('cpu').numpy()\n","\n","            # The accuracy of test messages is calculated for the batch and then accumulated\n","            all_preds.extend(logits)\n","            all_labels.extend(label_ids)\n","            total_eval_accuracy += flat_accuracy(logits, label_ids)\n","\n","\n","        # Final accuracy\n","        avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n","        print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n","\n","        # Calculate the average loss over all of the batches.\n","        avg_val_loss = total_eval_loss / len(validation_dataloader)\n","\n","        validation_time = format_time(time.time() - t0)\n","\n","        print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n","        print(\"  Validation took: {:}\".format(validation_time))\n","\n","        training_stats.append(\n","            {\n","                'epoch': epoch_i + 1,\n","                'Training Loss': avg_train_loss,\n","                'Valid. Loss': avg_val_loss,\n","                'Valid. Accur.': avg_val_accuracy,\n","                'Training Time': training_time,\n","                'Validation Time': validation_time\n","            }\n","        )\n","\n","    print(\"\")\n","    print(\"Training complete!\")\n","\n","    print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n","    classification_rep(all_labels,all_preds)\n","    return training_stats\n","\n","def print_training_stats(training_stats):\n","    pd.set_option('precision', 2)\n","    df_stats = pd.DataFrame(data=training_stats)\n","    df_stats = df_stats.set_index('epoch')\n","    print(df_stats)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"i9Ok_rpufmUS"},"source":["Setting hyperparameters and calling the model train function"]},{"cell_type":"code","metadata":{"id":"E1ZKbzQNRVMC","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b5a9a44c-b21a-4e6b-f186-d19cc78d7d88"},"source":["from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","from transformers import get_linear_schedule_with_warmup, AdamW\n","\n","from transformers import AlbertForSequenceClassification,AlbertConfig\n","\n","lr_s = [1e-5]\n","batch_sizes = [32]\n","from itertools import product\n","hyperparameters = list(product(*[lr_s, batch_sizes]))\n","print(hyperparameters)\n","training_statistics = []\n","for lr, batch_size in hyperparameters:\n","    print(model)\n","   \n","    # Assignining Model to GPU\n","    model.cuda()\n","    # Creating training and validation DataLoaders\n"," \n","    train_dataloader = DataLoader(\n","                train_dataset, \n","                sampler = RandomSampler(train_dataset),\n","                batch_size = batch_size \n","            )\n","\n","\n","    validation_dataloader = DataLoader(\n","                val_dataset, \n","                sampler = SequentialSampler(val_dataset), \n","                batch_size = batch_size\n","            )\n","    \n","\n","    optimizer = AdamW(model.parameters(),\n","                      lr = lr, \n","                      eps = 1e-8, \n","                      weight_decay=1e-2 #added this to reduce over-fitting and fix labelling bais\n","                    )\n","    \n","\n","   \n","    epochs = 2\n","    total_steps = len(train_dataloader) * epochs\n","    scheduler = get_linear_schedule_with_warmup(optimizer, \n","                                                num_warmup_steps = 0, # Default value in run_glue.py\n","                                                num_training_steps = total_steps)\n","\n","    seed_val = 42\n","    set_random(seed_val)\n","    \n","    print(\"Training with hyperparameters: batch size={}, lr={}\".format(batch_size, lr))\n","    training_stats = train_model(train_dataloader, optimizer, epochs)\n","    training_statistics.append(training_stats)\n","    print_training_stats(training_stats)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[(1e-05, 32)]\n","AlbertForSequenceClassification(\n","  (albert): AlbertModel(\n","    (embeddings): AlbertEmbeddings(\n","      (word_embeddings): Embedding(30000, 128, padding_idx=0)\n","      (position_embeddings): Embedding(256, 128)\n","      (token_type_embeddings): Embedding(3, 128)\n","      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0, inplace=False)\n","    )\n","    (encoder): AlbertTransformer(\n","      (embedding_hidden_mapping_in): Linear(in_features=128, out_features=768, bias=True)\n","      (albert_layer_groups): ModuleList(\n","        (0): AlbertLayerGroup(\n","          (albert_layers): ModuleList(\n","            (0): AlbertLayer(\n","              (full_layer_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (attention): AlbertAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (attention_dropout): Dropout(p=0, inplace=False)\n","                (output_dropout): Dropout(p=0, inplace=False)\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              )\n","              (ffn): Linear(in_features=768, out_features=3072, bias=True)\n","              (ffn_output): Linear(in_features=3072, out_features=768, bias=True)\n","              (dropout): Dropout(p=0, inplace=False)\n","            )\n","          )\n","        )\n","      )\n","    )\n","    (pooler): Linear(in_features=768, out_features=768, bias=True)\n","    (pooler_activation): Tanh()\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",")\n","Training with hyperparameters: batch size=32, lr=1e-05\n","\n","======== Epoch 1 / 2 ========\n","Training...\n","  Batch    40  of  13,027.    Elapsed: 0:00:19.\n","  Batch    80  of  13,027.    Elapsed: 0:00:38.\n","  Batch   120  of  13,027.    Elapsed: 0:00:57.\n","  Batch   160  of  13,027.    Elapsed: 0:01:15.\n","  Batch   200  of  13,027.    Elapsed: 0:01:34.\n","  Batch   240  of  13,027.    Elapsed: 0:01:53.\n","  Batch   280  of  13,027.    Elapsed: 0:02:12.\n","  Batch   320  of  13,027.    Elapsed: 0:02:31.\n","  Batch   360  of  13,027.    Elapsed: 0:02:50.\n","  Batch   400  of  13,027.    Elapsed: 0:03:09.\n","  Batch   440  of  13,027.    Elapsed: 0:03:27.\n","  Batch   480  of  13,027.    Elapsed: 0:03:46.\n","  Batch   520  of  13,027.    Elapsed: 0:04:05.\n","  Batch   560  of  13,027.    Elapsed: 0:04:24.\n","  Batch   600  of  13,027.    Elapsed: 0:04:43.\n","  Batch   640  of  13,027.    Elapsed: 0:05:02.\n","  Batch   680  of  13,027.    Elapsed: 0:05:21.\n","  Batch   720  of  13,027.    Elapsed: 0:05:39.\n","  Batch   760  of  13,027.    Elapsed: 0:05:58.\n","  Batch   800  of  13,027.    Elapsed: 0:06:17.\n","  Batch   840  of  13,027.    Elapsed: 0:06:36.\n","  Batch   880  of  13,027.    Elapsed: 0:06:55.\n","  Batch   920  of  13,027.    Elapsed: 0:07:14.\n","  Batch   960  of  13,027.    Elapsed: 0:07:32.\n","  Batch 1,000  of  13,027.    Elapsed: 0:07:51.\n","  Batch 1,040  of  13,027.    Elapsed: 0:08:10.\n","  Batch 1,080  of  13,027.    Elapsed: 0:08:29.\n","  Batch 1,120  of  13,027.    Elapsed: 0:08:48.\n","  Batch 1,160  of  13,027.    Elapsed: 0:09:07.\n","  Batch 1,200  of  13,027.    Elapsed: 0:09:26.\n","  Batch 1,240  of  13,027.    Elapsed: 0:09:44.\n","  Batch 1,280  of  13,027.    Elapsed: 0:10:03.\n","  Batch 1,320  of  13,027.    Elapsed: 0:10:22.\n","  Batch 1,360  of  13,027.    Elapsed: 0:10:41.\n","  Batch 1,400  of  13,027.    Elapsed: 0:11:00.\n","  Batch 1,440  of  13,027.    Elapsed: 0:11:19.\n","  Batch 1,480  of  13,027.    Elapsed: 0:11:37.\n","  Batch 1,520  of  13,027.    Elapsed: 0:11:56.\n","  Batch 1,560  of  13,027.    Elapsed: 0:12:15.\n","  Batch 1,600  of  13,027.    Elapsed: 0:12:34.\n","  Batch 1,640  of  13,027.    Elapsed: 0:12:53.\n","  Batch 1,680  of  13,027.    Elapsed: 0:13:12.\n","  Batch 1,720  of  13,027.    Elapsed: 0:13:31.\n","  Batch 1,760  of  13,027.    Elapsed: 0:13:49.\n","  Batch 1,800  of  13,027.    Elapsed: 0:14:08.\n","  Batch 1,840  of  13,027.    Elapsed: 0:14:27.\n","  Batch 1,880  of  13,027.    Elapsed: 0:14:46.\n","  Batch 1,920  of  13,027.    Elapsed: 0:15:05.\n","  Batch 1,960  of  13,027.    Elapsed: 0:15:24.\n","  Batch 2,000  of  13,027.    Elapsed: 0:15:42.\n","  Batch 2,040  of  13,027.    Elapsed: 0:16:01.\n","  Batch 2,080  of  13,027.    Elapsed: 0:16:20.\n","  Batch 2,120  of  13,027.    Elapsed: 0:16:39.\n","  Batch 2,160  of  13,027.    Elapsed: 0:16:58.\n","  Batch 2,200  of  13,027.    Elapsed: 0:17:17.\n","  Batch 2,240  of  13,027.    Elapsed: 0:17:36.\n","  Batch 2,280  of  13,027.    Elapsed: 0:17:54.\n","  Batch 2,320  of  13,027.    Elapsed: 0:18:13.\n","  Batch 2,360  of  13,027.    Elapsed: 0:18:32.\n","  Batch 2,400  of  13,027.    Elapsed: 0:18:51.\n","  Batch 2,440  of  13,027.    Elapsed: 0:19:10.\n","  Batch 2,480  of  13,027.    Elapsed: 0:19:29.\n","  Batch 2,520  of  13,027.    Elapsed: 0:19:47.\n","  Batch 2,560  of  13,027.    Elapsed: 0:20:06.\n","  Batch 2,600  of  13,027.    Elapsed: 0:20:25.\n","  Batch 2,640  of  13,027.    Elapsed: 0:20:44.\n","  Batch 2,680  of  13,027.    Elapsed: 0:21:03.\n","  Batch 2,720  of  13,027.    Elapsed: 0:21:22.\n","  Batch 2,760  of  13,027.    Elapsed: 0:21:40.\n","  Batch 2,800  of  13,027.    Elapsed: 0:21:59.\n","  Batch 2,840  of  13,027.    Elapsed: 0:22:18.\n","  Batch 2,880  of  13,027.    Elapsed: 0:22:37.\n","  Batch 2,920  of  13,027.    Elapsed: 0:22:56.\n","  Batch 2,960  of  13,027.    Elapsed: 0:23:15.\n","  Batch 3,000  of  13,027.    Elapsed: 0:23:34.\n","  Batch 3,040  of  13,027.    Elapsed: 0:23:52.\n","  Batch 3,080  of  13,027.    Elapsed: 0:24:11.\n","  Batch 3,120  of  13,027.    Elapsed: 0:24:30.\n","  Batch 3,160  of  13,027.    Elapsed: 0:24:49.\n","  Batch 3,200  of  13,027.    Elapsed: 0:25:08.\n","  Batch 3,240  of  13,027.    Elapsed: 0:25:27.\n","  Batch 3,280  of  13,027.    Elapsed: 0:25:46.\n","  Batch 3,320  of  13,027.    Elapsed: 0:26:04.\n","  Batch 3,360  of  13,027.    Elapsed: 0:26:23.\n","  Batch 3,400  of  13,027.    Elapsed: 0:26:42.\n","  Batch 3,440  of  13,027.    Elapsed: 0:27:01.\n","  Batch 3,480  of  13,027.    Elapsed: 0:27:20.\n","  Batch 3,520  of  13,027.    Elapsed: 0:27:39.\n","  Batch 3,560  of  13,027.    Elapsed: 0:27:57.\n","  Batch 3,600  of  13,027.    Elapsed: 0:28:16.\n","  Batch 3,640  of  13,027.    Elapsed: 0:28:35.\n","  Batch 3,680  of  13,027.    Elapsed: 0:28:54.\n","  Batch 3,720  of  13,027.    Elapsed: 0:29:13.\n","  Batch 3,760  of  13,027.    Elapsed: 0:29:32.\n","  Batch 3,800  of  13,027.    Elapsed: 0:29:51.\n","  Batch 3,840  of  13,027.    Elapsed: 0:30:09.\n","  Batch 3,880  of  13,027.    Elapsed: 0:30:28.\n","  Batch 3,920  of  13,027.    Elapsed: 0:30:47.\n","  Batch 3,960  of  13,027.    Elapsed: 0:31:06.\n","  Batch 4,000  of  13,027.    Elapsed: 0:31:25.\n","  Batch 4,040  of  13,027.    Elapsed: 0:31:44.\n","  Batch 4,080  of  13,027.    Elapsed: 0:32:03.\n","  Batch 4,120  of  13,027.    Elapsed: 0:32:21.\n","  Batch 4,160  of  13,027.    Elapsed: 0:32:40.\n","  Batch 4,200  of  13,027.    Elapsed: 0:32:59.\n","  Batch 4,240  of  13,027.    Elapsed: 0:33:18.\n","  Batch 4,280  of  13,027.    Elapsed: 0:33:37.\n","  Batch 4,320  of  13,027.    Elapsed: 0:33:56.\n","  Batch 4,360  of  13,027.    Elapsed: 0:34:14.\n","  Batch 4,400  of  13,027.    Elapsed: 0:34:33.\n","  Batch 4,440  of  13,027.    Elapsed: 0:34:52.\n","  Batch 4,480  of  13,027.    Elapsed: 0:35:11.\n","  Batch 4,520  of  13,027.    Elapsed: 0:35:30.\n","  Batch 4,560  of  13,027.    Elapsed: 0:35:49.\n","  Batch 4,600  of  13,027.    Elapsed: 0:36:08.\n","  Batch 4,640  of  13,027.    Elapsed: 0:36:26.\n","  Batch 4,680  of  13,027.    Elapsed: 0:36:45.\n","  Batch 4,720  of  13,027.    Elapsed: 0:37:04.\n","  Batch 4,760  of  13,027.    Elapsed: 0:37:23.\n","  Batch 4,800  of  13,027.    Elapsed: 0:37:42.\n","  Batch 4,840  of  13,027.    Elapsed: 0:38:01.\n","  Batch 4,880  of  13,027.    Elapsed: 0:38:20.\n","  Batch 4,920  of  13,027.    Elapsed: 0:38:38.\n","  Batch 4,960  of  13,027.    Elapsed: 0:38:57.\n","  Batch 5,000  of  13,027.    Elapsed: 0:39:16.\n","  Batch 5,040  of  13,027.    Elapsed: 0:39:35.\n","  Batch 5,080  of  13,027.    Elapsed: 0:39:54.\n","  Batch 5,120  of  13,027.    Elapsed: 0:40:13.\n","  Batch 5,160  of  13,027.    Elapsed: 0:40:31.\n","  Batch 5,200  of  13,027.    Elapsed: 0:40:50.\n","  Batch 5,240  of  13,027.    Elapsed: 0:41:09.\n","  Batch 5,280  of  13,027.    Elapsed: 0:41:28.\n","  Batch 5,320  of  13,027.    Elapsed: 0:41:47.\n","  Batch 5,360  of  13,027.    Elapsed: 0:42:06.\n","  Batch 5,400  of  13,027.    Elapsed: 0:42:25.\n","  Batch 5,440  of  13,027.    Elapsed: 0:42:43.\n","  Batch 5,480  of  13,027.    Elapsed: 0:43:02.\n","  Batch 5,520  of  13,027.    Elapsed: 0:43:21.\n","  Batch 5,560  of  13,027.    Elapsed: 0:43:40.\n","  Batch 5,600  of  13,027.    Elapsed: 0:43:59.\n","  Batch 5,640  of  13,027.    Elapsed: 0:44:18.\n","  Batch 5,680  of  13,027.    Elapsed: 0:44:36.\n","  Batch 5,720  of  13,027.    Elapsed: 0:44:55.\n","  Batch 5,760  of  13,027.    Elapsed: 0:45:14.\n","  Batch 5,800  of  13,027.    Elapsed: 0:45:33.\n","  Batch 5,840  of  13,027.    Elapsed: 0:45:52.\n","  Batch 5,880  of  13,027.    Elapsed: 0:46:11.\n","  Batch 5,920  of  13,027.    Elapsed: 0:46:30.\n","  Batch 5,960  of  13,027.    Elapsed: 0:46:48.\n","  Batch 6,000  of  13,027.    Elapsed: 0:47:07.\n","  Batch 6,040  of  13,027.    Elapsed: 0:47:26.\n","  Batch 6,080  of  13,027.    Elapsed: 0:47:45.\n","  Batch 6,120  of  13,027.    Elapsed: 0:48:04.\n","  Batch 6,160  of  13,027.    Elapsed: 0:48:23.\n","  Batch 6,200  of  13,027.    Elapsed: 0:48:42.\n","  Batch 6,240  of  13,027.    Elapsed: 0:49:00.\n","  Batch 6,280  of  13,027.    Elapsed: 0:49:19.\n","  Batch 6,320  of  13,027.    Elapsed: 0:49:38.\n","  Batch 6,360  of  13,027.    Elapsed: 0:49:57.\n","  Batch 6,400  of  13,027.    Elapsed: 0:50:16.\n","  Batch 6,440  of  13,027.    Elapsed: 0:50:35.\n","  Batch 6,480  of  13,027.    Elapsed: 0:50:54.\n","  Batch 6,520  of  13,027.    Elapsed: 0:51:12.\n","  Batch 6,560  of  13,027.    Elapsed: 0:51:31.\n","  Batch 6,600  of  13,027.    Elapsed: 0:51:50.\n","  Batch 6,640  of  13,027.    Elapsed: 0:52:09.\n","  Batch 6,680  of  13,027.    Elapsed: 0:52:28.\n","  Batch 6,720  of  13,027.    Elapsed: 0:52:47.\n","  Batch 6,760  of  13,027.    Elapsed: 0:53:05.\n","  Batch 6,800  of  13,027.    Elapsed: 0:53:24.\n","  Batch 6,840  of  13,027.    Elapsed: 0:53:43.\n","  Batch 6,880  of  13,027.    Elapsed: 0:54:02.\n","  Batch 6,920  of  13,027.    Elapsed: 0:54:21.\n","  Batch 6,960  of  13,027.    Elapsed: 0:54:40.\n","  Batch 7,000  of  13,027.    Elapsed: 0:54:59.\n","  Batch 7,040  of  13,027.    Elapsed: 0:55:17.\n","  Batch 7,080  of  13,027.    Elapsed: 0:55:36.\n","  Batch 7,120  of  13,027.    Elapsed: 0:55:55.\n","  Batch 7,160  of  13,027.    Elapsed: 0:56:14.\n","  Batch 7,200  of  13,027.    Elapsed: 0:56:33.\n","  Batch 7,240  of  13,027.    Elapsed: 0:56:52.\n","  Batch 7,280  of  13,027.    Elapsed: 0:57:11.\n","  Batch 7,320  of  13,027.    Elapsed: 0:57:29.\n","  Batch 7,360  of  13,027.    Elapsed: 0:57:48.\n","  Batch 7,400  of  13,027.    Elapsed: 0:58:07.\n","  Batch 7,440  of  13,027.    Elapsed: 0:58:26.\n","  Batch 7,480  of  13,027.    Elapsed: 0:58:45.\n","  Batch 7,520  of  13,027.    Elapsed: 0:59:04.\n","  Batch 7,560  of  13,027.    Elapsed: 0:59:22.\n","  Batch 7,600  of  13,027.    Elapsed: 0:59:41.\n","  Batch 7,640  of  13,027.    Elapsed: 1:00:00.\n","  Batch 7,680  of  13,027.    Elapsed: 1:00:19.\n","  Batch 7,720  of  13,027.    Elapsed: 1:00:38.\n","  Batch 7,760  of  13,027.    Elapsed: 1:00:57.\n","  Batch 7,800  of  13,027.    Elapsed: 1:01:15.\n","  Batch 7,840  of  13,027.    Elapsed: 1:01:34.\n","  Batch 7,880  of  13,027.    Elapsed: 1:01:53.\n","  Batch 7,920  of  13,027.    Elapsed: 1:02:12.\n","  Batch 7,960  of  13,027.    Elapsed: 1:02:31.\n","  Batch 8,000  of  13,027.    Elapsed: 1:02:50.\n","  Batch 8,040  of  13,027.    Elapsed: 1:03:09.\n","  Batch 8,080  of  13,027.    Elapsed: 1:03:27.\n","  Batch 8,120  of  13,027.    Elapsed: 1:03:46.\n","  Batch 8,160  of  13,027.    Elapsed: 1:04:05.\n","  Batch 8,200  of  13,027.    Elapsed: 1:04:24.\n","  Batch 8,240  of  13,027.    Elapsed: 1:04:43.\n","  Batch 8,280  of  13,027.    Elapsed: 1:05:02.\n","  Batch 8,320  of  13,027.    Elapsed: 1:05:20.\n","  Batch 8,360  of  13,027.    Elapsed: 1:05:39.\n","  Batch 8,400  of  13,027.    Elapsed: 1:05:58.\n","  Batch 8,440  of  13,027.    Elapsed: 1:06:17.\n","  Batch 8,480  of  13,027.    Elapsed: 1:06:36.\n","  Batch 8,520  of  13,027.    Elapsed: 1:06:55.\n","  Batch 8,560  of  13,027.    Elapsed: 1:07:14.\n","  Batch 8,600  of  13,027.    Elapsed: 1:07:32.\n","  Batch 8,640  of  13,027.    Elapsed: 1:07:51.\n","  Batch 8,680  of  13,027.    Elapsed: 1:08:10.\n","  Batch 8,720  of  13,027.    Elapsed: 1:08:29.\n","  Batch 8,760  of  13,027.    Elapsed: 1:08:48.\n","  Batch 8,800  of  13,027.    Elapsed: 1:09:07.\n","  Batch 8,840  of  13,027.    Elapsed: 1:09:25.\n","  Batch 8,880  of  13,027.    Elapsed: 1:09:44.\n","  Batch 8,920  of  13,027.    Elapsed: 1:10:03.\n","  Batch 8,960  of  13,027.    Elapsed: 1:10:22.\n","  Batch 9,000  of  13,027.    Elapsed: 1:10:41.\n","  Batch 9,040  of  13,027.    Elapsed: 1:11:00.\n","  Batch 9,080  of  13,027.    Elapsed: 1:11:19.\n","  Batch 9,120  of  13,027.    Elapsed: 1:11:37.\n","  Batch 9,160  of  13,027.    Elapsed: 1:11:56.\n","  Batch 9,200  of  13,027.    Elapsed: 1:12:15.\n","  Batch 9,240  of  13,027.    Elapsed: 1:12:34.\n","  Batch 9,280  of  13,027.    Elapsed: 1:12:53.\n","  Batch 9,320  of  13,027.    Elapsed: 1:13:12.\n","  Batch 9,360  of  13,027.    Elapsed: 1:13:31.\n","  Batch 9,400  of  13,027.    Elapsed: 1:13:49.\n","  Batch 9,440  of  13,027.    Elapsed: 1:14:08.\n","  Batch 9,480  of  13,027.    Elapsed: 1:14:27.\n","  Batch 9,520  of  13,027.    Elapsed: 1:14:46.\n","  Batch 9,560  of  13,027.    Elapsed: 1:15:05.\n","  Batch 9,600  of  13,027.    Elapsed: 1:15:24.\n","  Batch 9,640  of  13,027.    Elapsed: 1:15:42.\n","  Batch 9,680  of  13,027.    Elapsed: 1:16:01.\n","  Batch 9,720  of  13,027.    Elapsed: 1:16:20.\n","  Batch 9,760  of  13,027.    Elapsed: 1:16:39.\n","  Batch 9,800  of  13,027.    Elapsed: 1:16:58.\n","  Batch 9,840  of  13,027.    Elapsed: 1:17:17.\n","  Batch 9,880  of  13,027.    Elapsed: 1:17:36.\n","  Batch 9,920  of  13,027.    Elapsed: 1:17:54.\n","  Batch 9,960  of  13,027.    Elapsed: 1:18:13.\n","  Batch 10,000  of  13,027.    Elapsed: 1:18:32.\n","  Batch 10,040  of  13,027.    Elapsed: 1:18:51.\n","  Batch 10,080  of  13,027.    Elapsed: 1:19:10.\n","  Batch 10,120  of  13,027.    Elapsed: 1:19:29.\n","  Batch 10,160  of  13,027.    Elapsed: 1:19:48.\n","  Batch 10,200  of  13,027.    Elapsed: 1:20:06.\n","  Batch 10,240  of  13,027.    Elapsed: 1:20:25.\n","  Batch 10,280  of  13,027.    Elapsed: 1:20:44.\n","  Batch 10,320  of  13,027.    Elapsed: 1:21:03.\n","  Batch 10,360  of  13,027.    Elapsed: 1:21:22.\n","  Batch 10,400  of  13,027.    Elapsed: 1:21:41.\n","  Batch 10,440  of  13,027.    Elapsed: 1:21:59.\n","  Batch 10,480  of  13,027.    Elapsed: 1:22:18.\n","  Batch 10,520  of  13,027.    Elapsed: 1:22:37.\n","  Batch 10,560  of  13,027.    Elapsed: 1:22:56.\n","  Batch 10,600  of  13,027.    Elapsed: 1:23:15.\n","  Batch 10,640  of  13,027.    Elapsed: 1:23:34.\n","  Batch 10,680  of  13,027.    Elapsed: 1:23:53.\n","  Batch 10,720  of  13,027.    Elapsed: 1:24:11.\n","  Batch 10,760  of  13,027.    Elapsed: 1:24:30.\n","  Batch 10,800  of  13,027.    Elapsed: 1:24:49.\n","  Batch 10,840  of  13,027.    Elapsed: 1:25:08.\n","  Batch 10,880  of  13,027.    Elapsed: 1:25:27.\n","  Batch 10,920  of  13,027.    Elapsed: 1:25:46.\n","  Batch 10,960  of  13,027.    Elapsed: 1:26:04.\n","  Batch 11,000  of  13,027.    Elapsed: 1:26:23.\n","  Batch 11,040  of  13,027.    Elapsed: 1:26:42.\n","  Batch 11,080  of  13,027.    Elapsed: 1:27:01.\n","  Batch 11,120  of  13,027.    Elapsed: 1:27:20.\n","  Batch 11,160  of  13,027.    Elapsed: 1:27:39.\n","  Batch 11,200  of  13,027.    Elapsed: 1:27:58.\n","  Batch 11,240  of  13,027.    Elapsed: 1:28:16.\n","  Batch 11,280  of  13,027.    Elapsed: 1:28:35.\n","  Batch 11,320  of  13,027.    Elapsed: 1:28:54.\n","  Batch 11,360  of  13,027.    Elapsed: 1:29:13.\n","  Batch 11,400  of  13,027.    Elapsed: 1:29:32.\n","  Batch 11,440  of  13,027.    Elapsed: 1:29:51.\n","  Batch 11,480  of  13,027.    Elapsed: 1:30:09.\n","  Batch 11,520  of  13,027.    Elapsed: 1:30:28.\n","  Batch 11,560  of  13,027.    Elapsed: 1:30:47.\n","  Batch 11,600  of  13,027.    Elapsed: 1:31:06.\n","  Batch 11,640  of  13,027.    Elapsed: 1:31:25.\n","  Batch 11,680  of  13,027.    Elapsed: 1:31:44.\n","  Batch 11,720  of  13,027.    Elapsed: 1:32:03.\n","  Batch 11,760  of  13,027.    Elapsed: 1:32:21.\n","  Batch 11,800  of  13,027.    Elapsed: 1:32:40.\n","  Batch 11,840  of  13,027.    Elapsed: 1:32:59.\n","  Batch 11,880  of  13,027.    Elapsed: 1:33:18.\n","  Batch 11,920  of  13,027.    Elapsed: 1:33:37.\n","  Batch 11,960  of  13,027.    Elapsed: 1:33:56.\n","  Batch 12,000  of  13,027.    Elapsed: 1:34:14.\n","  Batch 12,040  of  13,027.    Elapsed: 1:34:33.\n","  Batch 12,080  of  13,027.    Elapsed: 1:34:52.\n","  Batch 12,120  of  13,027.    Elapsed: 1:35:11.\n","  Batch 12,160  of  13,027.    Elapsed: 1:35:30.\n","  Batch 12,200  of  13,027.    Elapsed: 1:35:49.\n","  Batch 12,240  of  13,027.    Elapsed: 1:36:07.\n","  Batch 12,280  of  13,027.    Elapsed: 1:36:26.\n","  Batch 12,320  of  13,027.    Elapsed: 1:36:45.\n","  Batch 12,360  of  13,027.    Elapsed: 1:37:04.\n","  Batch 12,400  of  13,027.    Elapsed: 1:37:23.\n","  Batch 12,440  of  13,027.    Elapsed: 1:37:42.\n","  Batch 12,480  of  13,027.    Elapsed: 1:38:01.\n","  Batch 12,520  of  13,027.    Elapsed: 1:38:19.\n","  Batch 12,560  of  13,027.    Elapsed: 1:38:38.\n","  Batch 12,600  of  13,027.    Elapsed: 1:38:57.\n","  Batch 12,640  of  13,027.    Elapsed: 1:39:16.\n","  Batch 12,680  of  13,027.    Elapsed: 1:39:35.\n","  Batch 12,720  of  13,027.    Elapsed: 1:39:54.\n","  Batch 12,760  of  13,027.    Elapsed: 1:40:13.\n","  Batch 12,800  of  13,027.    Elapsed: 1:40:31.\n","  Batch 12,840  of  13,027.    Elapsed: 1:40:50.\n","  Batch 12,880  of  13,027.    Elapsed: 1:41:09.\n","  Batch 12,920  of  13,027.    Elapsed: 1:41:28.\n","  Batch 12,960  of  13,027.    Elapsed: 1:41:47.\n","  Batch 13,000  of  13,027.    Elapsed: 1:42:06.\n","\n","  Average training loss: 1.05\n","  Training epcoh took: 1:42:18\n","\n","Running Validation...\n","  Accuracy: 0.46\n","  Validation Loss: 1.04\n","  Validation took: 0:03:56\n","\n","======== Epoch 2 / 2 ========\n","Training...\n","  Batch    40  of  13,027.    Elapsed: 0:00:19.\n","  Batch    80  of  13,027.    Elapsed: 0:00:38.\n","  Batch   120  of  13,027.    Elapsed: 0:00:57.\n","  Batch   160  of  13,027.    Elapsed: 0:01:15.\n","  Batch   200  of  13,027.    Elapsed: 0:01:34.\n","  Batch   240  of  13,027.    Elapsed: 0:01:53.\n","  Batch   280  of  13,027.    Elapsed: 0:02:12.\n","  Batch   320  of  13,027.    Elapsed: 0:02:31.\n","  Batch   360  of  13,027.    Elapsed: 0:02:50.\n","  Batch   400  of  13,027.    Elapsed: 0:03:08.\n","  Batch   440  of  13,027.    Elapsed: 0:03:27.\n","  Batch   480  of  13,027.    Elapsed: 0:03:46.\n","  Batch   520  of  13,027.    Elapsed: 0:04:05.\n","  Batch   560  of  13,027.    Elapsed: 0:04:24.\n","  Batch   600  of  13,027.    Elapsed: 0:04:43.\n","  Batch   640  of  13,027.    Elapsed: 0:05:02.\n","  Batch   680  of  13,027.    Elapsed: 0:05:20.\n","  Batch   720  of  13,027.    Elapsed: 0:05:39.\n","  Batch   760  of  13,027.    Elapsed: 0:05:58.\n","  Batch   800  of  13,027.    Elapsed: 0:06:17.\n","  Batch   840  of  13,027.    Elapsed: 0:06:36.\n","  Batch   880  of  13,027.    Elapsed: 0:06:55.\n","  Batch   920  of  13,027.    Elapsed: 0:07:14.\n","  Batch   960  of  13,027.    Elapsed: 0:07:32.\n","  Batch 1,000  of  13,027.    Elapsed: 0:07:51.\n","  Batch 1,040  of  13,027.    Elapsed: 0:08:10.\n","  Batch 1,080  of  13,027.    Elapsed: 0:08:29.\n","  Batch 1,120  of  13,027.    Elapsed: 0:08:48.\n","  Batch 1,160  of  13,027.    Elapsed: 0:09:07.\n","  Batch 1,200  of  13,027.    Elapsed: 0:09:25.\n","  Batch 1,240  of  13,027.    Elapsed: 0:09:44.\n","  Batch 1,280  of  13,027.    Elapsed: 0:10:03.\n","  Batch 1,320  of  13,027.    Elapsed: 0:10:22.\n","  Batch 1,360  of  13,027.    Elapsed: 0:10:41.\n","  Batch 1,400  of  13,027.    Elapsed: 0:11:00.\n","  Batch 1,440  of  13,027.    Elapsed: 0:11:19.\n","  Batch 1,480  of  13,027.    Elapsed: 0:11:37.\n","  Batch 1,520  of  13,027.    Elapsed: 0:11:56.\n","  Batch 1,560  of  13,027.    Elapsed: 0:12:15.\n","  Batch 1,600  of  13,027.    Elapsed: 0:12:34.\n","  Batch 1,640  of  13,027.    Elapsed: 0:12:53.\n","  Batch 1,680  of  13,027.    Elapsed: 0:13:12.\n","  Batch 1,720  of  13,027.    Elapsed: 0:13:30.\n","  Batch 1,760  of  13,027.    Elapsed: 0:13:49.\n","  Batch 1,800  of  13,027.    Elapsed: 0:14:08.\n","  Batch 1,840  of  13,027.    Elapsed: 0:14:27.\n","  Batch 1,880  of  13,027.    Elapsed: 0:14:46.\n","  Batch 1,920  of  13,027.    Elapsed: 0:15:05.\n","  Batch 1,960  of  13,027.    Elapsed: 0:15:24.\n","  Batch 2,000  of  13,027.    Elapsed: 0:15:42.\n","  Batch 2,040  of  13,027.    Elapsed: 0:16:01.\n","  Batch 2,080  of  13,027.    Elapsed: 0:16:20.\n","  Batch 2,120  of  13,027.    Elapsed: 0:16:39.\n","  Batch 2,160  of  13,027.    Elapsed: 0:16:58.\n","  Batch 2,200  of  13,027.    Elapsed: 0:17:17.\n","  Batch 2,240  of  13,027.    Elapsed: 0:17:35.\n","  Batch 2,280  of  13,027.    Elapsed: 0:17:54.\n","  Batch 2,320  of  13,027.    Elapsed: 0:18:13.\n","  Batch 2,360  of  13,027.    Elapsed: 0:18:32.\n","  Batch 2,400  of  13,027.    Elapsed: 0:18:51.\n","  Batch 2,440  of  13,027.    Elapsed: 0:19:10.\n","  Batch 2,480  of  13,027.    Elapsed: 0:19:29.\n","  Batch 2,520  of  13,027.    Elapsed: 0:19:47.\n","  Batch 2,560  of  13,027.    Elapsed: 0:20:06.\n","  Batch 2,600  of  13,027.    Elapsed: 0:20:25.\n","  Batch 2,640  of  13,027.    Elapsed: 0:20:44.\n","  Batch 2,680  of  13,027.    Elapsed: 0:21:03.\n","  Batch 2,720  of  13,027.    Elapsed: 0:21:22.\n","  Batch 2,760  of  13,027.    Elapsed: 0:21:41.\n","  Batch 2,800  of  13,027.    Elapsed: 0:21:59.\n","  Batch 2,840  of  13,027.    Elapsed: 0:22:18.\n","  Batch 2,880  of  13,027.    Elapsed: 0:22:37.\n","  Batch 2,920  of  13,027.    Elapsed: 0:22:56.\n","  Batch 2,960  of  13,027.    Elapsed: 0:23:15.\n","  Batch 3,000  of  13,027.    Elapsed: 0:23:34.\n","  Batch 3,040  of  13,027.    Elapsed: 0:23:52.\n","  Batch 3,080  of  13,027.    Elapsed: 0:24:11.\n","  Batch 3,120  of  13,027.    Elapsed: 0:24:30.\n","  Batch 3,160  of  13,027.    Elapsed: 0:24:49.\n","  Batch 3,200  of  13,027.    Elapsed: 0:25:08.\n","  Batch 3,240  of  13,027.    Elapsed: 0:25:27.\n","  Batch 3,280  of  13,027.    Elapsed: 0:25:45.\n","  Batch 3,320  of  13,027.    Elapsed: 0:26:04.\n","  Batch 3,360  of  13,027.    Elapsed: 0:26:23.\n","  Batch 3,400  of  13,027.    Elapsed: 0:26:42.\n","  Batch 3,440  of  13,027.    Elapsed: 0:27:01.\n","  Batch 3,480  of  13,027.    Elapsed: 0:27:20.\n","  Batch 3,520  of  13,027.    Elapsed: 0:27:39.\n","  Batch 3,560  of  13,027.    Elapsed: 0:27:57.\n","  Batch 3,600  of  13,027.    Elapsed: 0:28:16.\n","  Batch 3,640  of  13,027.    Elapsed: 0:28:35.\n","  Batch 3,680  of  13,027.    Elapsed: 0:28:54.\n","  Batch 3,720  of  13,027.    Elapsed: 0:29:13.\n","  Batch 3,760  of  13,027.    Elapsed: 0:29:32.\n","  Batch 3,800  of  13,027.    Elapsed: 0:29:51.\n","  Batch 3,840  of  13,027.    Elapsed: 0:30:09.\n","  Batch 3,880  of  13,027.    Elapsed: 0:30:28.\n","  Batch 3,920  of  13,027.    Elapsed: 0:30:47.\n","  Batch 3,960  of  13,027.    Elapsed: 0:31:06.\n","  Batch 4,000  of  13,027.    Elapsed: 0:31:25.\n","  Batch 4,040  of  13,027.    Elapsed: 0:31:44.\n","  Batch 4,080  of  13,027.    Elapsed: 0:32:02.\n","  Batch 4,120  of  13,027.    Elapsed: 0:32:21.\n","  Batch 4,160  of  13,027.    Elapsed: 0:32:40.\n","  Batch 4,200  of  13,027.    Elapsed: 0:32:59.\n","  Batch 4,240  of  13,027.    Elapsed: 0:33:18.\n","  Batch 4,280  of  13,027.    Elapsed: 0:33:37.\n","  Batch 4,320  of  13,027.    Elapsed: 0:33:56.\n","  Batch 4,360  of  13,027.    Elapsed: 0:34:14.\n","  Batch 4,400  of  13,027.    Elapsed: 0:34:33.\n","  Batch 4,440  of  13,027.    Elapsed: 0:34:52.\n","  Batch 4,480  of  13,027.    Elapsed: 0:35:11.\n","  Batch 4,520  of  13,027.    Elapsed: 0:35:30.\n","  Batch 4,560  of  13,027.    Elapsed: 0:35:49.\n","  Batch 4,600  of  13,027.    Elapsed: 0:36:08.\n","  Batch 4,640  of  13,027.    Elapsed: 0:36:26.\n","  Batch 4,680  of  13,027.    Elapsed: 0:36:45.\n","  Batch 4,720  of  13,027.    Elapsed: 0:37:04.\n","  Batch 4,760  of  13,027.    Elapsed: 0:37:23.\n","  Batch 4,800  of  13,027.    Elapsed: 0:37:42.\n","  Batch 4,840  of  13,027.    Elapsed: 0:38:01.\n","  Batch 4,880  of  13,027.    Elapsed: 0:38:19.\n","  Batch 4,920  of  13,027.    Elapsed: 0:38:38.\n","  Batch 4,960  of  13,027.    Elapsed: 0:38:57.\n","  Batch 5,000  of  13,027.    Elapsed: 0:39:16.\n","  Batch 5,040  of  13,027.    Elapsed: 0:39:35.\n","  Batch 5,080  of  13,027.    Elapsed: 0:39:54.\n","  Batch 5,120  of  13,027.    Elapsed: 0:40:13.\n","  Batch 5,160  of  13,027.    Elapsed: 0:40:31.\n","  Batch 5,200  of  13,027.    Elapsed: 0:40:50.\n","  Batch 5,240  of  13,027.    Elapsed: 0:41:09.\n","  Batch 5,280  of  13,027.    Elapsed: 0:41:28.\n","  Batch 5,320  of  13,027.    Elapsed: 0:41:47.\n","  Batch 5,360  of  13,027.    Elapsed: 0:42:06.\n","  Batch 5,400  of  13,027.    Elapsed: 0:42:24.\n","  Batch 5,440  of  13,027.    Elapsed: 0:42:43.\n","  Batch 5,480  of  13,027.    Elapsed: 0:43:02.\n","  Batch 5,520  of  13,027.    Elapsed: 0:43:21.\n","  Batch 5,560  of  13,027.    Elapsed: 0:43:40.\n","  Batch 5,600  of  13,027.    Elapsed: 0:43:59.\n","  Batch 5,640  of  13,027.    Elapsed: 0:44:18.\n","  Batch 5,680  of  13,027.    Elapsed: 0:44:36.\n","  Batch 5,720  of  13,027.    Elapsed: 0:44:55.\n","  Batch 5,760  of  13,027.    Elapsed: 0:45:14.\n","  Batch 5,800  of  13,027.    Elapsed: 0:45:33.\n","  Batch 5,840  of  13,027.    Elapsed: 0:45:52.\n","  Batch 5,880  of  13,027.    Elapsed: 0:46:11.\n","  Batch 5,920  of  13,027.    Elapsed: 0:46:29.\n","  Batch 5,960  of  13,027.    Elapsed: 0:46:48.\n","  Batch 6,000  of  13,027.    Elapsed: 0:47:07.\n","  Batch 6,040  of  13,027.    Elapsed: 0:47:26.\n","  Batch 6,080  of  13,027.    Elapsed: 0:47:45.\n","  Batch 6,120  of  13,027.    Elapsed: 0:48:04.\n","  Batch 6,160  of  13,027.    Elapsed: 0:48:23.\n","  Batch 6,200  of  13,027.    Elapsed: 0:48:41.\n","  Batch 6,240  of  13,027.    Elapsed: 0:49:00.\n","  Batch 6,280  of  13,027.    Elapsed: 0:49:19.\n","  Batch 6,320  of  13,027.    Elapsed: 0:49:38.\n","  Batch 6,360  of  13,027.    Elapsed: 0:49:57.\n","  Batch 6,400  of  13,027.    Elapsed: 0:50:16.\n","  Batch 6,440  of  13,027.    Elapsed: 0:50:34.\n","  Batch 6,480  of  13,027.    Elapsed: 0:50:53.\n","  Batch 6,520  of  13,027.    Elapsed: 0:51:12.\n","  Batch 6,560  of  13,027.    Elapsed: 0:51:31.\n","  Batch 6,600  of  13,027.    Elapsed: 0:51:50.\n","  Batch 6,640  of  13,027.    Elapsed: 0:52:09.\n","  Batch 6,680  of  13,027.    Elapsed: 0:52:27.\n","  Batch 6,720  of  13,027.    Elapsed: 0:52:46.\n","  Batch 6,760  of  13,027.    Elapsed: 0:53:05.\n","  Batch 6,800  of  13,027.    Elapsed: 0:53:24.\n","  Batch 6,840  of  13,027.    Elapsed: 0:53:43.\n","  Batch 6,880  of  13,027.    Elapsed: 0:54:02.\n","  Batch 6,920  of  13,027.    Elapsed: 0:54:21.\n","  Batch 6,960  of  13,027.    Elapsed: 0:54:39.\n","  Batch 7,000  of  13,027.    Elapsed: 0:54:58.\n","  Batch 7,040  of  13,027.    Elapsed: 0:55:17.\n","  Batch 7,080  of  13,027.    Elapsed: 0:55:36.\n","  Batch 7,120  of  13,027.    Elapsed: 0:55:55.\n","  Batch 7,160  of  13,027.    Elapsed: 0:56:14.\n","  Batch 7,200  of  13,027.    Elapsed: 0:56:32.\n","  Batch 7,240  of  13,027.    Elapsed: 0:56:51.\n","  Batch 7,280  of  13,027.    Elapsed: 0:57:10.\n","  Batch 7,320  of  13,027.    Elapsed: 0:57:29.\n","  Batch 7,360  of  13,027.    Elapsed: 0:57:48.\n","  Batch 7,400  of  13,027.    Elapsed: 0:58:07.\n","  Batch 7,440  of  13,027.    Elapsed: 0:58:25.\n","  Batch 7,480  of  13,027.    Elapsed: 0:58:44.\n","  Batch 7,520  of  13,027.    Elapsed: 0:59:03.\n","  Batch 7,560  of  13,027.    Elapsed: 0:59:22.\n","  Batch 7,600  of  13,027.    Elapsed: 0:59:41.\n","  Batch 7,640  of  13,027.    Elapsed: 1:00:00.\n","  Batch 7,680  of  13,027.    Elapsed: 1:00:19.\n","  Batch 7,720  of  13,027.    Elapsed: 1:00:37.\n","  Batch 7,760  of  13,027.    Elapsed: 1:00:56.\n","  Batch 7,800  of  13,027.    Elapsed: 1:01:15.\n","  Batch 7,840  of  13,027.    Elapsed: 1:01:34.\n","  Batch 7,880  of  13,027.    Elapsed: 1:01:53.\n","  Batch 7,920  of  13,027.    Elapsed: 1:02:12.\n","  Batch 7,960  of  13,027.    Elapsed: 1:02:30.\n","  Batch 8,000  of  13,027.    Elapsed: 1:02:49.\n","  Batch 8,040  of  13,027.    Elapsed: 1:03:08.\n","  Batch 8,080  of  13,027.    Elapsed: 1:03:27.\n","  Batch 8,120  of  13,027.    Elapsed: 1:03:46.\n","  Batch 8,160  of  13,027.    Elapsed: 1:04:05.\n","  Batch 8,200  of  13,027.    Elapsed: 1:04:23.\n","  Batch 8,240  of  13,027.    Elapsed: 1:04:42.\n","  Batch 8,280  of  13,027.    Elapsed: 1:05:01.\n","  Batch 8,320  of  13,027.    Elapsed: 1:05:20.\n","  Batch 8,360  of  13,027.    Elapsed: 1:05:39.\n","  Batch 8,400  of  13,027.    Elapsed: 1:05:58.\n","  Batch 8,440  of  13,027.    Elapsed: 1:06:17.\n","  Batch 8,480  of  13,027.    Elapsed: 1:06:35.\n","  Batch 8,520  of  13,027.    Elapsed: 1:06:54.\n","  Batch 8,560  of  13,027.    Elapsed: 1:07:13.\n","  Batch 8,600  of  13,027.    Elapsed: 1:07:32.\n","  Batch 8,640  of  13,027.    Elapsed: 1:07:51.\n","  Batch 8,680  of  13,027.    Elapsed: 1:08:10.\n","  Batch 8,720  of  13,027.    Elapsed: 1:08:29.\n","  Batch 8,760  of  13,027.    Elapsed: 1:08:47.\n","  Batch 8,800  of  13,027.    Elapsed: 1:09:06.\n","  Batch 8,840  of  13,027.    Elapsed: 1:09:25.\n","  Batch 8,880  of  13,027.    Elapsed: 1:09:44.\n","  Batch 8,920  of  13,027.    Elapsed: 1:10:03.\n","  Batch 8,960  of  13,027.    Elapsed: 1:10:22.\n","  Batch 9,000  of  13,027.    Elapsed: 1:10:40.\n","  Batch 9,040  of  13,027.    Elapsed: 1:10:59.\n","  Batch 9,080  of  13,027.    Elapsed: 1:11:18.\n","  Batch 9,120  of  13,027.    Elapsed: 1:11:37.\n","  Batch 9,160  of  13,027.    Elapsed: 1:11:56.\n","  Batch 9,200  of  13,027.    Elapsed: 1:12:15.\n","  Batch 9,240  of  13,027.    Elapsed: 1:12:34.\n","  Batch 9,280  of  13,027.    Elapsed: 1:12:52.\n","  Batch 9,320  of  13,027.    Elapsed: 1:13:11.\n","  Batch 9,360  of  13,027.    Elapsed: 1:13:30.\n","  Batch 9,400  of  13,027.    Elapsed: 1:13:49.\n","  Batch 9,440  of  13,027.    Elapsed: 1:14:08.\n","  Batch 9,480  of  13,027.    Elapsed: 1:14:27.\n","  Batch 9,520  of  13,027.    Elapsed: 1:14:45.\n","  Batch 9,560  of  13,027.    Elapsed: 1:15:04.\n","  Batch 9,600  of  13,027.    Elapsed: 1:15:23.\n","  Batch 9,640  of  13,027.    Elapsed: 1:15:42.\n","  Batch 9,680  of  13,027.    Elapsed: 1:16:01.\n","  Batch 9,720  of  13,027.    Elapsed: 1:16:20.\n","  Batch 9,760  of  13,027.    Elapsed: 1:16:39.\n","  Batch 9,800  of  13,027.    Elapsed: 1:16:57.\n","  Batch 9,840  of  13,027.    Elapsed: 1:17:16.\n","  Batch 9,880  of  13,027.    Elapsed: 1:17:35.\n","  Batch 9,920  of  13,027.    Elapsed: 1:17:54.\n","  Batch 9,960  of  13,027.    Elapsed: 1:18:13.\n","  Batch 10,000  of  13,027.    Elapsed: 1:18:32.\n","  Batch 10,040  of  13,027.    Elapsed: 1:18:50.\n","  Batch 10,080  of  13,027.    Elapsed: 1:19:09.\n","  Batch 10,120  of  13,027.    Elapsed: 1:19:28.\n","  Batch 10,160  of  13,027.    Elapsed: 1:19:47.\n","  Batch 10,200  of  13,027.    Elapsed: 1:20:06.\n","  Batch 10,240  of  13,027.    Elapsed: 1:20:25.\n","  Batch 10,280  of  13,027.    Elapsed: 1:20:43.\n","  Batch 10,320  of  13,027.    Elapsed: 1:21:02.\n","  Batch 10,360  of  13,027.    Elapsed: 1:21:21.\n","  Batch 10,400  of  13,027.    Elapsed: 1:21:40.\n","  Batch 10,440  of  13,027.    Elapsed: 1:21:59.\n","  Batch 10,480  of  13,027.    Elapsed: 1:22:18.\n","  Batch 10,520  of  13,027.    Elapsed: 1:22:37.\n","  Batch 10,560  of  13,027.    Elapsed: 1:22:55.\n","  Batch 10,600  of  13,027.    Elapsed: 1:23:14.\n","  Batch 10,640  of  13,027.    Elapsed: 1:23:33.\n","  Batch 10,680  of  13,027.    Elapsed: 1:23:52.\n","  Batch 10,720  of  13,027.    Elapsed: 1:24:11.\n","  Batch 10,760  of  13,027.    Elapsed: 1:24:30.\n","  Batch 10,800  of  13,027.    Elapsed: 1:24:48.\n","  Batch 10,840  of  13,027.    Elapsed: 1:25:07.\n","  Batch 10,880  of  13,027.    Elapsed: 1:25:26.\n","  Batch 10,920  of  13,027.    Elapsed: 1:25:45.\n","  Batch 10,960  of  13,027.    Elapsed: 1:26:04.\n","  Batch 11,000  of  13,027.    Elapsed: 1:26:23.\n","  Batch 11,040  of  13,027.    Elapsed: 1:26:42.\n","  Batch 11,080  of  13,027.    Elapsed: 1:27:00.\n","  Batch 11,120  of  13,027.    Elapsed: 1:27:19.\n","  Batch 11,160  of  13,027.    Elapsed: 1:27:38.\n","  Batch 11,200  of  13,027.    Elapsed: 1:27:57.\n","  Batch 11,240  of  13,027.    Elapsed: 1:28:16.\n","  Batch 11,280  of  13,027.    Elapsed: 1:28:35.\n","  Batch 11,320  of  13,027.    Elapsed: 1:28:53.\n","  Batch 11,360  of  13,027.    Elapsed: 1:29:12.\n","  Batch 11,400  of  13,027.    Elapsed: 1:29:31.\n","  Batch 11,440  of  13,027.    Elapsed: 1:29:50.\n","  Batch 11,480  of  13,027.    Elapsed: 1:30:09.\n","  Batch 11,520  of  13,027.    Elapsed: 1:30:28.\n","  Batch 11,560  of  13,027.    Elapsed: 1:30:47.\n","  Batch 11,600  of  13,027.    Elapsed: 1:31:05.\n","  Batch 11,640  of  13,027.    Elapsed: 1:31:24.\n","  Batch 11,680  of  13,027.    Elapsed: 1:31:43.\n","  Batch 11,720  of  13,027.    Elapsed: 1:32:02.\n","  Batch 11,760  of  13,027.    Elapsed: 1:32:21.\n","  Batch 11,800  of  13,027.    Elapsed: 1:32:40.\n","  Batch 11,840  of  13,027.    Elapsed: 1:32:58.\n","  Batch 11,880  of  13,027.    Elapsed: 1:33:17.\n","  Batch 11,920  of  13,027.    Elapsed: 1:33:36.\n","  Batch 11,960  of  13,027.    Elapsed: 1:33:55.\n","  Batch 12,000  of  13,027.    Elapsed: 1:34:14.\n","  Batch 12,040  of  13,027.    Elapsed: 1:34:33.\n","  Batch 12,080  of  13,027.    Elapsed: 1:34:52.\n","  Batch 12,120  of  13,027.    Elapsed: 1:35:10.\n","  Batch 12,160  of  13,027.    Elapsed: 1:35:29.\n","  Batch 12,200  of  13,027.    Elapsed: 1:35:48.\n","  Batch 12,240  of  13,027.    Elapsed: 1:36:07.\n","  Batch 12,280  of  13,027.    Elapsed: 1:36:26.\n","  Batch 12,320  of  13,027.    Elapsed: 1:36:45.\n","  Batch 12,360  of  13,027.    Elapsed: 1:37:03.\n","  Batch 12,400  of  13,027.    Elapsed: 1:37:22.\n","  Batch 12,440  of  13,027.    Elapsed: 1:37:41.\n","  Batch 12,480  of  13,027.    Elapsed: 1:38:00.\n","  Batch 12,520  of  13,027.    Elapsed: 1:38:19.\n","  Batch 12,560  of  13,027.    Elapsed: 1:38:38.\n","  Batch 12,600  of  13,027.    Elapsed: 1:38:57.\n","  Batch 12,640  of  13,027.    Elapsed: 1:39:15.\n","  Batch 12,680  of  13,027.    Elapsed: 1:39:34.\n","  Batch 12,720  of  13,027.    Elapsed: 1:39:53.\n","  Batch 12,760  of  13,027.    Elapsed: 1:40:12.\n","  Batch 12,800  of  13,027.    Elapsed: 1:40:31.\n","  Batch 12,840  of  13,027.    Elapsed: 1:40:50.\n","  Batch 12,880  of  13,027.    Elapsed: 1:41:09.\n","  Batch 12,920  of  13,027.    Elapsed: 1:41:27.\n","  Batch 12,960  of  13,027.    Elapsed: 1:41:46.\n","  Batch 13,000  of  13,027.    Elapsed: 1:42:05.\n","\n","  Average training loss: 1.03\n","  Training epcoh took: 1:42:17\n","\n","Running Validation...\n","  Accuracy: 0.47\n","  Validation Loss: 1.03\n","  Validation took: 0:03:56\n","\n","Training complete!\n","Total training took 3:32:27 (h:mm:ss)\n","----------- 92630 "],"name":"stdout"},{"output_type":"stream","text":["IOPub data rate exceeded.\n","The notebook server will temporarily stop sending output\n","to the client in order to avoid crashing it.\n","To change this limit, set the config variable\n","`--NotebookApp.iopub_data_rate_limit`.\n","\n","Current values:\n","NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n","NotebookApp.rate_limit_window=3.0 (secs)\n","\n"],"name":"stderr"},{"output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.00      0.00      0.00     18860\n","           1       0.47      0.95      0.63     42812\n","           2       0.46      0.09      0.15     30958\n","\n","    accuracy                           0.47     92630\n","   macro avg       0.31      0.34      0.26     92630\n","weighted avg       0.37      0.47      0.34     92630\n","\n","       Training Loss  Valid. Loss  Valid. Accur. Training Time Validation Time\n","epoch                                                                         \n","1               1.05         1.04           0.46       1:42:18         0:03:56\n","2               1.03         1.03           0.47       1:42:17         0:03:56\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"5qnLTqmdgAjS"},"source":["#Saving Fine-tuned Model"]},{"cell_type":"code","metadata":{"id":"gq0cfZyTRq0t","colab":{"base_uri":"https://localhost:8080/"},"outputId":"80c48ed7-bf6d-4161-d7b2-5716d2ac640a"},"source":["import os\n","#defining output directory\n","output_dir = '/content/gdrive/MyDrive/ModelFiles/1Year_3labels/'\n","\n","#making new directory\n","if not os.path.exists(output_dir):\n","    os.makedirs(output_dir)\n","\n","print(\"Saving model to %s\" % output_dir)\n","#Model Save\n","model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n","model_to_save.save_pretrained(output_dir)\n","tokenizer.save_pretrained(output_dir)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Saving model to /content/gdrive/MyDrive/ModelFiles/1Year_3labels/\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["('/content/gdrive/MyDrive/ModelFiles/1Year_3labels/tokenizer_config.json',\n"," '/content/gdrive/MyDrive/ModelFiles/1Year_3labels/special_tokens_map.json',\n"," '/content/gdrive/MyDrive/ModelFiles/1Year_3labels/spiece.model',\n"," '/content/gdrive/MyDrive/ModelFiles/1Year_3labels/added_tokens.json')"]},"metadata":{"tags":[]},"execution_count":25}]},{"cell_type":"code","metadata":{"id":"H7tUP1rqT39_"},"source":["model_save_name = 'finetuned_Albert1Year3labels.bin'\n","path = F\"//content/gdrive/MyDrive/ModelFiles/1Year_3labels/\"+model_save_name\n","torch.save(model.state_dict(), path)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DtBGzlMFwHx8"},"source":["#Evaluating Performance on Test dataset"]},{"cell_type":"code","metadata":{"id":"wTUo8rIxvalw","colab":{"base_uri":"https://localhost:8080/"},"outputId":"90f801e7-e938-4e6a-f2ea-fbdb6cf27034"},"source":["import pandas as pd\n","from torch.utils.data import TensorDataset, random_split\n","from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","\n","\n","print('Number of test sentences: {:,}\\n'.format(df_test.shape[0]))\n","\n","# Create test message and label lists\n","test_sentences = X_test\n","test_labels = y_test\n","\n","# Tokenizing all of the messages and mapping the tokens to thier word IDs.\n","input_ids = []\n","attention_masks = []\n","\n","# For each sentence encode_plus would tokenize, add special tokens ([CLS], [SEP]), map tokens to IDs, create attention masks\n","for sent in test_sentences:\n","\n","    encoded_dict = tokenizer.encode_plus(\n","                        sent,                      \n","                        add_special_tokens = True, \n","                        max_length = 160,          \n","                        pad_to_max_length = True,\n","                        return_attention_mask = True,\n","                        return_tensors = 'pt',     \n","                   )\n","    \n","    \n","    input_ids.append(encoded_dict['input_ids'])\n","   \n","    attention_masks.append(encoded_dict['attention_mask'])\n","\n","# Converting the lists into tensors\n","input_ids = torch.cat(input_ids, dim=0)\n","attention_masks = torch.cat(attention_masks, dim=0)\n","labels = torch.tensor(test_labels)\n","\n","  \n","batch_size = 8  \n","\n","# Creation of DataLoaders\n","prediction_data = TensorDataset(input_ids, attention_masks, labels)\n","prediction_sampler = SequentialSampler(prediction_data)\n","prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Number of test sentences: 51,462\n","\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"JzXDf0ZngKXU"},"source":["Loading Fine-tuned Model"]},{"cell_type":"code","metadata":{"id":"vdZhmIlRwRKL","colab":{"base_uri":"https://localhost:8080/"},"outputId":"a55cacfb-ac42-4c20-9b25-72ae4a1c0d58"},"source":["from transformers import BertModel, AlbertForSequenceClassification, AdamW, BertConfig\n","PRE_TRAINED_MODEL_NAME_OR_PATH = '/content/gdrive/MyDrive/ModelFiles/1Year_3labels'\n","model = AlbertForSequenceClassification.from_pretrained(PRE_TRAINED_MODEL_NAME_OR_PATH, num_labels = 3,output_attentions = False, output_hidden_states = False)\n","model.cuda()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["AlbertForSequenceClassification(\n","  (albert): AlbertModel(\n","    (embeddings): AlbertEmbeddings(\n","      (word_embeddings): Embedding(30000, 128, padding_idx=0)\n","      (position_embeddings): Embedding(256, 128)\n","      (token_type_embeddings): Embedding(3, 128)\n","      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0, inplace=False)\n","    )\n","    (encoder): AlbertTransformer(\n","      (embedding_hidden_mapping_in): Linear(in_features=128, out_features=768, bias=True)\n","      (albert_layer_groups): ModuleList(\n","        (0): AlbertLayerGroup(\n","          (albert_layers): ModuleList(\n","            (0): AlbertLayer(\n","              (full_layer_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (attention): AlbertAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (attention_dropout): Dropout(p=0, inplace=False)\n","                (output_dropout): Dropout(p=0, inplace=False)\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              )\n","              (ffn): Linear(in_features=768, out_features=3072, bias=True)\n","              (ffn_output): Linear(in_features=3072, out_features=768, bias=True)\n","              (dropout): Dropout(p=0, inplace=False)\n","            )\n","          )\n","        )\n","      )\n","    )\n","    (pooler): Linear(in_features=768, out_features=768, bias=True)\n","    (pooler_activation): Tanh()\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":33}]},{"cell_type":"markdown","metadata":{"id":"j9eHBSXYgOX9"},"source":["Prediction on test set\n"]},{"cell_type":"code","metadata":{"id":"r24ah5lVwdFj","colab":{"base_uri":"https://localhost:8080/"},"outputId":"6e201fdc-5290-4d70-e6c2-b85dd4ce620e"},"source":["\n","print('Predicting labels for {:,} test sentences...'.format(len(input_ids)))\n","\n","model.eval()\n","\n","\n","predictions , true_labels = [], []\n","\n","# Using the prediction dataloader\n","for batch in prediction_dataloader:\n","  \n","  batch = tuple(t.to(device) for t in batch)\n","  \n","  b_input_ids, b_input_mask, b_labels = batch\n","  \n","  with torch.no_grad():\n","      outputs = model(b_input_ids, token_type_ids=None, \n","                      attention_mask=b_input_mask)\n","\n","  logits = outputs[0]\n","\n","  logits = logits.detach().cpu().numpy()\n","  label_ids = b_labels.to('cpu').numpy()\n","  \n","  # Storing the predictions and true labels\n","  predictions.append(logits)\n","  true_labels.append(label_ids)\n","\n","print('    DONE.')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Predicting labels for 51,462 test sentences...\n","    DONE.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"No0vfA9-gQ3C"},"source":["Printing Classification Report"]},{"cell_type":"code","metadata":{"id":"uyKOdKvHwfhb","colab":{"base_uri":"https://localhost:8080/"},"outputId":"7eaaf082-f56e-4e09-e516-bcd37576515e"},"source":["import numpy as np\n","from sklearn.metrics import precision_recall_fscore_support\n","flat_predictions = np.concatenate(predictions, axis=0)\n","\n","# For every sample selecting the class with highest value\n","flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n","\n","flat_true_labels = np.concatenate(true_labels, axis=0)\n","\n","print(\"F1 score: {}\".format(precision_recall_fscore_support(flat_predictions, flat_true_labels, average='macro')))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["F1 score: (0.3550328126887882, 0.30830551743446377, 0.29093763791778987, None)\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"lm6qUghhwijn","colab":{"base_uri":"https://localhost:8080/"},"outputId":"96e32a2b-cf2c-4dd8-bf24-4f7cb9230ee7"},"source":["from sklearn.metrics import classification_report\n","print(classification_report(flat_true_labels, flat_predictions,zero_division=1))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       1.00      0.00      0.00     10502\n","           1       0.47      0.89      0.62     23934\n","           2       0.45      0.18      0.25     17026\n","\n","    accuracy                           0.47     51462\n","   macro avg       0.64      0.36      0.29     51462\n","weighted avg       0.57      0.47      0.37     51462\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"TDSKjurqDDTz"},"source":["#Company wise Prediction"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":580},"id":"FcwjnohBDEWu","outputId":"575a7410-c3f8-4fa8-e2ae-a31cc6deda40"},"source":["import pandas as pd\n","import numpy as np\n","df = pd.read_csv('/content/gdrive/My Drive/Combined_FAANG_percentage_2.2.csv')\n","# data = pd.read_csv('/content/gdrive/My Drive/Data/Combined_FAANG_binary_previous.csv')\n","df"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>symbol</th>\n","      <th>message</th>\n","      <th>datetime</th>\n","      <th>user</th>\n","      <th>message_id</th>\n","      <th>Date</th>\n","      <th>Time</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>AAPL</td>\n","      <td>peak profit last 6 expired option alerts aapl ...</td>\n","      <td>2020-07-19 09:49:35</td>\n","      <td>1442893</td>\n","      <td>229008387</td>\n","      <td>2020-07-19</td>\n","      <td>09:49:35</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>AAPL</td>\n","      <td>aapl jul 17 382 50 calls option volume 144 44 ...</td>\n","      <td>2020-07-19 09:47:26</td>\n","      <td>1442893</td>\n","      <td>229008357</td>\n","      <td>2020-07-19</td>\n","      <td>09:47:26</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>AAPL</td>\n","      <td>tsla market true bubble territory profitable c...</td>\n","      <td>2020-07-19 09:01:25</td>\n","      <td>1115913</td>\n","      <td>229007569</td>\n","      <td>2020-07-19</td>\n","      <td>09:01:25</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>AAPL</td>\n","      <td>aapl analyzed 26 analysts buy consensus 86 ana...</td>\n","      <td>2020-07-19 08:13:00</td>\n","      <td>47688</td>\n","      <td>229006733</td>\n","      <td>2020-07-19</td>\n","      <td>08:13:00</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>AAPL</td>\n","      <td>aapl new article dogs dow august 4 adopt ignore</td>\n","      <td>2020-07-19 07:54:05</td>\n","      <td>1555408</td>\n","      <td>229006403</td>\n","      <td>2020-07-19</td>\n","      <td>07:54:05</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>2566821</th>\n","      <td>NFLX</td>\n","      <td>get long nflx stop 44 50</td>\n","      <td>2009-09-22 12:21:48</td>\n","      <td>78</td>\n","      <td>591710</td>\n","      <td>2009-09-22</td>\n","      <td>12:21:48</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2566822</th>\n","      <td>NFLX</td>\n","      <td>nflx awards 1 mill better movie search engine ...</td>\n","      <td>2009-09-21 17:44:20</td>\n","      <td>384</td>\n","      <td>589527</td>\n","      <td>2009-09-21</td>\n","      <td>17:44:20</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2566823</th>\n","      <td>NFLX</td>\n","      <td>39 mbt 47 swapped nflx</td>\n","      <td>2009-09-21 15:48:19</td>\n","      <td>78</td>\n","      <td>588855</td>\n","      <td>2009-09-21</td>\n","      <td>15:48:19</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2566824</th>\n","      <td>NFLX</td>\n","      <td>long nflx</td>\n","      <td>2009-09-21 15:39:57</td>\n","      <td>78</td>\n","      <td>588781</td>\n","      <td>2009-09-21</td>\n","      <td>15:39:57</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2566825</th>\n","      <td>NFLX</td>\n","      <td>youtube talks stream movies rentals goog appl ...</td>\n","      <td>2009-09-02 20:25:14</td>\n","      <td>581</td>\n","      <td>536552</td>\n","      <td>2009-09-02</td>\n","      <td>20:25:14</td>\n","      <td>-1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>2566826 rows × 8 columns</p>\n","</div>"],"text/plain":["        symbol  ... label\n","0         AAPL  ...     1\n","1         AAPL  ...     1\n","2         AAPL  ...     1\n","3         AAPL  ...     1\n","4         AAPL  ...     1\n","...        ...  ...   ...\n","2566821   NFLX  ...     0\n","2566822   NFLX  ...     0\n","2566823   NFLX  ...     0\n","2566824   NFLX  ...     0\n","2566825   NFLX  ...    -1\n","\n","[2566826 rows x 8 columns]"]},"metadata":{"tags":[]},"execution_count":196}]},{"cell_type":"code","metadata":{"id":"SMBSJgNNJ11W"},"source":["df.drop(df[df['label'] == 0].index, inplace = True) \n","df[\"label\"].replace({-1: 0}, inplace=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":580},"id":"IisnZ0KZDSv2","outputId":"d8052168-c655-429b-b7ab-99f8877dc7ff"},"source":["df = df.drop(df[df['Date'] <= '2020-06-20'].index)\n","df"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>symbol</th>\n","      <th>message</th>\n","      <th>datetime</th>\n","      <th>user</th>\n","      <th>message_id</th>\n","      <th>Date</th>\n","      <th>Time</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>AAPL</td>\n","      <td>peak profit last 6 expired option alerts aapl ...</td>\n","      <td>2020-07-19 09:49:35</td>\n","      <td>1442893</td>\n","      <td>229008387</td>\n","      <td>2020-07-19</td>\n","      <td>09:49:35</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>AAPL</td>\n","      <td>aapl jul 17 382 50 calls option volume 144 44 ...</td>\n","      <td>2020-07-19 09:47:26</td>\n","      <td>1442893</td>\n","      <td>229008357</td>\n","      <td>2020-07-19</td>\n","      <td>09:47:26</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>AAPL</td>\n","      <td>tsla market true bubble territory profitable c...</td>\n","      <td>2020-07-19 09:01:25</td>\n","      <td>1115913</td>\n","      <td>229007569</td>\n","      <td>2020-07-19</td>\n","      <td>09:01:25</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>AAPL</td>\n","      <td>aapl analyzed 26 analysts buy consensus 86 ana...</td>\n","      <td>2020-07-19 08:13:00</td>\n","      <td>47688</td>\n","      <td>229006733</td>\n","      <td>2020-07-19</td>\n","      <td>08:13:00</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>AAPL</td>\n","      <td>aapl new article dogs dow august 4 adopt ignore</td>\n","      <td>2020-07-19 07:54:05</td>\n","      <td>1555408</td>\n","      <td>229006403</td>\n","      <td>2020-07-19</td>\n","      <td>07:54:05</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>1916645</th>\n","      <td>NFLX</td>\n","      <td>gnus nflx went hitting 40 early stage went way...</td>\n","      <td>2020-06-21 02:18:09</td>\n","      <td>3417009</td>\n","      <td>221638148</td>\n","      <td>2020-06-21</td>\n","      <td>02:18:09</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1916646</th>\n","      <td>NFLX</td>\n","      <td>nflx analyzed 30 analysts buy consensus 80 ana...</td>\n","      <td>2020-06-21 02:10:00</td>\n","      <td>47688</td>\n","      <td>221637550</td>\n","      <td>2020-06-21</td>\n","      <td>02:10:00</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1916647</th>\n","      <td>NFLX</td>\n","      <td>spy 4 years still talking obama wow imagine co...</td>\n","      <td>2020-06-21 01:22:23</td>\n","      <td>543250</td>\n","      <td>221633362</td>\n","      <td>2020-06-21</td>\n","      <td>01:22:23</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1916648</th>\n","      <td>NFLX</td>\n","      <td>spy gld tlt nflx find trading next week video ...</td>\n","      <td>2020-06-21 01:03:29</td>\n","      <td>438774</td>\n","      <td>221631558</td>\n","      <td>2020-06-21</td>\n","      <td>01:03:29</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1916649</th>\n","      <td>NFLX</td>\n","      <td>many times need stop morons benefit whole amd ...</td>\n","      <td>2020-06-21 00:40:24</td>\n","      <td>2859254</td>\n","      <td>221629714</td>\n","      <td>2020-06-21</td>\n","      <td>00:40:24</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>46472 rows × 8 columns</p>\n","</div>"],"text/plain":["        symbol  ... label\n","0         AAPL  ...     1\n","1         AAPL  ...     1\n","2         AAPL  ...     1\n","3         AAPL  ...     1\n","4         AAPL  ...     1\n","...        ...  ...   ...\n","1916645   NFLX  ...     1\n","1916646   NFLX  ...     1\n","1916647   NFLX  ...     1\n","1916648   NFLX  ...     1\n","1916649   NFLX  ...     1\n","\n","[46472 rows x 8 columns]"]},"metadata":{"tags":[]},"execution_count":198}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sjm4m9fhTS6o","outputId":"1aea8c99-634a-45d2-81ef-35c0826e0134"},"source":["df['symbol'].unique()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array(['AAPL', 'AMZN', 'FB', 'GOOGL', 'NFLX'], dtype=object)"]},"metadata":{"tags":[]},"execution_count":199}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oLdB33iQDVOa","outputId":"ff51c717-018e-42f2-eacb-e607018c18b9"},"source":["# Load a trained model and vocabulary that you have fine-tuned\n","from transformers import AlbertForSequenceClassification,  AdamW, BertConfig\n","from transformers import  AlbertTokenizer\n","\n","output_dir = '/content/gdrive/MyDrive/ModelFiles/1YEAR'\n","\n","model = AlbertForSequenceClassification.from_pretrained(output_dir)\n","tokenizer = AlbertTokenizer.from_pretrained(output_dir)\n","model.to(device)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["AlbertForSequenceClassification(\n","  (albert): AlbertModel(\n","    (embeddings): AlbertEmbeddings(\n","      (word_embeddings): Embedding(30000, 128, padding_idx=0)\n","      (position_embeddings): Embedding(256, 128)\n","      (token_type_embeddings): Embedding(2, 128)\n","      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0, inplace=False)\n","    )\n","    (encoder): AlbertTransformer(\n","      (embedding_hidden_mapping_in): Linear(in_features=128, out_features=768, bias=True)\n","      (albert_layer_groups): ModuleList(\n","        (0): AlbertLayerGroup(\n","          (albert_layers): ModuleList(\n","            (0): AlbertLayer(\n","              (full_layer_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (attention): AlbertAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (attention_dropout): Dropout(p=0, inplace=False)\n","                (output_dropout): Dropout(p=0, inplace=False)\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              )\n","              (ffn): Linear(in_features=768, out_features=3072, bias=True)\n","              (ffn_output): Linear(in_features=3072, out_features=768, bias=True)\n","              (dropout): Dropout(p=0, inplace=False)\n","            )\n","          )\n","        )\n","      )\n","    )\n","    (pooler): Linear(in_features=768, out_features=768, bias=True)\n","    (pooler_activation): Tanh()\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":200}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":580},"id":"uvNfi6FSEId1","outputId":"4d538cc4-b472-40f7-edce-8cbb04439ee3"},"source":["#To change the company change isin value \n","pred_df = df.loc[df['symbol'].isin(['GOOGL'])]\n","pred_df"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>symbol</th>\n","      <th>message</th>\n","      <th>datetime</th>\n","      <th>user</th>\n","      <th>message_id</th>\n","      <th>Date</th>\n","      <th>Time</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1793223</th>\n","      <td>GOOGL</td>\n","      <td>wmt walmart poised next competitor amazon alwa...</td>\n","      <td>2020-07-14 23:59:39</td>\n","      <td>3454111</td>\n","      <td>227816923</td>\n","      <td>2020-07-14</td>\n","      <td>23:59:39</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1793224</th>\n","      <td>GOOGL</td>\n","      <td>uber anyone info goog getting food delivery</td>\n","      <td>2020-07-14 23:35:51</td>\n","      <td>3681981</td>\n","      <td>227811719</td>\n","      <td>2020-07-14</td>\n","      <td>23:35:51</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1793225</th>\n","      <td>GOOGL</td>\n","      <td>goog analyzed 26 analysts buy consensus 96 ana...</td>\n","      <td>2020-07-14 23:18:00</td>\n","      <td>47688</td>\n","      <td>227807482</td>\n","      <td>2020-07-14</td>\n","      <td>23:18:00</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1793226</th>\n","      <td>GOOGL</td>\n","      <td>aapl amzn fb msft goog las vegas 39 cosmopolit...</td>\n","      <td>2020-07-14 23:07:24</td>\n","      <td>900651</td>\n","      <td>227805071</td>\n","      <td>2020-07-14</td>\n","      <td>23:07:24</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1793227</th>\n","      <td>GOOGL</td>\n","      <td>uber remember goog attempt smartphones easy th...</td>\n","      <td>2020-07-14 22:45:47</td>\n","      <td>3790554</td>\n","      <td>227799677</td>\n","      <td>2020-07-14</td>\n","      <td>22:45:47</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>1794982</th>\n","      <td>GOOGL</td>\n","      <td>39 see next year later right makes sense buy r...</td>\n","      <td>2020-06-21 01:15:13</td>\n","      <td>2870908</td>\n","      <td>221632669</td>\n","      <td>2020-06-21</td>\n","      <td>01:15:13</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1794983</th>\n","      <td>GOOGL</td>\n","      <td>spy trump praised justice scalia said wants pu...</td>\n","      <td>2020-06-21 00:57:57</td>\n","      <td>543250</td>\n","      <td>221631017</td>\n","      <td>2020-06-21</td>\n","      <td>00:57:57</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1794984</th>\n","      <td>GOOGL</td>\n","      <td>sure goog wants roku 39 client base also want ...</td>\n","      <td>2020-06-21 00:44:17</td>\n","      <td>2870908</td>\n","      <td>221629981</td>\n","      <td>2020-06-21</td>\n","      <td>00:44:17</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1794985</th>\n","      <td>GOOGL</td>\n","      <td>spy come trumper know bad way 39 watching thin...</td>\n","      <td>2020-06-21 00:44:05</td>\n","      <td>543250</td>\n","      <td>221629966</td>\n","      <td>2020-06-21</td>\n","      <td>00:44:05</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1794986</th>\n","      <td>GOOGL</td>\n","      <td>roku nice run sabrina merge 39 make sense goog...</td>\n","      <td>2020-06-21 00:36:28</td>\n","      <td>2870908</td>\n","      <td>221629391</td>\n","      <td>2020-06-21</td>\n","      <td>00:36:28</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1739 rows × 8 columns</p>\n","</div>"],"text/plain":["        symbol  ... label\n","1793223  GOOGL  ...     1\n","1793224  GOOGL  ...     1\n","1793225  GOOGL  ...     1\n","1793226  GOOGL  ...     1\n","1793227  GOOGL  ...     1\n","...        ...  ...   ...\n","1794982  GOOGL  ...     1\n","1794983  GOOGL  ...     1\n","1794984  GOOGL  ...     1\n","1794985  GOOGL  ...     1\n","1794986  GOOGL  ...     1\n","\n","[1739 rows x 8 columns]"]},"metadata":{"tags":[]},"execution_count":201}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NRZEu_UaE4Iz","outputId":"436dd634-e285-4d72-e84d-92837b7779f2"},"source":["pred_df.drop(columns = ['datetime', 'user', 'message_id', 'Time'], inplace = True)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py:4174: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  errors=errors,\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"y30GJMa7EnBX"},"source":["def convert_data(sentences,labels):\n","  \n","  input_ids = []\n","  attention_masks = []\n","\n","  # For each sentence encode_plus would tokenize, add special tokens ([CLS], [SEP]), map tokens to IDs, create attention masks\n","  for sent in sentences:\n","\n","      encoded_dict = tokenizer.encode_plus(\n","                          sent,                     \n","                          add_special_tokens = True, \n","                          max_length = 160,           \n","                          truncation = True,\n","                          pad_to_max_length = True,\n","                          return_attention_mask = True,\n","                          return_tensors = 'pt',     \n","                    )\n","  \n","      input_ids.append(encoded_dict['input_ids'])\n","      attention_masks.append(encoded_dict['attention_mask'])\n","\n","  input_ids = torch.cat(input_ids, dim=0)\n","  attention_masks = torch.cat(attention_masks, dim=0)\n","  labels = torch.tensor(labels)\n","  return input_ids, attention_masks, labels\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hTPtlQV-EKsA"},"source":["from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","from torch.utils.data import TensorDataset, random_split\n","import torch.nn.functional as F  #for softmax function \n","import numpy as np \n","def final_prediction(df):\n","  input_ids, attention_masks, labels = convert_data(df.message.values,df.label.values)\n","  batch_size = 8  \n","  prediction_data = TensorDataset(input_ids, attention_masks, labels)\n","  prediction_sampler = SequentialSampler(prediction_data)\n","  prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)\n","\n","  print('Predicting labels for {:,} test sentences...'.format(len(input_ids)))\n","\n","  model.eval()\n","  predictions , true_labels = [], []\n","  prediction = np.empty((0,2)) \n","\n","\n","  for batch in prediction_dataloader:\n","    batch = tuple(t for t in batch)\n","    b_input_ids, b_input_mask, b_labels = batch\n","    \n","    b_input_ids = batch[0].to(device)\n","    b_input_mask = batch[1].to(device)\n","    b_labels = batch[2].to(device)\n","    \n","    with torch.no_grad():\n","        outputs = model(b_input_ids, token_type_ids=None, \n","                        attention_mask=b_input_mask)\n","        pt_predictions = F.softmax(outputs[0], dim=-1)  \n","        prediction = np.append(prediction, pt_predictions.detach().cpu().numpy(), axis=0) \n","\n","    logits = outputs[0]\n","\n","    logits = logits.detach().cpu().numpy()\n","    label_ids = b_labels.to('cpu').numpy()\n","    predictions.append(logits)\n","    true_labels.append(label_ids)\n","\n","  print('    DONE.')\n","  pred_labels = np.argmax(prediction, axis=1)\n","  return predictions,true_labels, pred_labels"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eRTCArR2EMP5","outputId":"75432eaf-95f2-4cd1-ea25-2b614db814e4"},"source":["predictions,true_labels, pred_labels = final_prediction(pred_df)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Predicting labels for 1,739 test sentences...\n","    DONE.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":536},"id":"ZuqvibJwFcCJ","outputId":"2abe8d77-b320-404f-80d9-d1dde78e7999"},"source":["pred_df['pred_label'] = pred_labels\n","pred_df"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  \"\"\"Entry point for launching an IPython kernel.\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>symbol</th>\n","      <th>message</th>\n","      <th>Date</th>\n","      <th>label</th>\n","      <th>pred_label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1793223</th>\n","      <td>GOOGL</td>\n","      <td>wmt walmart poised next competitor amazon alwa...</td>\n","      <td>2020-07-14</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1793224</th>\n","      <td>GOOGL</td>\n","      <td>uber anyone info goog getting food delivery</td>\n","      <td>2020-07-14</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1793225</th>\n","      <td>GOOGL</td>\n","      <td>goog analyzed 26 analysts buy consensus 96 ana...</td>\n","      <td>2020-07-14</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1793226</th>\n","      <td>GOOGL</td>\n","      <td>aapl amzn fb msft goog las vegas 39 cosmopolit...</td>\n","      <td>2020-07-14</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1793227</th>\n","      <td>GOOGL</td>\n","      <td>uber remember goog attempt smartphones easy th...</td>\n","      <td>2020-07-14</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>1794982</th>\n","      <td>GOOGL</td>\n","      <td>39 see next year later right makes sense buy r...</td>\n","      <td>2020-06-21</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1794983</th>\n","      <td>GOOGL</td>\n","      <td>spy trump praised justice scalia said wants pu...</td>\n","      <td>2020-06-21</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1794984</th>\n","      <td>GOOGL</td>\n","      <td>sure goog wants roku 39 client base also want ...</td>\n","      <td>2020-06-21</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1794985</th>\n","      <td>GOOGL</td>\n","      <td>spy come trumper know bad way 39 watching thin...</td>\n","      <td>2020-06-21</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1794986</th>\n","      <td>GOOGL</td>\n","      <td>roku nice run sabrina merge 39 make sense goog...</td>\n","      <td>2020-06-21</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1739 rows × 5 columns</p>\n","</div>"],"text/plain":["        symbol  ... pred_label\n","1793223  GOOGL  ...          1\n","1793224  GOOGL  ...          1\n","1793225  GOOGL  ...          1\n","1793226  GOOGL  ...          1\n","1793227  GOOGL  ...          1\n","...        ...  ...        ...\n","1794982  GOOGL  ...          0\n","1794983  GOOGL  ...          1\n","1794984  GOOGL  ...          0\n","1794985  GOOGL  ...          1\n","1794986  GOOGL  ...          1\n","\n","[1739 rows x 5 columns]"]},"metadata":{"tags":[]},"execution_count":206}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u4JTRR4hMgWt","outputId":"10843cd9-69b1-476a-f6b7-1fc04d2c1b3c"},"source":["pred_df['label'].value_counts()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1    1279\n","0     460\n","Name: label, dtype: int64"]},"metadata":{"tags":[]},"execution_count":207}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YMfK01zFJdao","outputId":"ccfdd862-bbf9-4fa1-ce94-267a554295c9"},"source":["pred_df['pred_label'].value_counts()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1    1511\n","0     228\n","Name: pred_label, dtype: int64"]},"metadata":{"tags":[]},"execution_count":208}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G5uwXgyGFx_y","outputId":"ca609e50-e67d-4d5b-f3f3-73360a4682fb"},"source":["from sklearn.metrics import classification_report\n","print(classification_report(pred_df['label'].values, pred_df['pred_label'].values,zero_division=1))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.34      0.17      0.22       460\n","           1       0.75      0.88      0.81      1279\n","\n","    accuracy                           0.69      1739\n","   macro avg       0.54      0.52      0.52      1739\n","weighted avg       0.64      0.69      0.65      1739\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mPshcpM1W4sl","outputId":"a3dd2969-6b91-4120-f135-32060be265e0"},"source":["import numpy as np\n","\n","print(np.mean(pred_df['pred_label'].values))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0.8688901667625072\n"],"name":"stdout"}]}]}